./xla/hlo/evaluator/hlo_evaluator_test.cc:4996:  GTEST_SKIP();
./xla/service/cpu/tests/cpu_eigen_dot_operation_test.cc:66:  GTEST_SKIP() << "OneDNN rewrites dot instruction to custom-call.";
./xla/service/cpu/tests/cpu_eigen_dot_operation_test.cc:84:  GTEST_SKIP() << "OneDNN rewrites dot instruction to custom-call.";
./xla/service/dynamic_padder_test.cc:1411:  GTEST_SKIP();
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:355:    GTEST_SKIP() << "Conv-Bias-Relu6 fusion is supported and recommended with "
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:383:    GTEST_SKIP() << "Conv-Bias-Relu6 fusion is supported and recommended with "
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:405:    GTEST_SKIP()
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:719:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:744:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:780:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:816:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:858:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:902:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:1042:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_fused_conv_rewriter_test.cc:1084:  GTEST_SKIP() << "FP8 convolutions require CUDA 12 and cuDNN 8.9.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:57:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:63:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:126:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:132:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:195:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:201:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:265:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:271:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:335:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:341:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:392:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:398:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:469:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:475:    GTEST_SKIP()
./xla/service/gpu/cudnn_norm_rewriter_test.cc:546:  GTEST_SKIP() << "Layer norm kernels require CUDA 12 and cuDNN 8.9.5.";
./xla/service/gpu/cudnn_norm_rewriter_test.cc:552:    GTEST_SKIP()
./xla/service/gpu/determinism_test.cc:98:    GTEST_SKIP() << "No hipblas-lt support on this architecture!";
./xla/service/gpu/determinism_test.cc:119:    GTEST_SKIP() << "Triton not used on pre-Volta GPUs";
./xla/service/gpu/determinism_test.cc:122:  GTEST_SKIP() << "Triton Gemm rewriter is not yet supported on ROCM";
./xla/service/gpu/float_support_test.cc:77:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/gemm_algorithm_picker_test.cc:52:    GTEST_SKIP() << "Skipping this test as it is supported and recommended with "
./xla/service/gpu/gemm_algorithm_picker_test.cc:127:    GTEST_SKIP() << "Skipping this test as it is supported and recommended with "
./xla/service/gpu/gpu_layout_assignment_test.cc:406:    GTEST_SKIP() << "FP8 convolutions require HOPPER or newer archiecture.";
./xla/service/gpu/gpu_layout_assignment_test.cc:431:    GTEST_SKIP() << "Conv with Bfloat16 uses NHWC layout for "
./xla/service/gpu/gpu_layout_assignment_test.cc:456:    GTEST_SKIP() << "Conv with FP16 uses NHWC layout for "
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:68:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:511:      GTEST_SKIP() << absl::Substitute(
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:631:    GTEST_SKIP() << absl::Substitute(
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:730:      GTEST_SKIP() << absl::Substitute(
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:789:    GTEST_SKIP() << "Exponential op does not support F16.";
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:792:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:924:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:966:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1010:    GTEST_SKIP() << "Exponential op does not support F16.";
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1013:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1086:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1145:    GTEST_SKIP() << "Exponential op does not support F16.";
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1148:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1217:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1286:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1346:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1407:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1471:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1547:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1609:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1671:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1742:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1810:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1913:    GTEST_SKIP() << "rsqrt op does not support F16.";
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1916:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:1986:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:2055:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:2124:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:2189:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:2253:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_parametrized_test.cc:2318:    GTEST_SKIP() << R"(No BF16 before Ampere. Pre-Ampere BF16 behavior is tested
./xla/service/gpu/ir_emitter_triton_test.cc:1000:  GTEST_SKIP() << "This test times out when -UNDEBUG is set.";
./xla/service/gpu/ir_emitter_triton_test.cc:1452:      GTEST_SKIP() << "Triton fusion on pre-Ampere GPUs is limited.";
./xla/service/gpu/ir_emitter_triton_test.cc:2435:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:2478:    GTEST_SKIP() << "This test is for Ampere+ GPUs.";
./xla/service/gpu/ir_emitter_triton_test.cc:2632:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:2680:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:2751:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:2811:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:3348:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:3469:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:3495:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:3521:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/ir_emitter_triton_test.cc:3548:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/kernels/cutlass_gemm_fusion_test.cc:269:  GTEST_SKIP() << "Requires CUTLASS 3.3.0+";
./xla/service/gpu/model/analytical_latency_estimator_test.cc:110:    GTEST_SKIP() << "This test is for Pascal+ GPUs.";
./xla/service/gpu/model/hlo_op_profiler_test.cc:30:  GTEST_SKIP() << "Not built with --config=cuda";
./xla/service/gpu/priority_fusion_test.cc:397:  GTEST_SKIP() << "b/294198633";
./xla/service/gpu/runtime3/command_buffer_thunk_test.cc:547:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/service/gpu/runtime3/command_buffer_thunk_test.cc:629:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/service/gpu/runtime3/command_buffer_thunk_test.cc:714:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/service/gpu/runtime3/command_buffer_thunk_test.cc:795:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/service/gpu/tests/gemm_rewrite_test.cc:141:    GTEST_SKIP() << "BlasLt is not supported on this GPU architecture";
./xla/service/gpu/tests/gemm_rewrite_test.cc:167:    GTEST_SKIP()
./xla/service/gpu/tests/gemm_rewrite_test.cc:190:    GTEST_SKIP() << "BlasLt is not supported on this GPU architecture";
./xla/service/gpu/tests/gemm_rewrite_test.cc:308:      GTEST_SKIP() << "BlasLt is not supported on this GPU architecture";
./xla/service/gpu/tests/gemm_rewrite_test.cc:804:    GTEST_SKIP() << "TODO: Unsupported C64 gpublas-lt datatype on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:988:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1020:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1047:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1086:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1124:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1156:    GTEST_SKIP() << "DoBlasGemmWithAlgorithm is not yet implemented on ROCm";
./xla/service/gpu/tests/gemm_rewrite_test.cc:1902:    GTEST_SKIP()
./xla/service/gpu/tests/gemm_rewrite_test.cc:1944:    GTEST_SKIP()
./xla/service/gpu/tests/gemm_rewrite_test.cc:2143:      GTEST_SKIP() << "BlasLt is not supported on this GPU architecture";
./xla/service/gpu/tests/gemm_rewrite_test.cc:2969:    GTEST_SKIP() << "Padding of GEMM bf16 operands only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:3527:    GTEST_SKIP() << "TODO: Unsupported blas-lt epilogue on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:3590:    GTEST_SKIP() << "TODO: Unsupported blas-lt epilogue on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:3650:    GTEST_SKIP() << "TODO: Unsupported blas-lt epilogue on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:3714:    GTEST_SKIP() << "Padding of GEMM bf16 operands only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:3941:    GTEST_SKIP() << "Padding of GEMM operands only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4034:    GTEST_SKIP() << "Padding of GEMM operands only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4175:    GTEST_SKIP() << "Padding of GEMM operands only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4347:    GTEST_SKIP() << "Padding of GEMM operands in bfloat16 only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4440:    GTEST_SKIP() << "Padding of GEMM operands in bfloat16 only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4536:    GTEST_SKIP() << "Padding of GEMM operands in bfloat16 only implemented on "
./xla/service/gpu/tests/gemm_rewrite_test.cc:4589:    GTEST_SKIP() << "TODO: Unsupported blas-lt F64 datatype on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4769:    GTEST_SKIP() << "TODO: Unsupported mixed datatypes on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4806:    GTEST_SKIP() << "TODO: Unsupported mixed datatypes on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4842:    GTEST_SKIP() << "TODO: Unsupported mixed datatypes on ROCM";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4928:      GTEST_SKIP() << "F8 gemm rewrite is not yet supported on ROCm platform";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4935:    GTEST_SKIP() << "Test requires a pre-Ada GPU.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:4959:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5005:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5049:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5102:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5160:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5196:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5260:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5320:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5386:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5424:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5477:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5534:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5591:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5621:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5678:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5742:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5807:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5850:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5916:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:5983:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6056:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6117:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6178:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6252:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6334:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6404:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6485:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6547:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6607:  GTEST_SKIP() << "A matrix bias on a matmul is only supported in CUDA 12";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6665:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6723:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6786:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6862:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:6943:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:7020:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:7052:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:7120:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:7186:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gemm_rewrite_test.cc:7216:  GTEST_SKIP() << "F8 gemm rewrite is only supported in CUDA 12 and above.";
./xla/service/gpu/tests/gpu_fused_mha_test.cc:447:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:476:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:504:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:534:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:565:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:595:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:625:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:653:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:677:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:705:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1275:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1307:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1336:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1368:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1617:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1647:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:1774:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:2162:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/gpu_fused_mha_test.cc:2191:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./xla/service/gpu/tests/reduction_vectorization_test.cc:58:    GTEST_SKIP() << "Maxwell GPUs are less vectorized";
./xla/service/gpu/triton_autotuner_test.cc:360:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/triton_autotuner_test.cc:389:    GTEST_SKIP() << "No BF16 before Ampere.";
./xla/service/gpu/triton_autotuner_test.cc:462:    GTEST_SKIP() << "Not enough shared memory to run big tiles before Ampere.";
./xla/service/gpu/triton_autotuner_test.cc:500:    GTEST_SKIP() << "Not enough shared memory to run big tiles before Ampere.";
./xla/stream_executor/cuda/cuda_command_buffer_test.cc:316:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/stream_executor/cuda/cuda_command_buffer_test.cc:408:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/stream_executor/cuda/cuda_command_buffer_test.cc:511:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/stream_executor/cuda/cuda_command_buffer_test.cc:607:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/stream_executor/cuda/cuda_command_buffer_test.cc:662:    GTEST_SKIP() << "CUDA graph conditionals are not supported";
./xla/tests/array_elementwise_ops_test.cc:1598:  GTEST_SKIP()
./xla/tests/collective_ops_test.cc:40:    GTEST_SKIP() << "Test requires at least " << x << " devices"; \
./xla/tests/collective_ops_test.cc:1046:    GTEST_SKIP();
./xla/tests/constants_test.cc:291:    GTEST_SKIP() << "Bitcasts are unsupported by MLIR";
./xla/tests/custom_call_test.cc:173:    GTEST_SKIP() << "Appears to test an XLA current implementation detail";
./xla/tests/custom_call_test.cc:326:    GTEST_SKIP() << "Invalid values unsupported by MLIR";
./xla/tests/local_client_execute_test.cc:760:    GTEST_SKIP_("requires two devices");
./xla/tests/local_client_execute_test.cc:787:    GTEST_SKIP();
./xla/tests/manifest_checking_test.cc:121:      GTEST_SKIP();
./xla/tests/onednn_matmul_test.cc:52:    GTEST_SKIP() << "CPU does not support BF16.";
./xla/tests/reduce_hlo_test.cc:78:    GTEST_SKIP() << "Explicit layouts not supported by MLIR";
./xla/tests/scatter_test.cc:858:    GTEST_SKIP() << "Variadic scatter not supported by MLIR";
./xla/tests/token_hlo_test.cc:32:    GTEST_SKIP() << "Returning tokens unsupported by MLIR";
./xla/tests/token_hlo_test.cc:47:    GTEST_SKIP() << "Returning tokens unsupported by MLIR";
./xla/tests/token_hlo_test.cc:65:    GTEST_SKIP() << "Returning tokens unsupported by MLIR";
./xla/tests/token_hlo_test.cc:217:    GTEST_SKIP() << "Invalid HLO unsupported by MLIR";
./xla/tests/token_hlo_test.cc:249:    GTEST_SKIP() << "Invalid HLO unsupported by MLIR";
./xla/tests/token_hlo_test.cc:292:    GTEST_SKIP() << "Invalid MHLO";
./xla/tests/triangular_solve_test.cc:459:      GTEST_SKIP() << "triggers a bug in cuda 12. b/287345077";
./xla/tests/tuple_test.cc:381:    GTEST_SKIP() << "Bitcast is not supported by the MLIR pipeline";
./xla/tools/multihost_hlo_runner/functional_hlo_runner_test.cc:68:    GTEST_SKIP() << "Requires " << kRequiredDeviceCount
./xla/tools/multihost_hlo_runner/functional_hlo_runner_test.cc:97:    GTEST_SKIP() << "Requires " << kRequiredDeviceCount
./xla/tools/multihost_hlo_runner/functional_hlo_runner_test.cc:129:    GTEST_SKIP() << "Requires " << kRequiredDeviceCount
./res1.txt:8:xla/service/gpu/ir_emitter_triton_test.cc:2478:    GTEST_SKIP() << "This test is for Ampere+ GPUs.";
./res1.txt:9:xla/service/gpu/tests/gpu_fused_mha_test.cc:447:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:10:xla/service/gpu/tests/gpu_fused_mha_test.cc:476:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:11:xla/service/gpu/tests/gpu_fused_mha_test.cc:504:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:12:xla/service/gpu/tests/gpu_fused_mha_test.cc:534:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:13:xla/service/gpu/tests/gpu_fused_mha_test.cc:565:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:14:xla/service/gpu/tests/gpu_fused_mha_test.cc:595:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:15:xla/service/gpu/tests/gpu_fused_mha_test.cc:625:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:16:xla/service/gpu/tests/gpu_fused_mha_test.cc:653:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:17:xla/service/gpu/tests/gpu_fused_mha_test.cc:677:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:18:xla/service/gpu/tests/gpu_fused_mha_test.cc:705:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:19:xla/service/gpu/tests/gpu_fused_mha_test.cc:1275:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:20:xla/service/gpu/tests/gpu_fused_mha_test.cc:1307:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:21:xla/service/gpu/tests/gpu_fused_mha_test.cc:1336:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:22:xla/service/gpu/tests/gpu_fused_mha_test.cc:1368:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:23:xla/service/gpu/tests/gpu_fused_mha_test.cc:1617:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:24:xla/service/gpu/tests/gpu_fused_mha_test.cc:1647:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:25:xla/service/gpu/tests/gpu_fused_mha_test.cc:1774:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:26:xla/service/gpu/tests/gpu_fused_mha_test.cc:2162:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
./res1.txt:27:xla/service/gpu/tests/gpu_fused_mha_test.cc:2191:      GTEST_SKIP() << "Fused MHA is supported with the Nvidia AMPERE+ GPUs and "
