diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,131 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-# Build with ccache if the package is present
-set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
-if(LLVM_CCACHE_BUILD)
-  find_program(CCACHE_PROGRAM ccache)
-  if(CCACHE_PROGRAM)
-      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
-      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
-      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
-          CACHE STRING "Parameters to pass through to ccache")
-
-      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
-      if (LLVM_CCACHE_MAXSIZE)
-        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
-      endif()
-      if (LLVM_CCACHE_DIR)
-        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
-      endif()
-      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
-  else()
-    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
-  endif()
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/CMakeLists.txt b/stablehlo/stablehlo/CMakeLists.txt
--- stablehlo/stablehlo/CMakeLists.txt
+++ stablehlo/stablehlo/CMakeLists.txt
@@ -15,6 +15,7 @@
 add_subdirectory(api)
 add_subdirectory(conversions)
 add_subdirectory(dialect)
+add_subdirectory(experimental)
 add_subdirectory(integrations)
 add_subdirectory(reference)
 add_subdirectory(tests)
diff --ruN a/stablehlo/stablehlo/api/PortableApi.h b/stablehlo/stablehlo/api/PortableApi.h
--- stablehlo/stablehlo/api/PortableApi.h
+++ stablehlo/stablehlo/api/PortableApi.h
@@ -27,7 +27,7 @@
 
 /// Return the current version for portable API.
 /// Increments on all meaningful changes to this file.
-inline int64_t getApiVersion() { return 5; }
+inline int64_t getApiVersion() { return 6; }
 
 // Get the current StableHLO version.
 //
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/convolution.mlir b/stablehlo/stablehlo/conversions/linalg/tests/convolution.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/convolution.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/convolution.mlir
@@ -37,8 +37,8 @@
     >,
     feature_group_count = 1 : i64,
     padding = dense<[[0, 0]]> : tensor<1x2xi64>,
-    rhs_dilation = dense<1> : tensor<1xi64>,
-    window_strides = dense<1> : tensor<1xi64>,
+    rhs_dilation = array<i64: 1>,
+    window_strides = array<i64: 1>,
     someattr
   } : (tensor<?x8x?xf32>, tensor<2x?x?xf32>) -> tensor<?x7x?xf32>
   func.return %0 : tensor<?x7x?xf32>
@@ -80,8 +80,8 @@
     >,
     feature_group_count = 1 : i64,
     padding = dense<[[0, 0], [0, 0]]> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<?x4x5x?xf32>, tensor<3x2x?x?xf32>) -> tensor<?x2x4x?xf32>
   func.return %0 : tensor<?x2x4x?xf32>
 }
@@ -223,8 +223,8 @@
     >,
     feature_group_count = 1 : i64,
     padding = dense<[[0, 0], [0, 0], [0, 0]]> : tensor<3x2xi64>,
-    rhs_dilation = dense<1> : tensor<3xi64>,
-    window_strides = dense<1> : tensor<3xi64>
+    rhs_dilation = array<i64: 1, 1, 1>,
+    window_strides = array<i64: 1, 1, 1>
   } : (tensor<?x8x8x8x?xf32>, tensor<2x2x2x?x?xf32>) -> tensor<?x7x7x7x?xf32>
   func.return %0 : tensor<?x7x7x7x?xf32>
 }
@@ -263,8 +263,8 @@
     >,
     feature_group_count = 1 : i64,
     padding = dense<0> : tensor<2x2xi64>,
-    rhs_dilation = dense<[2, 1]> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 2, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<1x4x5x2xf32>, tensor<2x2x2x3xf32>) -> tensor<1x2x4x3xf32>
   func.return %0 : tensor<1x2x4x3xf32>
 }
@@ -349,8 +349,8 @@
     >,
     feature_group_count = 2 : i64,
     padding = dense<0> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>,
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>,
     someattr} : (tensor<2x4x5x2xf32>, tensor<2x2x1x6xf32>) -> tensor<2x3x4x6xf32>
   func.return %0 : tensor<2x3x4x6xf32>
 }
@@ -390,8 +390,8 @@
     >,
     feature_group_count = 2 : i64,
     padding = dense<[[0, 0], [1, 1]]> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>,
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>,
     someattr} : (tensor<2x4x5x2xf32>, tensor<2x2x1x4xf32>) -> tensor<2x3x6x4xf32>
   func.return %0 : tensor<2x3x6x4xf32>
 }
@@ -438,8 +438,8 @@
     >,
     feature_group_count = 96 : i64,
     padding = dense<0> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<2> : tensor<2xi64>} : (tensor<1x113x113x96xf32>, tensor<3x3x1x96xf32>) -> tensor<1x56x56x96xf32>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 2, 2>} : (tensor<1x113x113x96xf32>, tensor<3x3x1x96xf32>) -> tensor<1x56x56x96xf32>
   func.return %0 : tensor<1x56x56x96xf32>
 }
 // CHECK-DAG:     %[[CST:.+]] = arith.constant 0.000000e+00 : f32
@@ -476,8 +476,8 @@
     >,
     feature_group_count = 96 : i64,
     padding = dense<[[1, 1], [2, 2]]> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<2> : tensor<2xi64>} : (tensor<1x113x113x96xf32>, tensor<3x3x1x96xf32>) -> tensor<1x57x58x96xf32>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 2, 2>} : (tensor<1x113x113x96xf32>, tensor<3x3x1x96xf32>) -> tensor<1x57x58x96xf32>
   func.return %0 : tensor<1x57x58x96xf32>
 }
 // CHECK-DAG:     %[[ZERO:.*]] = arith.constant 0.000000e+00 : f32
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/gather.mlir b/stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
@@ -14,7 +14,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     someattr
   } : (tensor<1x4x8xi32>, tensor<1x8x2xi32>) -> tensor<1x8x8xi32>
   func.return %res : tensor<1x8x8xi32>
@@ -59,7 +59,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     someattr
   } : (tensor<1x4x8xi32>, tensor<1x8x2xui32>) -> tensor<1x8x8xi32>
   func.return %res : tensor<1x8x8xi32>
@@ -84,7 +84,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 8>
   } : (tensor<1x4x8xui32>, tensor<1x8x2xi32>) -> tensor<1x8x8xui32>
   func.return %res : tensor<1x8x8xui32>
 }
@@ -107,7 +107,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[4, 2]> : tensor<2xi64>
+    slice_sizes = array<i64: 4, 2>
   } : (tensor<6x3xi32>, tensor<5x2xi32>) -> tensor<5x4x2xi32>
   func.return %res : tensor<5x4x2xi32>
 }
@@ -148,7 +148,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[2, 3, 4]> : tensor<3xi64>
+    slice_sizes = array<i64: 2, 3, 4>
   } : (tensor<?x?x?xi32>, tensor<5x2xi32>) -> tensor<2x3x4x5xi32>
   func.return %res : tensor<2x3x4x5xi32>
 }
@@ -198,7 +198,7 @@
       start_index_map = [3, 1, 2, 0]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 2, 1, 4]> : tensor<4xi64>
+    slice_sizes = array<i64: 1, 2, 1, 4>
   } : (tensor<6x3x2x7xi32>, tensor<5x4xi32>) -> tensor<5x2x4xi32>
   func.return %res : tensor<5x2x4xi32>
 }
@@ -256,7 +256,7 @@
       start_index_map = [0]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[3, 4]> : tensor<2xi64>
+    slice_sizes = array<i64: 3, 4>
   } : (tensor<?x?xi32>, tensor<5x2xi32>) -> tensor<3x4x5x2xi32>
   func.return %res : tensor<3x4x5x2xi32>
 }
@@ -297,7 +297,7 @@
       start_index_map = [0]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[3, 4]> : tensor<2xi64>
+    slice_sizes = array<i64: 3, 4>
   } : (tensor<?x?xi32>, tensor<?x?xi32>) -> tensor<3x4x?xi32>
   func.return %res : tensor<3x4x?xi32>
 }
@@ -338,7 +338,7 @@
       start_index_map = [0]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[3, 4]> : tensor<2xi64>
+    slice_sizes = array<i64: 3, 4>
   } : (tensor<*xi32>, tensor<?x?xi32>) -> tensor<?x?x?xi32>
   func.return %res : tensor<?x?x?xi32>
 }
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
@@ -651,7 +651,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
   : (tensor<?xf32>, tensor<4xf32>) -> tensor<?xf32>
   func.return %0 : tensor<?xf32>
 }
@@ -671,7 +671,7 @@
   ^bb0(%arg2: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg2 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
   : (tensor<?xf32>) -> tensor<?xf32>
   func.return %0 : tensor<?xf32>
 }
@@ -702,7 +702,7 @@
        {comparison_direction = #stablehlo<comparison_direction EQ>}
        : (tensor<f32>, tensor<f32>) -> tensor<i1>
     "stablehlo.return"(%3) : (tensor<i1>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
   : (tensor<?xcomplex<f32>>, tensor<?xcomplex<f32>>) -> tensor<?xi1>
   func.return %0 : tensor<?xi1>
 }
@@ -1089,8 +1089,8 @@
     stablehlo.return %9 : tensor<f32>
   }) {
     padding = dense<0> : tensor<4x2xi64>,
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<2x8x8x1xf32>, tensor<2x4x4x1xf32>, tensor<f32>) -> tensor<2x8x8x1xf32>
 
   return %0 : tensor<2x8x8x1xf32>
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/reduce.mlir b/stablehlo/stablehlo/conversions/linalg/tests/reduce.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/reduce.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/reduce.mlir
@@ -10,7 +10,7 @@
   ^bb0(%arg3: tensor<i32>, %arg4 : tensor<i32>):
     %1 = stablehlo.add %arg3, %arg4 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>, someattr} : (tensor<5x4xi32>, tensor<i32>) -> tensor<5xi32>
+  }) {dimensions = array<i64: 1>, someattr} : (tensor<5x4xi32>, tensor<i32>) -> tensor<5xi32>
   func.return %0 : tensor<5xi32>
 }
 // CHECK-DAG: %[[INIT:.*]] = tensor.extract %{{.*}} : tensor<i32>
@@ -43,7 +43,7 @@
   ^bb0(%arg3: tensor<i32>, %arg4 : tensor<i32>):
     %1 = stablehlo.add %arg3, %arg4 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>, someattr} : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
+  }) {dimensions = array<i64: 1>, someattr} : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
   func.return %0 : tensor<*xi32>
 }
 // CHECK: stablehlo.reduce
@@ -60,7 +60,7 @@
   ^bb0(%arg3: tensor<i32>, %arg4 : tensor<i32>):
     %1 = stablehlo.maximum %arg3, %arg4 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<5x4xi32>, tensor<i32>) -> tensor<4xi32>
+  }) {dimensions = array<i64: 0>} : (tensor<5x4xi32>, tensor<i32>) -> tensor<4xi32>
   func.return %0 : tensor<4xi32>
 }
 // CHECK-DAG: %[[INIT:.*]] = tensor.extract %{{.*}} : tensor<i32>
@@ -90,7 +90,7 @@
   ^bb0(%arg3: tensor<i32>, %arg4 : tensor<i32>):
     %1 = stablehlo.maximum %arg3, %arg4 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<5x4xi32>, tensor<i32>) -> tensor<?xi32>
+  }) {dimensions = array<i64: 0>} : (tensor<5x4xi32>, tensor<i32>) -> tensor<?xi32>
   func.return %0 : tensor<?xi32>
 }
 
@@ -112,7 +112,7 @@
   ^bb0(%arg1: tensor<f32>, %arg2: tensor<f32>):
     %1 = stablehlo.add %arg1, %arg2 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
+  }) {dimensions = array<i64: 1>} : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
   func.return %0 : tensor<1xf32>
 }
 // CHECK-DAG: %[[INIT_TENSOR:.*]] = tensor.empty()
@@ -137,7 +137,7 @@
   ^bb0(%arg2: tensor<i32>, %arg3: tensor<i32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<[0, 2]> : tensor<2xi64>} : (tensor<5x4x3xi32>, tensor<i32>) -> tensor<4xi32>
+  }) {dimensions = array<i64: 0, 2>} : (tensor<5x4x3xi32>, tensor<i32>) -> tensor<4xi32>
   func.return %0 : tensor<4xi32>
 }
 // CHECK-DAG: %[[INIT:.*]] = tensor.extract %{{.*}} : tensor<i32>
@@ -216,7 +216,7 @@
   ^bb0(%arg3: tensor<i32>, %arg4 : tensor<i32>):
     %1 = stablehlo.add %arg3, %arg4 : tensor<i32>
     "stablehlo.return"(%1) : (tensor<i32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<?x?xi32>, tensor<i32>) -> tensor<?xi32>
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xi32>, tensor<i32>) -> tensor<?xi32>
   func.return %0 : tensor<?xi32>
 }
 // CHECK-DAG: %[[INIT:.*]] = tensor.extract %{{.*}} : tensor<i32>
@@ -255,7 +255,7 @@
     %673 = "stablehlo.select"(%669, %arg3, %arg16) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
     %674 = "stablehlo.select"(%671, %672, %673) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%670, %674) : (tensor<i32>, tensor<i32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<9x2xi32>, tensor<9x2xi32>, tensor<i32>, tensor<i32>) -> (tensor<2xi32>, tensor<2xi32>)
+  }) {dimensions = array<i64: 0>} : (tensor<9x2xi32>, tensor<9x2xi32>, tensor<i32>, tensor<i32>) -> (tensor<2xi32>, tensor<2xi32>)
   func.return %res0, %res1 : tensor<2xi32>, tensor<2xi32>
 }
 // CHECK-DAG:    %[[CST0:.*]] = arith.constant -2147483648 : i32
@@ -316,7 +316,7 @@
     %1 = "stablehlo.select"(%0, %arg7, %arg9) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32>
     %2 = "stablehlo.select"(%0, %arg8, %arg10) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<i32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<128x10xf32>, tensor<128x10xi32>, tensor<f32>, tensor<i32>) ->(tensor<128xf32>, tensor<128xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<128x10xf32>, tensor<128x10xi32>, tensor<f32>, tensor<i32>) ->(tensor<128xf32>, tensor<128xi32>)
   func.return %res0, %res1 : tensor<128xf32>, tensor<128xi32>
 }
 // CHECK-DAG:        %[[CST0:.*]] = arith.constant 1.000000e+00 : f32
@@ -399,8 +399,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       someattr} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
   func.return %0 : tensor<1x8x8x64xf32>
 }
@@ -426,8 +426,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
   func.return %0 : tensor<1x8x8x64xf32>
 }
 // CHECK:         %[[WINDOW:.+]] = tensor.empty() : tensor<3x3xf32>
@@ -451,8 +451,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
   func.return %0 : tensor<1x8x8x64xf32>
 }
 // CHECK:         %[[WINDOW:.+]] = tensor.empty() : tensor<3x3xf32>
@@ -475,8 +475,8 @@
   ^bb0(%arg1: tensor<f32>, %arg2 : tensor<f32>):
     %2 = stablehlo.maximum %arg1, %arg2 : tensor<f32>
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>} : (tensor<1x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x64xf32>
   func.return %1 : tensor<1x8x8x64xf32>
 }
 
@@ -502,8 +502,8 @@
     %1 = stablehlo.add %arg2, %arg4 : tensor<f32>
     %2 = stablehlo.maximum %arg3, %arg5 : tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x17x17x64xf32>, tensor<1x17x17x64xf32>, tensor<f32>, tensor<f32>) -> (tensor<1x8x8x64xf32>, tensor<1x8x8x64xf32>)
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>} : (tensor<1x17x17x64xf32>, tensor<1x17x17x64xf32>, tensor<f32>, tensor<f32>) -> (tensor<1x8x8x64xf32>, tensor<1x8x8x64xf32>)
   func.return %0#0, %0#1 : tensor<1x8x8x64xf32>, tensor<1x8x8x64xf32>
 }
 
@@ -537,8 +537,8 @@
   ^bb0(%arg1: tensor<ui32>, %arg2: tensor<ui32>):
     stablehlo.return %arg1 : tensor<ui32>
   }) {
-    window_dimensions = dense<[1, 1]> : tensor<2xi64>,
-    window_strides = dense<[1, 1]> : tensor<2xi64>
+    window_dimensions = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<1x1xui32>, tensor<ui32>) -> tensor<1x1xui32>
   return %1 : tensor<1x1xui32>
 }
@@ -554,8 +554,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<?x?x?x?xf32>, tensor<f32>) -> tensor<?x?x?x?xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>} : (tensor<?x?x?x?xf32>, tensor<f32>) -> tensor<?x?x?x?xf32>
   func.return %0 : tensor<?x?x?x?xf32>
 }
 // CHECK-DAG:     %[[C0:.+]] = arith.constant 0 : index
@@ -593,8 +593,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 3, 1]> : tensor<5xi64>,
-      window_strides = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
   func.return %0 : tensor<1x8x8x8x64xf32>
 }
 // CHECK:         %[[WINDOW:.+]] = tensor.empty() : tensor<3x3x3xf32>
@@ -618,8 +618,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 3, 1]> : tensor<5xi64>,
-      window_strides = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
   func.return %0 : tensor<1x8x8x8x64xf32>
 }
 // CHECK:         %[[WINDOW:.+]] = tensor.empty() : tensor<3x3x3xf32>
@@ -643,8 +643,8 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {window_dimensions = dense<[1, 3, 3, 3, 1]> : tensor<5xi64>,
-      window_strides = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
+  }) {window_dimensions = array<i64: 1, 3, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x8x64xf32>
   func.return %0 : tensor<1x8x8x8x64xf32>
 }
 // CHECK:         %[[WINDOW:.+]] = tensor.empty() : tensor<3x3x3xf32>
@@ -668,9 +668,9 @@
   ^bb0(%arg2: tensor<f32>, %arg3 : tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<[1, 1, 1, 2, 1]> : tensor<5xi64>,
-      window_dimensions = dense<[1, 3, 3, 3, 1]> : tensor<5xi64>,
-      window_strides = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x16x64xf32>
+  }) {base_dilations = array<i64: 1, 1, 1, 2, 1>,
+      window_dimensions = array<i64: 1, 3, 3, 3, 1>,
+      window_strides = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x17x17x17x64xf32>, tensor<f32>) -> tensor<1x8x8x16x64xf32>
   func.return %0 : tensor<1x8x8x16x64xf32>
 }
 
@@ -690,7 +690,11 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<1> : tensor<2xi64>, padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>, window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<[1, 2]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x7xf32>
+  }) {base_dilations = array<i64: 1, 1>,
+      padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>,
+      window_dilations = array<i64: 1, 2>,
+      window_dimensions = array<i64: 1, 2>,
+      window_strides = array<i64: 2, 1>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x7xf32>
   func.return %0 : tensor<4x7xf32>
 }
 // CHECK: %[[INIT:.+]] = tensor.empty() : tensor<4x7xf32>
@@ -727,7 +731,11 @@
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     %2 = stablehlo.multiply %1, %c2 : tensor<f32>
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<1> : tensor<2xi64>, padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>, window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<[1, 2]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x7xf32>
+  }) {base_dilations = array<i64: 1, 1>,
+      padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>,
+      window_dilations = array<i64: 1, 2>,
+      window_dimensions = array<i64: 1, 2>,
+      window_strides = array<i64: 2, 1>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x7xf32>
   func.return %0 : tensor<4x7xf32>
 }
 
@@ -747,7 +755,10 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>, window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<[1, 2]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<3x7xf32>
+  }) {padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>,
+      window_dilations = array<i64: 1, 2>,
+      window_dimensions = array<i64: 1, 2>,
+      window_strides = array<i64: 2, 1>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<3x7xf32>
   func.return %0 : tensor<3x7xf32>
 }
 // CHECK: %[[PADVAL:.+]] = tensor.extract %[[ARG1]][] : tensor<f32>
@@ -764,7 +775,10 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<[2, 1]> : tensor<2xi64>, window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<[1, 2]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<3x4xf32>
+  }) {base_dilations = array<i64: 2, 1>,
+      window_dilations = array<i64: 1, 2>,
+      window_dimensions = array<i64: 1, 2>,
+      window_strides = array<i64: 2, 1>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<3x4xf32>
   func.return %0 : tensor<3x4xf32>
 }
 // CHECK: %[[PADVAL:.+]] = tensor.extract %[[ARG1]][] : tensor<f32>
@@ -782,7 +796,11 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<[2, 1]> : tensor<2xi64>, padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>, window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<[1, 2]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<4x7xf32>
+  }) {base_dilations = array<i64: 2, 1>,
+      padding = dense<[[0, 3], [1, 2]]> : tensor<2x2xi64>,
+      window_dilations = array<i64: 1, 2>,
+      window_dimensions = array<i64: 1, 2>,
+      window_strides = array<i64: 2, 1>} : (tensor<3x6xf32>, tensor<f32>) -> tensor<4x7xf32>
   func.return %0 : tensor<4x7xf32>
 }
 // CHECK: %[[PADVAL:.+]] = tensor.extract %[[ARG1]][] : tensor<f32>
@@ -799,7 +817,11 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {base_dilations = dense<> : tensor<0xi64>, padding = dense<> : tensor<0x2xi64>, window_dilations = dense<> : tensor<0xi64>, window_dimensions = dense<> : tensor<0xi64>, window_strides = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  }) {base_dilations = array<i64>,
+      padding = dense<> : tensor<0x2xi64>,
+      window_dilations = array<i64>,
+      window_dimensions = array<i64>,
+      window_strides = array<i64>} : (tensor<f32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
 // CHECK: linalg.generic {indexing_maps = [#[[MAP]], #[[MAP]], #[[MAP]]]
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
@@ -30,11 +30,12 @@
 /// Apply dilation and padding to the input of a convolution.
 Value applyConvolutionPadding(Location loc, Value input,
                               DenseIntElementsAttr padding,
-                              Attribute lhsDilation,
+                              DenseI64ArrayAttr lhsDilation,
                               llvm::ArrayRef<int64_t> dimMappings,
                               OpBuilder &rewriter) {
   SmallVector<int64_t> lhsDilationValues;
-  if (lhsDilation) lhsDilationValues = hlo::getI64Array(lhsDilation);
+  if (lhsDilation)
+    lhsDilationValues = llvm::to_vector(lhsDilation.asArrayRef());
   bool noPadding = !padding || isSplatValue(padding, 0);
   bool noDilation = !lhsDilation || hlo::isSplatArray(lhsDilationValues, 1);
   if (noPadding && noDilation) return input;
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgReduce.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgReduce.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgReduce.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgReduce.cpp
@@ -129,7 +129,7 @@
     }
     auto srcRank = cast<ShapedType>(adaptor.getInputs()[0].getType()).getRank();
 
-    SmallVector<int64_t> reductionDims = op.getDimensions();
+    SmallVector<int64_t> reductionDims = llvm::to_vector(op.getDimensions());
 
     SmallVector<Type> resultTypes;
     if (failed(typeConverter->convertTypes(op.getResultTypes(), resultTypes)))
@@ -226,7 +226,7 @@
                                          "unsupported reduce (noop or empty)");
     }
 
-    auto reductionDims = op.getDimensions();
+    auto reductionDims = llvm::to_vector(op.getDimensions());
     // stablehlo.reduce doesn't specify the order of the reduction dimensions.
     llvm::sort(reductionDims);
 
@@ -346,17 +346,17 @@
 
     llvm::SmallVector<int64_t> baseDilations;
     if (op.getBaseDilations()) {
-      baseDilations = *op.getBaseDilations();
+      baseDilations = llvm::to_vector(*op.getBaseDilations());
     }
 
     llvm::SmallVector<int64_t> windowStrides(windowDimensions.size(), 1);
     if (op.getWindowStrides()) {
-      windowStrides = *op.getWindowStrides();
+      windowStrides = llvm::to_vector(*op.getWindowStrides());
     }
 
     llvm::SmallVector<int64_t> windowDilations(windowDimensions.size(), 1);
     if (op.getWindowDilations()) {
-      windowDilations = *op.getWindowDilations();
+      windowDilations = llvm::to_vector(*op.getWindowDilations());
     }
 
     auto rank = static_cast<int64_t>(windowDimensions.size());
diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/binary.mlir b/stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
@@ -109,7 +109,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 2, 5]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 2, 5>
   } : (tensor<3x4x5xi32>, tensor<3x2xi32>) -> tensor<3x2x5xi32>
   return %0 : tensor<3x2x5xi32>
 }
@@ -127,7 +127,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 2, 5]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 2, 5>
   } : (tensor<*xi32>, tensor<3x2xi32>) -> tensor<*xi32>
   return %0 : tensor<*xi32>
 }
@@ -182,7 +182,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
+  }) {dimensions = array<i64: 1>} : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32>
   return %0 : tensor<1xf32>
 }
 
@@ -194,7 +194,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<5x4xf32>, tensor<f32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<5x4xf32>, tensor<f32>) -> tensor<4xf32>
   return %0 : tensor<4xf32>
 }
 
diff --ruN a/stablehlo/stablehlo/dialect/AssemblyFormat.cpp b/stablehlo/stablehlo/dialect/AssemblyFormat.cpp
--- stablehlo/stablehlo/dialect/AssemblyFormat.cpp
+++ stablehlo/stablehlo/dialect/AssemblyFormat.cpp
@@ -255,46 +255,6 @@
 //===----------------------------------------------------------------------===//
 // Attribute Printers and Parsers
 //===----------------------------------------------------------------------===//
-
-void printDenseI64Array(OpAsmPrinter& p, Operation* op,
-                        DenseIntElementsAttr attr) {
-  if (attr.getType().getRank() != 1)
-    llvm::report_fatal_error("printDenseI64Array only supports rank-1 arrays");
-  auto values = llvm::to_vector(attr.getValues<int64_t>());
-  DenseI64ArrayAttr arrayAttr =
-      DenseI64ArrayAttr::get(op->getContext(), values);
-  arrayAttr.print(p);
-}
-
-ParseResult parseDenseI64Array(OpAsmParser& parser,
-                               DenseIntElementsAttr& attr) {
-  DenseI64ArrayAttr arrayAttr = DenseI64ArrayAttr::parse(parser, Type{})
-                                    .dyn_cast_or_null<DenseI64ArrayAttr>();
-  if (!arrayAttr) return failure();
-
-  ArrayRef<int64_t> data = arrayAttr.asArrayRef();
-  RankedTensorType type =
-      RankedTensorType::get(data.size(), parser.getBuilder().getI64Type());
-  attr = DenseIntElementsAttr::get(type, data);
-  return success();
-}
-
-void printI64DenseArrayOrElements1D(OpAsmPrinter& p, Operation* op,
-                                    Attribute attr) {
-  if (auto elems = dyn_cast<DenseIntElementsAttr>(attr)) {
-    printDenseI64Array(p, op, elems);
-    return;
-  }
-  dyn_cast<DenseI64ArrayAttr>(attr).print(p);
-}
-
-ParseResult parseI64DenseArrayOrElements1D(OpAsmParser& parser,
-                                           Attribute& attr) {
-  if ((attr = DenseI64ArrayAttr::parse(parser, Type{}))) {
-    return success();
-  }
-  return failure();
-}
 
 void printSliceRanges(OpAsmPrinter& p, Operation* op,
                       ArrayRef<int64_t> startIndices,
diff --ruN a/stablehlo/stablehlo/dialect/AssemblyFormat.h b/stablehlo/stablehlo/dialect/AssemblyFormat.h
--- stablehlo/stablehlo/dialect/AssemblyFormat.h
+++ stablehlo/stablehlo/dialect/AssemblyFormat.h
@@ -174,31 +174,6 @@
 // Attribute Printers and Parsers
 //===----------------------------------------------------------------------===//
 
-// DenseI64Array - Used to print DenseIntElementsAttrs that are verified to have
-// rank 1 as an i64 array without needing the dense specifier or type specifier.
-//
-//   Generic:
-//     { dense<[1, 2]> : tensor<2xi64> }
-//   Custom:
-//     [1, 2]
-void printDenseI64Array(OpAsmPrinter& p, Operation* op,
-                        DenseIntElementsAttr attr);
-
-ParseResult parseDenseI64Array(OpAsmParser& parser, DenseIntElementsAttr& attr);
-
-// I64DenseArrayOrElements1D - Used to print an attr that can be either
-// I64ElementsAttr (DenseIntElementsAttr) or DenseI64ArrayAttr.
-//
-//   Dense elements:
-//     { dense<[1, 2]> : tensor<2xi64> }
-//   Array:
-//     { array<i64: 1, 2> }
-void printI64DenseArrayOrElements1D(OpAsmPrinter& p, Operation* op,
-                                    Attribute attr);
-
-ParseResult parseI64DenseArrayOrElements1D(OpAsmParser& parser,
-                                           Attribute& attr);
-
 // SliceRanges - Used to print multi-dimensional ranges for slice.
 void printSliceRanges(OpAsmPrinter& p, Operation* op,
                       ArrayRef<int64_t> startIndices,
diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -617,18 +617,5 @@
       false);
 }
 
-SmallVector<bool> getBoolArray(Attribute attr) {
-  if (!attr) return {};
-  if (auto elements = attr.dyn_cast<DenseIntOrFPElementsAttr>())
-    return llvm::to_vector(elements.getValues<bool>());
-  if (auto array = attr.dyn_cast<DenseBoolArrayAttr>()) {
-    return SmallVector<bool>(array.asArrayRef());
-  }
-  llvm::report_fatal_error(
-      "called getBoolArray on Attribute that was neither a "
-      "DenseIntOrFPElementsAttr or a DenseBoolArrayAttr",
-      false);
-}
-
 }  // namespace hlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -57,11 +57,6 @@
 }
 
 // Checks whether every position in the given array contains the given value.
-// This is especially useful for dealing with instances of
-// I64DenseArrayOrElements1DAttr, which returns a SmallVector<int64_t> as its
-// value no matter what actual attribute is backing it.
-// TODO(#1578): Remove this code once all uses of I64DenseArrayOrElements1DAttr
-// have been removed.
 bool isSplatArray(ArrayRef<int64_t> arr, int64_t val);
 
 // Returns a vector of the int64 values in a I64DenseArrayOrElements1DAttr.
@@ -70,13 +65,6 @@
 // TODO(#1578): Remove this code once all uses of I64DenseArrayOrElements1DAttr
 // have been removed.
 SmallVector<int64_t> getI64Array(Attribute);
-
-// Returns a vector of the bool values in a BoolDenseArrayOrElementsAttr.
-// Such an Attr can be backed by either a DenseIntOrFPElementsAttr or
-// a DenseBoolArrayAttr.
-// TODO(#1578): Remove this code once all uses of BoolDenseArrayOrElementsAttr
-// have been removed.
-SmallVector<bool> getBoolArray(Attribute);
 
 //  Verifies that the two types have compatible shape with bounds but allows
 //  different element types.
diff --ruN a/stablehlo/stablehlo/dialect/StablehloAttrs.td b/stablehlo/stablehlo/dialect/StablehloAttrs.td
--- stablehlo/stablehlo/dialect/StablehloAttrs.td
+++ stablehlo/stablehlo/dialect/StablehloAttrs.td
@@ -179,35 +179,18 @@
   let hasCustomAssemblyFormat = 1;
 }
 
-def StableHLO_BoolElementsAttr :
-    ElementsAttrBase<
-      And<[CPred<"$_self.isa<::mlir::DenseIntOrFPElementsAttr>()">,
-           CPred<"$_self.cast<::mlir::DenseIntOrFPElementsAttr>().getType().getElementType().isInteger(1)">]>,
-      "constant boolean vector/tensor attribute"> {
-  let storageType = [{ ::mlir::DenseElementsAttr }];
-  let returnType = [{ ::mlir::DenseElementsAttr }];
-
-  let convertFromStorage = "$_self";
-}
-
-def BoolDenseArrayOrElementsAttr : Attr<Or<[DenseBoolArrayAttr.predicate, StableHLO_BoolElementsAttr.predicate]>, "either a DenseBoolArrayAttr or a StableHLO_BoolElementsAttr"> {
-  let storageType = "mlir::Attribute";
-  let returnType = "SmallVector<bool>";
-  let convertFromStorage = "hlo::getBoolArray($_self)";
-}
-
 def StableHLO_ConvolutionAttributes {
   dag attributes = (ins
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$window_strides,
+    OptionalAttr<DenseI64ArrayAttr>:$window_strides,
     // Default value: two zeros for each of the spatial dimension.
     OptionalAttr<I64ElementsAttr>:$padding,
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$lhs_dilation,
+    OptionalAttr<DenseI64ArrayAttr>:$lhs_dilation,
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$rhs_dilation,
+    OptionalAttr<DenseI64ArrayAttr>:$rhs_dilation,
     // Default value: false for each of the spatial dimension.
-    OptionalAttr<BoolDenseArrayOrElementsAttr>:$window_reversal,
+    OptionalAttr<DenseBoolArrayAttr>:$window_reversal,
     StableHLO_ConvDimensionNumbers:$dimension_numbers,
     I64Attr:$feature_group_count,
     I64Attr:$batch_group_count,
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -1401,10 +1401,10 @@
 // Builder that takes a constructor for its region and infers result types
 void ReduceWindowOp::build(
     OpBuilder& odsBuilder, OperationState& odsState, ValueRange inputs,
-    ValueRange init_values, DenseIntElementsAttr window_dimensions,
-    /*optional*/ DenseIntElementsAttr window_strides,
-    /*optional*/ DenseIntElementsAttr base_dilations,
-    /*optional*/ DenseIntElementsAttr window_dilations,
+    ValueRange init_values, DenseI64ArrayAttr window_dimensions,
+    /*optional*/ DenseI64ArrayAttr window_strides,
+    /*optional*/ DenseI64ArrayAttr base_dilations,
+    /*optional*/ DenseI64ArrayAttr window_dilations,
     /*optional*/ DenseIntElementsAttr padding,
     function_ref<void(OpBuilder&, Location, ValueRange)> bodyBuilder) {
   odsState.addOperands(inputs);
@@ -1631,7 +1631,7 @@
         parser.parseKeyword("reducer"))
       return failure();
     OpBuilder builder(parser.getBuilder().getContext());
-    result.addAttribute("dimensions", builder.getI64TensorAttr(dimensions));
+    result.addAttribute("dimensions", builder.getDenseI64ArrayAttr(dimensions));
 
     // Parse the "reducer" region now.
     SmallVector<OpAsmParser::UnresolvedOperand, 2> reducerOperands;
@@ -1769,7 +1769,7 @@
   // dimension attribute.
   result.addTypes(reduceOpFnType.getResults());
   result.location = innerOp->getLoc();
-  result.addAttribute("dimensions", builder.getI64TensorAttr(dimensions));
+  result.addAttribute("dimensions", builder.getDenseI64ArrayAttr(dimensions));
   return success();
 }
 
@@ -2423,10 +2423,8 @@
 
 using mlir::hlo::parseComplexOpType;
 using mlir::hlo::parseCustomCallTarget;
-using mlir::hlo::parseDenseI64Array;
 using mlir::hlo::parseDotDimensionNumbers;
 using mlir::hlo::parseExponentMantissa;
-using mlir::hlo::parseI64DenseArrayOrElements1D;
 using mlir::hlo::parsePairwiseOpType;
 using mlir::hlo::parseSameOperandsAndResultType;
 using mlir::hlo::parseSelectOpType;
@@ -2436,10 +2434,8 @@
 using mlir::hlo::parseVariadicSameOperandsAndResultType;
 using mlir::hlo::printComplexOpType;
 using mlir::hlo::printCustomCallTarget;
-using mlir::hlo::printDenseI64Array;
 using mlir::hlo::printDotDimensionNumbers;
 using mlir::hlo::printExponentMantissa;
-using mlir::hlo::printI64DenseArrayOrElements1D;
 using mlir::hlo::printPairwiseOpType;
 using mlir::hlo::printSameOperandsAndResultType;
 using mlir::hlo::printSelectOpType;
@@ -3109,11 +3105,11 @@
 }  // namespace
 
 void printWindowAttributes(OpAsmPrinter& p, Operation* /*op*/,
-                           std::optional<Attribute> windowStrides,
+                           std::optional<DenseI64ArrayAttr> windowStrides,
                            std::optional<DenseIntElementsAttr> padding,
-                           std::optional<Attribute> lhsDilation,
-                           std::optional<Attribute> rhsDilation,
-                           std::optional<Attribute> windowReversal) {
+                           std::optional<DenseI64ArrayAttr> lhsDilation,
+                           std::optional<DenseI64ArrayAttr> rhsDilation,
+                           std::optional<DenseBoolArrayAttr> windowReversal) {
   using pair_t = std::pair<Attribute, StringRef>;
   std::array<pair_t, 5> printedAttributes = {{
       {windowStrides ? *windowStrides : nullptr, "stride"},
@@ -3134,20 +3130,23 @@
     if (attr.second == "pad") {
       printWindowPadding(p, attr.first.dyn_cast<DenseIntElementsAttr>());
     } else if (attr.second == "reverse") {
-      llvm::interleaveComma(hlo::getBoolArray(attr.first), p);
+      llvm::interleaveComma(
+          attr.first.dyn_cast<DenseBoolArrayAttr>().asArrayRef(), p);
     } else {
-      llvm::interleaveComma(hlo::getI64Array(attr.first), p);
+      llvm::interleaveComma(
+          attr.first.dyn_cast<DenseI64ArrayAttr>().asArrayRef(), p);
     }
 
     p << ']';
   });
 }
 
-ParseResult parseWindowAttributes(OpAsmParser& parser, Attribute& windowStrides,
+ParseResult parseWindowAttributes(OpAsmParser& parser,
+                                  DenseI64ArrayAttr& windowStrides,
                                   DenseIntElementsAttr& padding,
-                                  Attribute& lhsDilation,
-                                  Attribute& rhsDilation,
-                                  Attribute& windowReversal) {
+                                  DenseI64ArrayAttr& lhsDilation,
+                                  DenseI64ArrayAttr& rhsDilation,
+                                  DenseBoolArrayAttr& windowReversal) {
   StringRef attributeName;
 
   llvm::StringSet<> allowedAttributeNames{
@@ -3202,13 +3201,12 @@
                                          int64Parser))
         return failure();
       if (attributeName == "reverse") {
-        auto ty = RankedTensorType::get({static_cast<int64_t>(values.size())},
-                                        parser.getBuilder().getIntegerType(1));
         auto boolVector = llvm::to_vector<4>(
             llvm::map_range(values, [](int64_t v) { return v != 0; }));
-        windowReversal = DenseElementsAttr::get(ty, boolVector);
+        windowReversal =
+            DenseBoolArrayAttr::get(parser.getContext(), boolVector);
       } else {
-        auto attr = parser.getBuilder().getI64TensorAttr(values);
+        auto attr = parser.getBuilder().getDenseI64ArrayAttr(values);
 
         if (attributeName == "stride") {
           windowStrides = attr;
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h
--- stablehlo/stablehlo/dialect/StablehloOps.h
+++ stablehlo/stablehlo/dialect/StablehloOps.h
@@ -88,17 +88,18 @@
 
 // Custom formatting for convolution window attributes.
 void printWindowAttributes(OpAsmPrinter &p, Operation *op,
-                           std::optional<Attribute> windowStrides,
+                           std::optional<DenseI64ArrayAttr> windowStrides,
                            std::optional<DenseIntElementsAttr> padding,
-                           std::optional<Attribute> lhsDilation,
-                           std::optional<Attribute> rhsDilation,
-                           std::optional<Attribute> windowReversal);
+                           std::optional<DenseI64ArrayAttr> lhsDilation,
+                           std::optional<DenseI64ArrayAttr> rhsDilation,
+                           std::optional<DenseBoolArrayAttr> windowReversal);
 
-ParseResult parseWindowAttributes(OpAsmParser &parser, Attribute &windowStrides,
+ParseResult parseWindowAttributes(OpAsmParser &parser,
+                                  DenseI64ArrayAttr &windowStrides,
                                   DenseIntElementsAttr &padding,
-                                  Attribute &lhsDilation,
-                                  Attribute &rhsDilation,
-                                  Attribute &windowReversal);
+                                  DenseI64ArrayAttr &lhsDilation,
+                                  DenseI64ArrayAttr &rhsDilation,
+                                  DenseBoolArrayAttr &windowReversal);
 
 }  // end namespace stablehlo
 }  // end namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -1472,7 +1472,7 @@
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs, /*reduce_i1*/
     Variadic<HLO_Tensor>:$init_values, /*reduce_i2*/
-    I64DenseArrayOrElements1DAttr:$dimensions /*reduce_i3*/
+    DenseI64ArrayAttr:$dimensions /*reduce_i3*/
   );
   let regions = (region SizedRegion<1>:$body /*reduce_i4*/);
 
@@ -1870,7 +1870,7 @@
   }];
   let arguments = (ins
     HLO_Tensor:$operand /*broadcast_in_dim_i1*/,
-    I64DenseArrayOrElements1DAttr:$broadcast_dimensions /*broadcast_in_dim_i2*/
+    DenseI64ArrayAttr:$broadcast_dimensions /*broadcast_in_dim_i2*/
   );
 
   let results = (outs HLO_StaticShapeTensor);
@@ -1878,7 +1878,7 @@
   let hasVerifier = 1;
 
   let assemblyFormat = [{
-    $operand `,` `dims` `=` custom<I64DenseArrayOrElements1D>($broadcast_dimensions)
+    $operand `,` `dims` `=` $broadcast_dimensions
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -1903,9 +1903,9 @@
   let arguments = (ins
     HLO_Tensor:$operand,
     HLO_DimensionTensor:$output_dimensions,
-    I64DenseArrayOrElements1DAttr:$broadcast_dimensions,
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$known_expanding_dimensions,
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$known_nonexpanding_dimensions
+    DenseI64ArrayAttr:$broadcast_dimensions,
+    OptionalAttr<DenseI64ArrayAttr>:$known_expanding_dimensions,
+    OptionalAttr<DenseI64ArrayAttr>:$known_nonexpanding_dimensions
   );
 
   let results = (outs HLO_Tensor);
@@ -1913,7 +1913,7 @@
   let builders = [
       OpBuilder<(ins
         "Type":$result_type, "Value":$operand, "Value":$output_dimensions,
-        "Attribute":$broadcast_dimensions), [{
+        "DenseI64ArrayAttr":$broadcast_dimensions), [{
       build($_builder, $_state, result_type, operand, output_dimensions,
           broadcast_dimensions, /*known_expanding_dimensions=*/{},
           /*known_nonexpanding_dimensions=*/{});
@@ -1923,7 +1923,7 @@
   let hasVerifier = 1;
 
   let assemblyFormat = [{
-    $operand `,` $output_dimensions `,` `dims` `=` custom<I64DenseArrayOrElements1D>($broadcast_dimensions)
+    $operand `,` $output_dimensions `,` `dims` `=` $broadcast_dimensions
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -2126,7 +2126,7 @@
   let extraClassDeclaration = [{
     bool hasWindowReversal() {
       auto reversal = getWindowReversalAttr();
-      return reversal && llvm::any_of(hlo::getBoolArray(reversal), [](bool v) { return v; });
+      return reversal && llvm::any_of(reversal.asArrayRef(), [](bool v) { return v; });
     }
   }];
 
@@ -2396,7 +2396,7 @@
     HLO_Tensor:$operand /*gather_i1*/,
     HLO_IntTensor:$start_indices /*gather_i2*/,
     StableHLO_GatherDimensionNumbers:$dimension_numbers /*gather_i3, gather_i4, gather_i5, gather_i6*/,
-    I64DenseArrayOrElements1DAttr:$slice_sizes /*gather_i7*/,
+    DenseI64ArrayAttr:$slice_sizes /*gather_i7*/,
     DefaultValuedOptionalAttr<BoolAttr, "false">:$indices_are_sorted /*gather_i8*/
   );
 
@@ -2455,7 +2455,7 @@
   }];
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs /*map_i1*/,
-    I64DenseArrayOrElements1DAttr:$dimensions /*map_i2*/
+    DenseI64ArrayAttr:$dimensions /*map_i2*/
   );
   let regions = (region SizedRegion<1>:$computation /*map_i3*/);
   let results = (outs HLO_Tensor);
@@ -2616,8 +2616,8 @@
     HLO_Tensor:$operand, /*select_and_scatter_i1*/
     HLO_Tensor:$source, /*select_and_scatter_i2*/
     HLO_Tensor:$init_value, /*select_and_scatter_i3*/
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$window_dimensions, /*select_and_scatter_i4*/
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$window_strides, /*select_and_scatter_i5*/
+    OptionalAttr<DenseI64ArrayAttr>:$window_dimensions, /*select_and_scatter_i4*/
+    OptionalAttr<DenseI64ArrayAttr>:$window_strides, /*select_and_scatter_i5*/
     OptionalAttr<I64ElementsAttr>:$padding /*select_and_scatter_i6*/
   );
 
@@ -2881,13 +2881,13 @@
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs /*reduce_window_i1*/,
     Variadic<HLO_Tensor>:$init_values /*reduce_window_i2*/,
-    I64DenseArrayOrElements1DAttr:$window_dimensions /*reduce_window_i3*/,
+    DenseI64ArrayAttr:$window_dimensions /*reduce_window_i3*/,
     // If strides or dilations attributes are missing then the default value is
     // one for each of the operand dimensions. Similarly, padding values are zero
     // for both low and high in each of the dimensions, if not specified.
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$window_strides /*reduce_window_i4*/,
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$base_dilations /*reduce_window_i5*/,
-    OptionalAttr<I64DenseArrayOrElements1DAttr>:$window_dilations /*reduce_window_i6*/,
+    OptionalAttr<DenseI64ArrayAttr>:$window_strides /*reduce_window_i4*/,
+    OptionalAttr<DenseI64ArrayAttr>:$base_dilations /*reduce_window_i5*/,
+    OptionalAttr<DenseI64ArrayAttr>:$window_dilations /*reduce_window_i6*/,
     OptionalAttr<I64ElementsAttr>:$padding /*reduce_window_i7*/
   );
 
@@ -2902,10 +2902,10 @@
   let builders = [
     OpBuilder<(ins "Type":$result_type, "Value":$operand,
       "Value":$init_value,
-      "DenseIntElementsAttr":$window_dimensions,
-      "DenseIntElementsAttr":$window_strides,
-      "DenseIntElementsAttr":$base_dilations,
-      "DenseIntElementsAttr":$window_dilations,
+      "DenseI64ArrayAttr":$window_dimensions,
+      "DenseI64ArrayAttr":$window_strides,
+      "DenseI64ArrayAttr":$base_dilations,
+      "DenseI64ArrayAttr":$window_dilations,
       "DenseIntElementsAttr":$padding),
     [{
       build($_builder, $_state, TypeRange(result_type), ValueRange(operand),
@@ -2914,10 +2914,10 @@
     }]>,
     OpBuilder<(ins "ValueRange":$operands,
       "ValueRange":$init_values,
-      "DenseIntElementsAttr":$window_dimensions,
-      "DenseIntElementsAttr":$window_strides,
-      "DenseIntElementsAttr":$base_dilations,
-      "DenseIntElementsAttr":$window_dilations,
+      "DenseI64ArrayAttr":$window_dimensions,
+      "DenseI64ArrayAttr":$window_strides,
+      "DenseI64ArrayAttr":$base_dilations,
+      "DenseI64ArrayAttr":$window_dilations,
       "DenseIntElementsAttr":$padding,
       "function_ref<void(OpBuilder &, Location, ValueRange)>":$bodyBuilder
     )>,
diff --ruN a/stablehlo/stablehlo/experimental/BUILD.bazel b/stablehlo/stablehlo/experimental/BUILD.bazel
--- stablehlo/stablehlo/experimental/BUILD.bazel
+++ stablehlo/stablehlo/experimental/BUILD.bazel
@@ -0,0 +1,114 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+cc_library(
+    name = "experimental_base",
+    srcs = [
+        "dialect/Base.cpp",
+    ],
+    hdrs = [
+        "dialect/Base.h",
+    ],
+    deps = [
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "experimental_stablehlo_ops",
+    srcs = [
+        "dialect/StablehloOps.cpp",
+    ],
+    hdrs = [
+        "dialect/StablehloOps.h",
+    ],
+    deps = [
+        ":experimental_base",
+        "//:stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+gentbl_cc_library(
+    name = "experimental_stablehlo_pass_inc_gen",
+    tbl_outs = [
+        (
+            [
+                "-gen-pass-decls",
+            ],
+            "transforms/Passes.h.inc",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "transforms/Passes.td",
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+cc_library(
+    name = "experimental_stablehlo_passes",
+    srcs = [
+        "transforms/StablehloCanonicalizeDynamism.cpp",
+        "transforms/StablehloRefineShapes.cpp",
+    ],
+    hdrs = [
+        "transforms/Passes.h",
+    ],
+    deps = [
+        ":experimental_stablehlo_ops",
+        ":experimental_stablehlo_pass_inc_gen",
+        "//:base",
+        "//:chlo_ops",
+        "//:stablehlo_ops",
+        "//:stablehlo_ops_inc_gen",
+        "//:stablehlo_passes",
+        "//:stablehlo_type_inference",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InferTypeOpInterface",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_binary(
+    name = "experimental-stablehlo-opt",
+    srcs = [
+        "tools/StablehloOptMain.cpp",
+    ],
+    deps = [
+        ":experimental_stablehlo_passes",
+        "//:interpreter_ops",
+        "//:register",
+        "//:stablehlo_passes",
+        "//:test_utils",
+        "//:tosa_passes",
+        "@llvm-project//mlir:AllExtensions",
+        "@llvm-project//mlir:AllPassesAndDialects",
+        "@llvm-project//mlir:MlirOptLib",
+        "@llvm-project//mlir:TosaDialect",
+    ],
+)
diff --ruN a/stablehlo/stablehlo/experimental/CMakeLists.txt b/stablehlo/stablehlo/experimental/CMakeLists.txt
--- stablehlo/stablehlo/experimental/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/CMakeLists.txt
@@ -0,0 +1,18 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_subdirectory(dialect)
+add_subdirectory(tests)
+add_subdirectory(tools)
+add_subdirectory(transforms)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.cpp b/stablehlo/stablehlo/experimental/dialect/Base.cpp
--- stablehlo/stablehlo/experimental/dialect/Base.cpp
+++ stablehlo/stablehlo/experimental/dialect/Base.cpp
@@ -0,0 +1,39 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/Base.h"
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
+}  // namespace hlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.h b/stablehlo/stablehlo/experimental/dialect/Base.h
--- stablehlo/stablehlo/experimental/dialect/Base.h
+++ stablehlo/stablehlo/experimental/dialect/Base.h
@@ -0,0 +1,35 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+
+#include "llvm/ADT/ArrayRef.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
+}  // namespace hlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
diff --ruN a/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt b/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
--- stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
@@ -0,0 +1,42 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_mlir_library(ExperimentalStablehloBase
+  PARTIAL_SOURCES_INTENDED
+  Base.cpp
+
+  LINK_LIBS PUBLIC
+  MLIRIR
+)
+
+add_mlir_dialect_library(ExperimentalStablehloOps
+  PARTIAL_SOURCES_INTENDED
+  StablehloOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  ExperimentalStablehloBase
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
+)
+
+target_include_directories(ExperimentalStablehloOps INTERFACE
+  $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
+  $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp b/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
@@ -0,0 +1,641 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+
+#include <cstdint>
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getAttr("rng_algorithm").cast<RngAlgorithmAttr>().getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() || kType.getRank() != 0 ||
+      !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+LogicalResult TopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 1)
+    return op_.emitError("expects size(operands) = 1");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "mhlo.topk")
+    return op_.emitError() << "expects @mhlo.topk";
+
+  auto operand = op_.getInputs()[0];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  if (!topkAttributes) {
+    return op_.emitError()
+           << "mhlo.attributes missing or not a dictionary attribute";
+  }
+
+  IntegerAttr k_attr = topkAttributes.get("k").dyn_cast_or_null<IntegerAttr>();
+  if (!k_attr) {
+    return op_.emitError() << "mhlo.attributes.k not present or not an integer";
+  }
+  int64_t k = k_attr.getInt();
+
+  // mhlo.topk_c5
+  if (k < 0) return op_.emitError() << "expects k >= 0";
+
+  // mhlo.topk_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // mhlo.topk_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // mhlo.topk_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // mhlo.topk_c1 && mhlo.topk_c2
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] = k;
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension, and "
+                              "that the last dimension of the values shape "
+                              "has a size k";
+
+  // mhlo.topk_c3
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // mhlo.topk_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+int64_t TopKOpAdaptor::getK() {
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  return topkAttributes.get("k").cast<mlir::IntegerAttr>().getInt();
+}
+
+bool TopKOpAdaptor::getLargest() {
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  IntegerAttr largest =
+      topkAttributes.get("largest").dyn_cast_or_null<mlir::IntegerAttr>();
+
+  return (!largest) ? true : largest.getInt();
+}
+
+std::optional<TopKOpAdaptor> getTopKOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "mhlo.topk") return {};
+  return TopKOpAdaptor(op);
+}
+
+LogicalResult TanOpAdaptor::verify() {
+  if (op_->getNumOperands() != 1)
+    return op_.emitError("expects size(operands) = 1");
+  if (op_->getNumResults() != 1)
+    return op_.emitError("expects size(results) = 1");
+  auto operand = op_.getInputs()[0];
+  auto result = op_.getResults()[0];
+
+  // tan_c1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  auto resultType = result.getType().dyn_cast<ShapedType>();
+  if (!hlo::isCompatibleForHloTypeInference(operandType, resultType))
+    return op_.emitError()
+           << "expects operand and result to have compatible shapes";
+  return success();
+}
+
+TypedValue<ShapedType> TanOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<TanOpAdaptor> getTanOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "mhlo.tan") return {};
+  return TanOpAdaptor(op);
+}
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.h b/stablehlo/stablehlo/experimental/dialect/StablehloOps.h
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.h
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.h
@@ -0,0 +1,347 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/Base.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+///////////////////
+// MHLO Op Wrappers
+// There are some ops in MHLO which have experimental support in StableHLO
+// programs by representing them as custom_calls with the target `mhlo.op_name`.
+// The level of support of these ops is similar to the other custom_calls in
+// this file. Generally these ops will be added to StableHLO and their
+// experimental support can be deprecated in favor of op's type inference.
+///////////////////
+
+// The TopK experiment provides a StableHLO adapter to MHLO TopKOp.
+// In the future we expect stablehlo.top_k to be added which will use the same
+// refinement rules.
+//
+// Within this experiment, TopKOp is represented via the serialized MHLO
+// `stablehlo.custom_call @mhlo.topk` custom call.
+//
+// The semantics of experimental TopKOp are inherited from the semantics of
+// mhlo.topk.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | constant of type si64                        |
+// | (I3)  | `largest`       | constant of type i1                          |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `shape(values)[-1] = k`
+// * (C3) `element_type(values) = element_type(operand)`
+// * (C4) `shape(indices) = shape(values)`
+// * (C5) `k >= 0`
+//
+class TopKOpAdaptor {
+ public:
+  TopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+  int64_t getK();
+  bool getLargest();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a TopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "mhlo.topk".
+std::optional<TopKOpAdaptor> getTopKOp(CustomCallOp op);
+
+// The TanOp experiment provides a StableHLO adapter to MHLO TanOp.
+// In the future we expect stablehlo.tan to be added which will use the same
+// refinement rules.
+//
+// Within this experiment, TanOp is represented via the serialized MHLO
+// `stablehlo.custom_call @mhlo.tan` custom call. S
+//
+// Semantics of experimental TanOp are inherited from semantics of mhlo.tan.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of floating-point or complex type     |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `result`       | tensor of floating-point or complex type |
+//
+// #### Constraints
+//
+// * (C1) baseline_type(operand) = baseline_type(result)
+//
+class TanOpAdaptor {
+ public:
+  TanOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a TanOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "mhlo.tan".
+std::optional<TanOpAdaptor> getTanOp(CustomCallOp op);
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
diff --ruN a/stablehlo/stablehlo/experimental/tests/BUILD.bazel b/stablehlo/stablehlo/experimental/tests/BUILD.bazel
--- stablehlo/stablehlo/experimental/tests/BUILD.bazel
+++ stablehlo/stablehlo/experimental/tests/BUILD.bazel
@@ -0,0 +1,59 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@bazel_skylib//rules:expand_template.bzl", "expand_template")
+load("@llvm-project//llvm:lit_test.bzl", "lit_test", "package_path")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+# Equivalent of configure_lit_site_cfg from CMakeLists.txt.
+expand_template(
+    name = "lit_site_cfg_py_gen",
+    testonly = True,
+    out = "lit.site.cfg.py",
+    substitutions = {
+        "@LIT_SITE_CFG_IN_HEADER@": "# Autogenerated, do not edit.",
+        "@LLVM_TOOLS_DIR@": package_path("@llvm-project//llvm:BUILD"),
+        "\"@STABLEHLO_TOOLS_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+        "\"@STABLEHLO_SOURCE_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+    },
+    template = "lit.site.cfg.py.in",
+)
+
+# Equivalent of add_lit_testsuite from CMakeLists.txt.
+[
+    lit_test(
+        name = "%s.test" % src,
+        size = "small",
+        srcs = [src],
+        data = [
+            "lit.cfg.py",
+            "lit.site.cfg.py",
+            "//:stablehlo-opt",
+            "//:stablehlo-translate",
+            "//stablehlo/experimental:experimental-stablehlo-opt",
+            "@llvm-project//llvm:FileCheck",
+            "@llvm-project//llvm:not",
+        ] + glob(["%s.bc" % src]),
+        tags = ["stablehlo_tests"],
+    )
+    for src in glob(["**/*.mlir"])
+]
+
+test_suite(
+    name = "experimental_stablehlo_tests",
+    tags = ["experimental_stablehlo_tests"],
+)
diff --ruN a/stablehlo/stablehlo/experimental/tests/CMakeLists.txt b/stablehlo/stablehlo/experimental/tests/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tests/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tests/CMakeLists.txt
@@ -0,0 +1,29 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+configure_lit_site_cfg(
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.py.in
+  ${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg.py
+  MAIN_CONFIG
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.cfg.py
+)
+add_lit_testsuite(check-experimental-stablehlo-tests "Running the experimental/tests/ suite"
+  ${CMAKE_CURRENT_BINARY_DIR}
+  DEPENDS
+  FileCheck
+  experimental-stablehlo-opt
+  stablehlo-translate
+)
+add_dependencies(check-stablehlo-quick check-experimental-stablehlo-tests)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.cfg.py b/stablehlo/stablehlo/experimental/tests/lit.cfg.py
--- stablehlo/stablehlo/experimental/tests/lit.cfg.py
+++ stablehlo/stablehlo/experimental/tests/lit.cfg.py
@@ -0,0 +1,46 @@
+"""Lit configuration to drive test in this repo."""
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# -*- Python -*-
+# pylint: disable=undefined-variable
+
+import os
+
+import lit.formats
+from lit.llvm import llvm_config
+
+# Populate Lit configuration with the minimal required metadata.
+# Some metadata is populated in lit.site.cfg.py.in.
+config.name = 'STABLEHLO_TESTS_SUITE'
+config.test_format = lit.formats.ShTest(not llvm_config.use_lit_shell)
+config.suffixes = ['.mlir']
+config.test_source_root = os.path.dirname(__file__)
+
+# Disallow reusing variables across CHECK-LABEL matches.
+# A variable can eschew this (be made "global") by prefixing its name with $.
+config.environment['FILECHECK_OPTS'] = '-enable-var-scope'
+
+# Make LLVM and StableHLO tools available in RUN directives
+tools = [
+  'FileCheck',
+  'experimental-stablehlo-opt',
+  'stablehlo-translate',
+  'not',
+]
+tool_dirs = [
+  config.llvm_tools_dir,
+  config.stablehlo_tools_dir,
+]
+llvm_config.add_tool_substitutions(tools, tool_dirs)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in b/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
--- stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
+++ stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
@@ -0,0 +1,21 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+@LIT_SITE_CFG_IN_HEADER@
+
+import lit.llvm
+lit.llvm.initialize(lit_config, config)
+config.llvm_tools_dir = "@LLVM_TOOLS_DIR@"
+config.stablehlo_tools_dir = "@STABLEHLO_TOOLS_DIR@"
+lit_config.load_config(config, "@STABLEHLO_SOURCE_DIR@" + "/stablehlo/experimental/tests/lit.cfg.py")
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
@@ -0,0 +1,344 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-canonicalize-dynamism --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: } : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: } : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
@@ -0,0 +1,196 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-refine-shapes --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<*xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<*xf32>
+  func.return %5 : tensor<*xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<*xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<*xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_topk
+func.func @refine_mhlo_topk(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // CHECK: mhlo.topk{{.*}} -> (tensor<5x4xf32>, tensor<5x4xi32>)
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_too_many_operands
+func.func @refine_mhlo_error_too_many_operands(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects size(operands) = 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0, %arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>, tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_too_few_results
+func.func @refine_mhlo_error_too_few_results(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>) {
+  // expected-error@+1{{expects size(results) = 2}}
+  %0 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>)
+  return %0 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_wrong_output_1_type
+func.func @refine_mhlo_error_wrong_output_1_type(%arg0: tensor<5x16xf32>) -> (tensor<f32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects values (result #0) to be a tensor of integer or floating-point type of rank at least 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<f32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<f32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_wrong_output_2_type
+func.func @refine_mhlo_error_wrong_output_2_type(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>) {
+  // expected-error@+1{{expects indices (result #1) to be a tensor of si32 of rank at least 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c1_wrong_output_shape
+func.func @refine_mhlo_error_c1_wrong_output_shape(%arg0: tensor<5x16xf32>) -> (tensor<?x?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values shape to match the operand}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c2_last_dim_not_k
+func.func @refine_mhlo_error_c2_last_dim_not_k(%arg0: tensor<5x16xf32>) -> (tensor<?x5xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values shape to match the operand}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x5xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x5xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c3_wrong_output_type
+func.func @refine_mhlo_error_c3_wrong_output_type(%arg0: tensor<5x16xf32>) -> (tensor<?x?xi32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values element type to be the same as the operand element type}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xi32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xi32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c4_outputs_shape_mismatch
+func.func @refine_mhlo_error_c4_outputs_shape_mismatch(%arg0: tensor<5x16xf32>) -> (tensor<?x4xf32>, tensor<?x5xi32>) {
+  // expected-error@+1{{expects the indices shape to match the values shape}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x4xf32>, tensor<?x5xi32>)
+  return %0#0, %0#1 : tensor<?x4xf32>, tensor<?x5xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c5_negative_k
+func.func @refine_mhlo_error_c5_negative_k(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects k >= 0}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = -4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+
+// -----
+
+// CHECK-LABEL: @refine_tan
+func.func @refine_tan(%arg0: tensor<16xf32>) -> tensor<?xf32> {
+  // CHECK: mhlo.tan{{.*}} -> tensor<16xf32>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "mhlo.tan",
+    mhlo.attributes = {},
+    mhlo.version = 1 : i64
+  } : (tensor<16xf32>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// -----
+
+// tan_op I1
+// CHECK-LABEL: func @tan_op_error_too_many_inputs
+func.func @tan_op_error_too_many_inputs(%arg0: tensor<1xf32>) -> (tensor<1xf32>) {
+  // expected-error@+1{{expects size(operands) = 1}}
+  %0 = "stablehlo.custom_call"(%arg0, %arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
+  return %0 : tensor<1xf32>
+}
+
+// -----
+
+// tan_op O1
+// CHECK-LABEL: func @tan_op_error_too_many_outputs
+func.func @tan_op_error_too_many_outputs(%arg0: tensor<1xf32>) -> (tensor<1xf32>) {
+  // expected-error@+1{{expects size(results) = 1}}
+  %0:2 = "stablehlo.custom_call"(%arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>) -> (tensor<1xf32>, tensor<1xf32>)
+  return %0#1 : tensor<1xf32>
+}
+
+// -----
+
+// tan_op C1
+// CHECK-LABEL: func @tan_op_error_values_bad_element_type
+func.func @tan_op_error_values_bad_element_type(%arg0: tensor<1xf32>) -> (tensor<1xi32>) {
+  // expected-error@+1{{expects operand and result to have compatible shapes}}
+  %0 = "stablehlo.custom_call"(%arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>) -> tensor<1xi32>
+  return %0 : tensor<1xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tools/CMakeLists.txt b/stablehlo/stablehlo/experimental/tools/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tools/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tools/CMakeLists.txt
@@ -0,0 +1,41 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_OPTIONAL_SOURCES
+  StablehloOptMain.cpp
+)
+
+# stablehlo-opt
+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)
+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)
+get_property(extension_libs GLOBAL PROPERTY MLIR_EXTENSION_LIBS)
+set(LIBS
+        ${dialect_libs}
+        ${conversion_libs}
+        ${extension_libs}
+        ExperimentalStablehloPasses
+        MLIROptLib
+        StablehloRegister
+        StablehloTestUtils
+        StablehloPasses
+        InterpreterOps
+        StablehloTOSATransforms
+        )
+add_llvm_executable(experimental-stablehlo-opt StablehloOptMain.cpp)
+llvm_update_compile_flags(experimental-stablehlo-opt)
+target_link_libraries(experimental-stablehlo-opt PRIVATE ${LIBS})
+
+mlir_check_all_link_libraries(experimental-stablehlo-opt)
+
diff --ruN a/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp b/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
--- stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
+++ stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
@@ -0,0 +1,46 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/InitAllDialects.h"
+#include "mlir/InitAllExtensions.h"
+#include "mlir/InitAllPasses.h"
+#include "mlir/Tools/mlir-opt/MlirOptMain.h"
+#include "stablehlo/conversions/tosa/transforms/Passes.h"
+#include "stablehlo/dialect/Register.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/reference/InterpreterOps.h"
+#include "stablehlo/tests/TestUtils.h"
+#include "stablehlo/transforms/Passes.h"
+
+int main(int argc, char **argv) {
+  mlir::registerAllPasses();
+  mlir::hlo::registerAllTestPasses();
+  mlir::stablehlo::registerPassPipelines();
+  mlir::stablehlo::registerPasses();
+  mlir::stablehlo::experimental::registerPasses();
+  mlir::tosa::registerStablehloLegalizeToTosaPassPass();
+  mlir::tosa::registerStablehloPrepareForTosaPassPass();
+
+  mlir::DialectRegistry registry;
+  mlir::registerAllDialects(registry);
+  mlir::registerAllExtensions(registry);
+  mlir::stablehlo::registerAllDialects(registry);
+  registry.insert<mlir::stablehlo::interpreter::InterpreterDialect>();
+
+  return failed(
+      mlir::MlirOptMain(argc, argv, "Experimental StableHLO optimizer driver\n", registry));
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt b/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
--- stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
@@ -0,0 +1,39 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_TARGET_DEFINITIONS Passes.td)
+mlir_tablegen(Passes.h.inc -gen-pass-decls)
+add_public_tablegen_target(ExperimentalPassesIncGen)
+
+add_mlir_dialect_library(ExperimentalStablehloPasses
+  PARTIAL_SOURCES_INTENDED
+  StablehloCanonicalizeDynamism.cpp
+  StablehloRefineShapes.cpp
+
+  DEPENDS
+  ExperimentalPassesIncGen
+
+  LINK_LIBS PUBLIC
+  ChloOps
+  MLIRFuncDialect
+  MLIRIR
+  MLIRInferTypeOpInterface
+  MLIRSupport
+  MLIRTransformUtils
+  ExperimentalStablehloOps
+  StablehloBase
+  StablehloOps
+  StablehloPasses
+  StablehloTypeInference
+)
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.h b/stablehlo/stablehlo/experimental/transforms/Passes.h
--- stablehlo/stablehlo/experimental/transforms/Passes.h
+++ stablehlo/stablehlo/experimental/transforms/Passes.h
@@ -0,0 +1,37 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+#define STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+
+#include <memory>
+
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+  
+#define GEN_PASS_DECL_STABLEHLOCANONICALIZEDYNAMISMPASS
+#define GEN_PASS_DECL_STABLEHLOREFINESHAPESPASS
+#define GEN_PASS_REGISTRATION
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.td b/stablehlo/stablehlo/experimental/transforms/Passes.td
--- stablehlo/stablehlo/experimental/transforms/Passes.td
+++ stablehlo/stablehlo/experimental/transforms/Passes.td
@@ -0,0 +1,31 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def StablehloCanonicalizeDynamismPass : Pass<"experimental-stablehlo-canonicalize-dynamism", "func::FuncOp"> {
+  let summary = "(Experimental) Canonicalizes dynamic StableHLO ops into static ops.";
+  let description = [{
+    Experimental version of the --stablehlo-canonicalize-dynamism pass.
+  }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
+}
+
+def StablehloRefineShapesPass : Pass<"experimental-stablehlo-refine-shapes", "ModuleOp"> {
+  let summary = "(Experimental) Refines shapes across a StableHLO program.";
+  let description = [{
+    Experimental version of the --stablehlo-refine-shapes pass.
+  }];
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
@@ -0,0 +1,166 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2023 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOCANONICALIZEDYNAMISMPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getDenseI64ArrayAttr(windowDimensions),
+        rewriter.getDenseI64ArrayAttr(windowStrides),
+        rewriter.getDenseI64ArrayAttr(baseDilations),
+        rewriter.getDenseI64ArrayAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) || valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(op, op->getResultTypes(),
+                                              op.getOperand(), k[0]);
+    return success();
+  }
+};
+
+struct StablehloCanonicalizeDynamismPass
+    : public impl::StablehloCanonicalizeDynamismPassBase<
+          StablehloCanonicalizeDynamismPass> {
+  using StablehloCanonicalizeDynamismPassBase::
+      StablehloCanonicalizeDynamismPassBase;
+
+  void runOnOperation() override {
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloCanonicalizeDynamismPatterns(&patterns, &getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
+    if (failed(applyPatternsAndFoldGreedily(getOperation(), std::move(patterns),
+                                            config))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
@@ -0,0 +1,196 @@
+/* Copyright 2022 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/transforms/StablehloRefineShapes.h"
+
+#include <cstdint>
+
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/Base.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/dialect/TypeInference.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOREFINESHAPESPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDimensions)
+                                .getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(windowStrides).getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(baseDilations).getValues<int64_t>()),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDilations)
+                                .getValues<int64_t>()),
+            hlo::getPaddingAttr(&rewriter, padding), op.getBody(),
+            inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the shape
+    // of `initial_state`, so we ignore it and provide an empty refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct RefineTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    TopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    outputShape.back() = op.getK();
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct RefineTanOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getTanOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    TanOpAdaptor op = *maybeOp;
+    return refineReturnShape(rewriter, op,
+                             op.getOperand().getType().getShape());
+  }
+};
+
+struct StablehloRefineShapesPass
+    : public impl::StablehloRefineShapesPassBase<StablehloRefineShapesPass> {
+  using StablehloRefineShapesPassBase::StablehloRefineShapesPassBase;
+
+  void runOnOperation() override {
+    auto func = getStablehloRefineShapesTarget(getOperation());
+    if (!func) return signalPassFailure();
+
+    // The algorithm behind this pass consists of a single traversal of the
+    // function. This is sufficient because we only support one function per
+    // program at the moment.
+    // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+    // There have been recent refactors to applyPatternsAndFoldGreedily
+    // upstream, and that might be the reason.
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloRefineShapesPatterns(&patterns, &getContext());
+    patterns.add<RefineDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<RefineDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<RefineDynamicTopKOpPattern>(&getContext());
+    patterns.add<RefineTanOpPattern>(&getContext());
+    patterns.add<RefineTopKOpPattern>(&getContext());
+    if (failed(
+            applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.dynamic_iota %3, dim = 0 : (tensor<1xi32>) -> tensor<?xf32>
     %5 = stablehlo.constant dense<0> : tensor<i64>
     %6 = stablehlo.broadcast_in_dim %5, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %8 = stablehlo.constant dense<2> : tensor<i64>
     %9 = stablehlo.multiply %arg0, %8 : tensor<i64>
     %10 = stablehlo.convert %9 : (tensor<i64>) -> tensor<i32>
diff --ruN a/stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir b/stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
--- stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
+++ stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
@@ -9,7 +9,7 @@
     %2 = stablehlo.dynamic_iota %1, dim = 0 : (tensor<1xi32>) -> tensor<?xi64>
     %3 = stablehlo.constant dense<0> : tensor<i64>
     %4 = stablehlo.broadcast_in_dim %3, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %5 = "stablehlo.gather"(%arg1, %4) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %5 = "stablehlo.gather"(%arg1, %4) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %6 = stablehlo.convert %2 : (tensor<?xi64>) -> tensor<?xf32>
     %7 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %8 = stablehlo.reshape %7 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
@@ -96,7 +96,7 @@
     %89 = stablehlo.concatenate %86, %88, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %90 = stablehlo.reshape %89 : (tensor<2xi32>) -> tensor<1x2xi32>
     %91 = stablehlo.concatenate %90, dim = 0 : (tensor<1x2xi32>) -> tensor<1x2xi32>
-    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = dense<2> : tensor<1xi64>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
+    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = array<i64: 2, 2>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
     return %92 : tensor<1x?x16xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
@@ -96,7 +96,7 @@
     %89 = stablehlo.concatenate %86, %88, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %90 = stablehlo.reshape %89 : (tensor<2xi32>) -> tensor<1x2xi32>
     %91 = stablehlo.concatenate %90, dim = 0 : (tensor<1x2xi32>) -> tensor<1x2xi32>
-    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = dense<2> : tensor<1xi64>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
+    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = array<i64: 2, 2>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
     return %92 : tensor<1x?x16xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<1x2xf32>, tensor<1x2xi32>)
     %1 = call @expected() : () -> tensor<1xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<1x2xf32>, tensor<1x2xi32>) -> tensor<1xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1>} : (tensor<1x2xf32>, tensor<1x2xi32>) -> tensor<1xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<1xf32>, tensor<1xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<2x3x3xf32>, tensor<2x3xi32>)
     %1 = call @expected() : () -> tensor<2x3x2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = dense<[1, 3, 2]> : tensor<3xi64>} : (tensor<2x3x3xf32>, tensor<2x3xi32>) -> tensor<2x3x2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3, 2>} : (tensor<2x3x3xf32>, tensor<2x3xi32>) -> tensor<2x3x2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3x2xf32>, tensor<2x3x2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<2x6x3xf32>, tensor<2x3xi32>)
     %1 = call @expected() : () -> tensor<2x3x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = dense<[1, 3, 3]> : tensor<3xi64>} : (tensor<2x6x3xf32>, tensor<2x3xi32>) -> tensor<2x3x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3, 3>} : (tensor<2x6x3xf32>, tensor<2x3xi32>) -> tensor<2x3x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3x3xf32>, tensor<2x3x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<3x10xf32>, tensor<3x2xi32>)
     %1 = call @expected() : () -> tensor<3x5xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 5]> : tensor<2xi64>} : (tensor<3x10xf32>, tensor<3x2xi32>) -> tensor<3x5xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 5>} : (tensor<3x10xf32>, tensor<3x2xi32>) -> tensor<3x5xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<4x6xf32>, tensor<4x2xi32>)
     %1 = call @expected() : () -> tensor<4x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x2xi32>) -> tensor<4x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<4x6xf32>, tensor<4x2xi32>) -> tensor<4x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<4x3xf32>, tensor<4x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xbf16>, tensor<1xi32>) -> tensor<bf16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xbf16>, tensor<1xi32>) -> tensor<bf16>
     %39 = stablehlo.constant dense<0x7FC0> : tensor<bf16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<bf16>
     return %40 : tensor<bf16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi1>, tensor<1xi32>) -> tensor<i1>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi1>, tensor<1xi32>) -> tensor<i1>
     %39 = stablehlo.constant dense<true> : tensor<i1>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i1>
     return %40 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xcomplex<f32>>, tensor<1xi32>) -> tensor<complex<f32>>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xcomplex<f32>>, tensor<1xi32>) -> tensor<complex<f32>>
     %39 = stablehlo.constant dense<(0x7FC00000,0.000000e+00)> : tensor<complex<f32>>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<complex<f32>>
     return %40 : tensor<complex<f32>>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xf16>, tensor<1xi32>) -> tensor<f16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xf16>, tensor<1xi32>) -> tensor<f16>
     %39 = stablehlo.constant dense<0x7E00> : tensor<f16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<f16>
     return %40 : tensor<f16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xf32>, tensor<1xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xf32>, tensor<1xi32>) -> tensor<f32>
     %39 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<f32>
     return %40 : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi16>, tensor<1xi32>) -> tensor<i16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi16>, tensor<1xi32>) -> tensor<i16>
     %39 = stablehlo.constant dense<-32768> : tensor<i16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i16>
     return %40 : tensor<i16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi32>, tensor<1xi32>) -> tensor<i32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi32>, tensor<1xi32>) -> tensor<i32>
     %39 = stablehlo.constant dense<-2147483648> : tensor<i32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i32>
     return %40 : tensor<i32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi8>, tensor<1xi32>) -> tensor<i8>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi8>, tensor<1xi32>) -> tensor<i8>
     %39 = stablehlo.constant dense<-128> : tensor<i8>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i8>
     return %40 : tensor<i8>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui16>, tensor<1xi32>) -> tensor<ui16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui16>, tensor<1xi32>) -> tensor<ui16>
     %39 = stablehlo.constant dense<65535> : tensor<ui16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui16>
     return %40 : tensor<ui16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui32>, tensor<1xi32>) -> tensor<ui32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui32>, tensor<1xi32>) -> tensor<ui32>
     %39 = stablehlo.constant dense<4294967295> : tensor<ui32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui32>
     return %40 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui8>, tensor<1xi32>) -> tensor<ui8>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui8>, tensor<1xi32>) -> tensor<ui8>
     %39 = stablehlo.constant dense<255> : tensor<ui8>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui8>
     return %40 : tensor<ui8>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
@@ -23,7 +23,7 @@
     %15 = stablehlo.broadcast_in_dim %9, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %16 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %17 = stablehlo.concatenate %14, %15, %16, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
-    %18 = "stablehlo.gather"(%0, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3xi32>) -> tensor<f32>
+    %18 = "stablehlo.gather"(%0, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 1>} : (tensor<10x10x10xf32>, tensor<3xi32>) -> tensor<f32>
     %19 = stablehlo.custom_call @check.eq(%18, %1) : (tensor<f32>, tensor<f32>) -> tensor<i1>
     return %19 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<1> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<10xf32>, tensor<10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
@@ -9,7 +9,7 @@
     %1 = call @expected() : () -> tensor<10x10xf32>
     %2 = stablehlo.constant dense<0> : tensor<i32>
     %3 = stablehlo.broadcast_in_dim %2, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<5> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[3, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 3, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<300> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[3, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 3, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
@@ -9,7 +9,7 @@
     %1 = call @expected() : () -> tensor<10x10xf32>
     %2 = stablehlo.constant dense<2> : tensor<i32>
     %3 = stablehlo.broadcast_in_dim %2, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, indices_are_sorted = true, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, indices_are_sorted = true, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<10> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<10> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
     return %1 : tensor<1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [0] : (tensor<1xi1>) -> tensor<1x10x10xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<10x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
     return %1 : tensor<10x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [1] : (tensor<1xi1>) -> tensor<10x1x10xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<10x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<10x10x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
     return %1 : tensor<10x10x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [2] : (tensor<1xi1>) -> tensor<10x10x1xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<10x10x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     return %1 : tensor<2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<2xi1>) -> tensor<2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<10x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     return %1 : tensor<10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1] : (tensor<2xi1>) -> tensor<10x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<10x10x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     return %1 : tensor<10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2] : (tensor<2xi1>) -> tensor<10x10x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<2x10x10xf32>
     return %1 : tensor<2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<1> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<10> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [0] : (tensor<2xi1>) -> tensor<2x10x10xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<10x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x2x10xf32>
     return %1 : tensor<10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<10> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<1> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [1] : (tensor<2xi1>) -> tensor<10x2x10xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<10x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<10x10x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x10x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x10x2xf32>
     return %1 : tensor<10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<10> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<10> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<2xi1>) -> tensor<10x10x2xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<10x10x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<2x2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     return %1 : tensor<2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<2x2xi1>) -> tensor<2x2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x2x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     return %1 : tensor<10x2x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<2x2xi1>) -> tensor<10x2x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x10x2x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     return %1 : tensor<10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<2x2xi1>) -> tensor<10x10x2x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<3x1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
     return %1 : tensor<3x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<3x1xi1>) -> tensor<3x1x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<3x1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<10x3x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
     return %1 : tensor<10x3x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<3x1xi1>) -> tensor<10x3x1x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x3x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<10x10x3x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
     return %1 : tensor<10x10x3x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<3x1xi1>) -> tensor<10x10x3x1xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x3x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<2x2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     return %1 : tensor<2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<2x2xi1>) -> tensor<2x2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x2x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     return %1 : tensor<10x2x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<2x2xi1>) -> tensor<10x2x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x10x2x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     return %1 : tensor<10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<2x2xi1>) -> tensor<10x10x2x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<5x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
     return %1 : tensor<5x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<5xi1>) -> tensor<5x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<5x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<10x5x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
     return %1 : tensor<10x5x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1] : (tensor<5xi1>) -> tensor<10x5x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x5x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<10x10x5xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
     return %1 : tensor<10x10x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2] : (tensor<5xi1>) -> tensor<10x10x5xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x5xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<2x2x1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
     return %1 : tensor<2x2x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1, 2] : (tensor<2x2x1xi1>) -> tensor<2x2x1x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<10x2x2x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
     return %1 : tensor<10x2x2x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2, 3] : (tensor<2x2x1xi1>) -> tensor<10x2x2x1x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<10x10x2x2x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
     return %1 : tensor<10x10x2x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3, 4] : (tensor<2x2x1xi1>) -> tensor<10x10x2x2x1xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir b/stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10x5xf32>, tensor<3x1xi32>)
     %1 = call @expected() : () -> tensor<3x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<10x5xf32>, tensor<3x1xi32>) -> tensor<3x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<10x5xf32>, tensor<3x1xi32>) -> tensor<3x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir b/stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10x6xf32>, tensor<2x2xi32>)
     %1 = call @expected() : () -> tensor<2x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<10x6xf32>, tensor<2x2xi32>) -> tensor<2x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<10x6xf32>, tensor<2x2xi32>) -> tensor<2x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir b/stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10xf32>, tensor<3x1xi32>)
     %1 = call @expected() : () -> tensor<3x2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<10xf32>, tensor<3x1xi32>) -> tensor<3x2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 2>} : (tensor<10xf32>, tensor<3x1xi32>) -> tensor<3x2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x2xf32>, tensor<3x2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir b/stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
--- stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<5xf32>, tensor<2x1xi32>)
     %1 = call @expected() : () -> tensor<2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<5xf32>, tensor<2x1xi32>) -> tensor<2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<5xf32>, tensor<2x1xi32>) -> tensor<2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2xf32>, tensor<2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
@@ -6,7 +6,7 @@
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<4xf32> {
     %0 = stablehlo.constant dense<1> : tensor<i64>
     %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %2 = "stablehlo.gather"(%arg1, %1) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
+    %2 = "stablehlo.gather"(%arg1, %1) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
     return %2 : tensor<4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
@@ -12,7 +12,7 @@
     %5 = stablehlo.add %1, %2 : tensor<i64>
     %6 = stablehlo.select %4, %5, %1 : tensor<i1>, tensor<i64>
     %7 = stablehlo.broadcast_in_dim %6, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %8 = "stablehlo.gather"(%arg1, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
+    %8 = "stablehlo.gather"(%arg1, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
     return %8 : tensor<4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
@@ -21,7 +21,7 @@
     %14 = stablehlo.constant dense<1> : tensor<1xi32>
     %15 = stablehlo.concatenate %13, %14, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %16 = stablehlo.dynamic_broadcast_in_dim %11, %15, dims = [0] : (tensor<?xi64>, tensor<2xi32>) -> tensor<?x1xi64>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<?x1xi64>) -> tensor<?x4xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<?x1xi64>) -> tensor<?x4xf32>
     return %17 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
@@ -20,7 +20,7 @@
     %13 = stablehlo.constant dense<1> : tensor<1xi32>
     %14 = stablehlo.concatenate %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %15 = stablehlo.dynamic_broadcast_in_dim %10, %14, dims = [0] : (tensor<?xi32>, tensor<2xi32>) -> tensor<?x1xi32>
-    %16 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<3x4xf32>, tensor<?x1xi32>) -> tensor<?x4xf32>
+    %16 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 4>} : (tensor<3x4xf32>, tensor<?x1xi32>) -> tensor<?x4xf32>
     return %16 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir b/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
--- stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
+++ stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
@@ -39,7 +39,7 @@
     %4 = stablehlo.constant dense<0> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %8 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %9 = stablehlo.reshape %8 : (tensor<i32>) -> tensor<1xi32>
     %10 = stablehlo.constant dense<0> : tensor<1xi32>
@@ -56,21 +56,21 @@
     %21 = stablehlo.constant dense<1> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %23 = stablehlo.concatenate %20, %22, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %25 = stablehlo.pad %18, %24, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x5xf32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<0> : tensor<i32>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %30 = stablehlo.concatenate %27, %29, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %32 = stablehlo.pad %25, %31, low = [0, 5], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x10xf32>
     %33 = stablehlo.constant dense<1> : tensor<i32>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %35 = stablehlo.constant dense<1> : tensor<i32>
     %36 = stablehlo.broadcast_in_dim %35, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %37 = stablehlo.concatenate %34, %36, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %39 = stablehlo.pad %32, %38, low = [0, 0], high = [0, 1], interior = [0, 0] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?x11xf32>
     return %39 : tensor<?x11xf32>
   }
diff --ruN a/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir b/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
--- stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
+++ stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
@@ -16,7 +16,7 @@
     %4 = stablehlo.constant dense<0> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %8 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %9 = stablehlo.reshape %8 : (tensor<i32>) -> tensor<1xi32>
     %10 = stablehlo.constant dense<0> : tensor<1xi32>
@@ -33,21 +33,21 @@
     %21 = stablehlo.constant dense<1> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %23 = stablehlo.concatenate %20, %22, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %25 = stablehlo.pad %18, %24, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x5xf32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<0> : tensor<i32>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %30 = stablehlo.concatenate %27, %29, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %32 = stablehlo.pad %25, %31, low = [0, 5], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x10xf32>
     %33 = stablehlo.constant dense<1> : tensor<i32>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %35 = stablehlo.constant dense<1> : tensor<i32>
     %36 = stablehlo.broadcast_in_dim %35, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %37 = stablehlo.concatenate %34, %36, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %39 = stablehlo.pad %32, %38, low = [0, 0], high = [0, 1], interior = [0, 0] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?x11xf32>
     return %39 : tensor<?x11xf32>
   }
diff --ruN a/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir b/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
--- stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
+++ stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
@@ -90,7 +90,7 @@
     %40 = stablehlo.add %33, %37 : tensor<i64>
     %41 = stablehlo.select %39, %40, %33 : tensor<i1>, tensor<i64>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %43 = "stablehlo.gather"(%8, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %43 = "stablehlo.gather"(%8, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %44 = stablehlo.constant dense<5> : tensor<i64>
     %45 = stablehlo.multiply %arg0, %44 : tensor<i64>
     %46 = stablehlo.convert %45 : tensor<i64>
@@ -99,7 +99,7 @@
     %49 = stablehlo.add %34, %46 : tensor<i64>
     %50 = stablehlo.select %48, %49, %34 : tensor<i1>, tensor<i64>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %52 = "stablehlo.gather"(%8, %51) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %52 = "stablehlo.gather"(%8, %51) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %53 = stablehlo.convert %43 : (tensor<f32>) -> tensor<f64>
     %54 = stablehlo.multiply %53, %22 : tensor<f64>
     %55 = stablehlo.convert %52 : (tensor<f32>) -> tensor<f64>
diff --ruN a/stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir b/stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
--- stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
+++ stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
@@ -88,9 +88,9 @@
     %31 = stablehlo.convert %26 : (tensor<f64>) -> tensor<i64>
     %32 = stablehlo.convert %30 : (tensor<f64>) -> tensor<i64>
     %33 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %34 = "stablehlo.gather"(%11, %33) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %34 = "stablehlo.gather"(%11, %33) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %35 = stablehlo.broadcast_in_dim %32, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %36 = "stablehlo.gather"(%11, %35) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %36 = "stablehlo.gather"(%11, %35) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %37 = stablehlo.convert %34 : (tensor<f32>) -> tensor<f64>
     %38 = stablehlo.multiply %37, %22 : tensor<f64>
     %39 = stablehlo.convert %36 : (tensor<f32>) -> tensor<f64>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
@@ -323,7 +323,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -641,7 +641,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -758,7 +758,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
@@ -326,7 +326,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -644,7 +644,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -761,7 +761,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
@@ -323,7 +323,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -641,7 +641,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -758,7 +758,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
@@ -326,7 +326,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -644,7 +644,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -761,7 +761,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
@@ -201,7 +201,7 @@
     %65 = stablehlo.concatenate %64#2, %64#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %66 = stablehlo.constant dense<0> : tensor<i32>
     %67 = stablehlo.broadcast_in_dim %66, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %71 = stablehlo.constant dense<16> : tensor<ui32>
@@ -306,7 +306,7 @@
     %106 = stablehlo.concatenate %105#2, %105#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %107 = stablehlo.constant dense<0> : tensor<i32>
     %108 = stablehlo.broadcast_in_dim %107, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %110 = stablehlo.broadcast_in_dim %109, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %111 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %112 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
@@ -200,7 +200,7 @@
     %64 = stablehlo.concatenate %63#2, %63#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %65 = stablehlo.constant dense<0> : tensor<i32>
     %66 = stablehlo.broadcast_in_dim %65, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %67 = "stablehlo.gather"(%64, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %67 = "stablehlo.gather"(%64, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %68 = stablehlo.reshape %67 : (tensor<1xui32>) -> tensor<ui32>
     %69 = stablehlo.constant dense<0> : tensor<1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<1xui32>
@@ -290,7 +290,7 @@
     %90 = stablehlo.concatenate %89#2, %89#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %91 = stablehlo.constant dense<0> : tensor<i32>
     %92 = stablehlo.broadcast_in_dim %91, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %93 = "stablehlo.gather"(%90, %92) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %93 = "stablehlo.gather"(%90, %92) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %94 = stablehlo.reshape %93 : (tensor<1xui32>) -> tensor<ui32>
     %95 = stablehlo.subtract %17, %12 : tensor<i32>
     %96 = stablehlo.convert %95 : (tensor<i32>) -> tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
@@ -201,7 +201,7 @@
     %65 = stablehlo.concatenate %64#2, %64#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %66 = stablehlo.constant dense<0> : tensor<i32>
     %67 = stablehlo.broadcast_in_dim %66, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %71 = stablehlo.constant dense<8> : tensor<ui32>
@@ -306,7 +306,7 @@
     %106 = stablehlo.concatenate %105#2, %105#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %107 = stablehlo.constant dense<0> : tensor<i32>
     %108 = stablehlo.broadcast_in_dim %107, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %110 = stablehlo.broadcast_in_dim %109, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %111 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %112 = stablehlo.constant dense<8> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
@@ -203,7 +203,7 @@
     %67 = stablehlo.concatenate %66#2, %66#3, dim = 0 : (tensor<3xui32>, tensor<3xui32>) -> tensor<6xui32>
     %68 = stablehlo.constant dense<0> : tensor<i32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %70 = "stablehlo.gather"(%67, %69) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<5> : tensor<1xi64>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
+    %70 = "stablehlo.gather"(%67, %69) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 5>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
     %71 = stablehlo.broadcast_in_dim %70, dims = [1] : (tensor<5xui32>) -> tensor<1x5xui32>
     %72 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %73 = stablehlo.constant dense<8> : tensor<ui32>
@@ -307,7 +307,7 @@
     %107 = stablehlo.concatenate %106#2, %106#3, dim = 0 : (tensor<3xui32>, tensor<3xui32>) -> tensor<6xui32>
     %108 = stablehlo.constant dense<0> : tensor<i32>
     %109 = stablehlo.broadcast_in_dim %108, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %110 = "stablehlo.gather"(%107, %109) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<5> : tensor<1xi64>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
+    %110 = "stablehlo.gather"(%107, %109) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 5>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
     %111 = stablehlo.broadcast_in_dim %110, dims = [1] : (tensor<5xui32>) -> tensor<1x5xui32>
     %112 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %113 = stablehlo.constant dense<8> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %28 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %29 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %28 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %29 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.reshape %26 : (tensor<1xui32>) -> tensor<ui32>
     %28 = stablehlo.constant dense<9> : tensor<ui32>
     %29 = stablehlo.shift_right_logical %27, %28 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
     return %2 : tensor<?x7xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %11 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %11 : tensor<f32>
-    }) {padding = dense<[[0, 0], [1, 1], [0, 0]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 4, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<1x?x1xf32>, tensor<f32>) -> tensor<1x?x1xf32>
+    }) {padding = dense<[[0, 0], [1, 1], [0, 0]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 4, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<1x?x1xf32>, tensor<f32>) -> tensor<1x?x1xf32>
     return %10 : tensor<1x?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {base_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x10xf32>
+    }) {base_dilations = array<i64: 1, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %6 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[5, 6], [3, 4]]> : tensor<2x2xi64>, window_dimensions = dense<13> : tensor<2xi64>, window_strides = dense<[5, 6]> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<3x2xf32>
+    }) {padding = dense<[[5, 6], [3, 4]]> : tensor<2x2xi64>, window_dimensions = array<i64: 13, 13>, window_strides = array<i64: 5, 6>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<3x2xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x2xf32>, tensor<3x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<12x12xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<12x12xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<12x12xf32>, tensor<12x12xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<6x6xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<6x6xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<6x6xf32>, tensor<6x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[1, 2], [0, 3]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<6x8xf32>
+    }) {padding = dense<[[1, 2], [0, 3]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<6x8xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<6x8xf32>, tensor<6x8xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x4xf32>
+    }) {window_dilations = array<i64: 1, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x4xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<1> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x6xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>, window_strides = dense<[1, 2]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x3xf32>
+    }) {window_dimensions = array<i64: 2, 2>, window_strides = array<i64: 1, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x3xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<1x2x1xf32>, tensor<f32>) -> tensor<1x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 1>} : (tensor<1x2x1xf32>, tensor<f32>) -> tensor<1x1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x2x4x1xf32>, tensor<f32>) -> tensor<1x1x3x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 1>} : (tensor<1x2x4x1xf32>, tensor<f32>) -> tensor<1x1x3x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1x3x1xf32>, tensor<1x1x3x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2]> : tensor<2xi64>} : (tensor<1x2xf32>, tensor<f32>) -> tensor<1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2>} : (tensor<1x2xf32>, tensor<f32>) -> tensor<1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x4x3x2x1xf32>, tensor<f32>) -> tensor<1x3x2x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x4x3x2x1xf32>, tensor<f32>) -> tensor<1x3x2x1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3x2x1x1xf32>, tensor<1x3x2x1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[2, 1]> : tensor<2xi64>} : (tensor<2x1xf32>, tensor<f32>) -> tensor<1x1xf32>
+    }) {window_dimensions = array<i64: 2, 1>} : (tensor<2x1xf32>, tensor<f32>) -> tensor<1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x3xf32>, tensor<f32>) -> tensor<1x3x2xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x3xf32>, tensor<f32>) -> tensor<1x3x2xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3x2xf32>, tensor<1x3x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<2x4xf32>, tensor<f32>) -> tensor<1x3xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<2x4xf32>, tensor<f32>) -> tensor<1x3xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<1xi64>} : (tensor<2xf32>, tensor<f32>) -> tensor<1xf32>
+    }) {window_dimensions = array<i64: 2>} : (tensor<2xf32>, tensor<f32>) -> tensor<1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1xf32>, tensor<1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i1>
       stablehlo.return %5 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i1>
       stablehlo.return %6 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
@@ -20,7 +20,7 @@
       %12 = stablehlo.select %7, %11, %8 : tensor<i1>, tensor<i1>
       %13 = stablehlo.select %12, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %13 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
@@ -21,7 +21,7 @@
       %13 = stablehlo.select %8, %12, %9 : tensor<i1>, tensor<i1>
       %14 = stablehlo.select %13, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %14 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i1>
       stablehlo.return %6 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
@@ -21,7 +21,7 @@
       %13 = stablehlo.select %8, %12, %9 : tensor<i1>, tensor<i1>
       %14 = stablehlo.select %13, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %14 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %2 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
       stablehlo.return %2 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
     return %1 : tensor<?x7xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<[1, 401]> : tensor<2xi64>, window_strides = dense<[1, 160]> : tensor<2xi64>} : (tensor<1x16000xf32>, tensor<f32>) -> tensor<1x98xf32>
+    }) {window_dimensions = array<i64: 1, 401>, window_strides = array<i64: 1, 160>} : (tensor<1x16000xf32>, tensor<f32>) -> tensor<1x98xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<1x98xf32>, tensor<1x98xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[2, 3]> : tensor<2xi64>, window_dilations = dense<[3, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x14xf32>, tensor<4x14xf32>)
+    }) {base_dilations = array<i64: 2, 3>, window_dilations = array<i64: 3, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x14xf32>, tensor<4x14xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<4x14xf32>, tensor<4x14xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[2, 3]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<6x15xf32>, tensor<6x15xf32>)
+    }) {base_dilations = array<i64: 2, 3>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<6x15xf32>, tensor<6x15xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<6x15xf32>, tensor<6x15xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dilations = dense<[2, 3]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x3xf32>, tensor<2x3xf32>)
+    }) {window_dilations = array<i64: 2, 3>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x3xf32>, tensor<2x3xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<bf16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<bf16>
       stablehlo.return %7, %8 : tensor<bf16>, tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<bf16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<bf16>
       stablehlo.return %7, %8 : tensor<bf16>, tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f16>
       stablehlo.return %7, %8 : tensor<f16>, tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f16>
       stablehlo.return %7, %8 : tensor<f16>, tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x6xf32>, tensor<4x6xf32>)
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x6xf32>, tensor<4x6xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x4xf32>, tensor<3x4xf32>)
+    }) {window_dimensions = array<i64: 2, 3>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x4xf32>, tensor<3x4xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>, window_strides = dense<[2, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x2xf32>, tensor<2x2xf32>)
+    }) {window_dimensions = array<i64: 2, 2>, window_strides = array<i64: 2, 3>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x2xf32>, tensor<2x2xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %8 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xbf16>, tensor<1x3x5xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xbf16>, tensor<1x3x5xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xbf16>) -> tensor<2x4x6xbf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xbf16>, tensor<2x4x6xbf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %8 = stablehlo.or %arg0, %arg1 : tensor<i1>
       stablehlo.return %8 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi1>, tensor<1x3x5xi1>, tensor<i1>) -> tensor<2x4x6xi1>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi1>, tensor<1x3x5xi1>, tensor<i1>) -> tensor<2x4x6xi1>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi1>) -> tensor<2x4x6xi1>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi1>, tensor<2x4x6xi1>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %8 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf16>, tensor<1x3x5xf16>, tensor<f16>) -> tensor<2x4x6xf16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf16>, tensor<1x3x5xf16>, tensor<f16>) -> tensor<2x4x6xf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf16>) -> tensor<2x4x6xf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf16>, tensor<2x4x6xf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %8 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi16>, tensor<1x3x5xi16>, tensor<i16>) -> tensor<2x4x6xi16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi16>, tensor<1x3x5xi16>, tensor<i16>) -> tensor<2x4x6xi16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi16>) -> tensor<2x4x6xi16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi16>, tensor<2x4x6xi16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %8 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi32>, tensor<1x3x5xi32>, tensor<i32>) -> tensor<2x4x6xi32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi32>, tensor<1x3x5xi32>, tensor<i32>) -> tensor<2x4x6xi32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi32>) -> tensor<2x4x6xi32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi32>, tensor<2x4x6xi32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %8 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi8>, tensor<1x3x5xi8>, tensor<i8>) -> tensor<2x4x6xi8>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi8>, tensor<1x3x5xi8>, tensor<i8>) -> tensor<2x4x6xi8>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi8>) -> tensor<2x4x6xi8>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi8>, tensor<2x4x6xi8>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %8 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui16>, tensor<1x3x5xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui16>, tensor<1x3x5xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui16>) -> tensor<2x4x6xui16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui16>, tensor<2x4x6xui16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %8 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui32>, tensor<1x3x5xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui32>, tensor<1x3x5xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui32>) -> tensor<2x4x6xui32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui32>, tensor<2x4x6xui32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %8 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui8>, tensor<1x3x5xui8>, tensor<ui8>) -> tensor<2x4x6xui8>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui8>, tensor<1x3x5xui8>, tensor<ui8>) -> tensor<2x4x6xui8>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui8>) -> tensor<2x4x6xui8>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui8>, tensor<2x4x6xui8>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<4x6x8xf32>, tensor<3x5x7xf32>, tensor<f32>) -> tensor<4x6x8xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<4x6x8xf32>, tensor<3x5x7xf32>, tensor<f32>) -> tensor<4x6x8xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 3, 5, 7>, start_indices = array<i64: 1, 1, 1>, strides = array<i64: 1, 1, 1>} : (tensor<4x6x8xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %8 : tensor<bf16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xbf16>, tensor<2x1x6xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xbf16>, tensor<2x1x6xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xbf16>) -> tensor<2x4x6xbf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xbf16>, tensor<2x4x6xbf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %8 : tensor<f16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xf16>, tensor<2x1x6xf16>, tensor<f16>) -> tensor<2x4x6xf16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xf16>, tensor<2x1x6xf16>, tensor<f16>) -> tensor<2x4x6xf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf16>) -> tensor<2x4x6xf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf16>, tensor<2x4x6xf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<2x1x6xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xf32>, tensor<2x1x6xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %8 : tensor<i16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xi16>, tensor<2x1x6xi16>, tensor<i16>) -> tensor<2x4x6xi16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xi16>, tensor<2x1x6xi16>, tensor<i16>) -> tensor<2x4x6xi16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi16>) -> tensor<2x4x6xi16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi16>, tensor<2x4x6xi16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %8 : tensor<i32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xi32>, tensor<2x1x6xi32>, tensor<i32>) -> tensor<2x4x6xi32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xi32>, tensor<2x1x6xi32>, tensor<i32>) -> tensor<2x4x6xi32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi32>) -> tensor<2x4x6xi32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi32>, tensor<2x4x6xi32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %8 : tensor<ui16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xui16>, tensor<2x1x6xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xui16>, tensor<2x1x6xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui16>) -> tensor<2x4x6xui16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui16>, tensor<2x4x6xui16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %8 : tensor<ui32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xui32>, tensor<2x1x6xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xui32>, tensor<2x1x6xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui32>) -> tensor<2x4x6xui32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui32>, tensor<2x4x6xui32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<2x3x4xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 2, 3>} : (tensor<2x4x6xf32>, tensor<2x3x4xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>, window_strides = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x2x2xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>, window_strides = array<i64: 1, 2, 3>} : (tensor<2x4x6xf32>, tensor<1x2x2xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
@@ -35,7 +35,7 @@
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi64>
     %25 = stablehlo.convert %24 : (tensor<1xi64>) -> tensor<1xi32>
     %26 = stablehlo.broadcast_in_dim %25, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %27 = "stablehlo.gather"(%16, %26) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %27 = "stablehlo.gather"(%16, %26) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %28 = stablehlo.convert %arg0 : tensor<i64>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %30 = stablehlo.constant dense<1> : tensor<i64>
@@ -52,7 +52,7 @@
     %41 = stablehlo.select %37, %40, %9 : tensor<1xi1>, tensor<1xi64>
     %42 = stablehlo.convert %41 : (tensor<1xi64>) -> tensor<1xi32>
     %43 = stablehlo.broadcast_in_dim %42, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %44 = "stablehlo.gather"(%34, %43) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %44 = "stablehlo.gather"(%34, %43) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %45 = stablehlo.subtract %27, %44 : tensor<1xi64>
     %46 = stablehlo.constant dense<0> : tensor<i64>
     %47 = stablehlo.broadcast_in_dim %46, dims = [] : (tensor<i64>) -> tensor<2x1xi64>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
@@ -40,7 +40,7 @@
     %29 = stablehlo.select %25, %28, %16 : tensor<1xi1>, tensor<1xi64>
     %30 = stablehlo.convert %29 : (tensor<1xi64>) -> tensor<1xi32>
     %31 = stablehlo.broadcast_in_dim %30, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %32 = "stablehlo.gather"(%22, %31) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %32 = "stablehlo.gather"(%22, %31) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %33 = stablehlo.constant dense<1> : tensor<i64>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %35 = stablehlo.constant dense<2> : tensor<i64>
@@ -55,7 +55,7 @@
     %44 = stablehlo.select %40, %43, %17 : tensor<1xi1>, tensor<1xi64>
     %45 = stablehlo.convert %44 : (tensor<1xi64>) -> tensor<1xi32>
     %46 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %47 = "stablehlo.gather"(%37, %46) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %47 = "stablehlo.gather"(%37, %46) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %48 = stablehlo.subtract %32, %47 : tensor<1xi64>
     %49 = stablehlo.constant dense<0> : tensor<i64>
     %50 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -78,7 +78,7 @@
       %79 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %79 : tensor<i1>
     }
-    %66 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 2]> : tensor<2xi64>} : (tensor<?x2xf32>, tensor<?x1xi64>) -> tensor<?x2xf32>
+    %66 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2>} : (tensor<?x2xf32>, tensor<?x1xi64>) -> tensor<?x2xf32>
     %67 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %68 = stablehlo.reshape %67 : (tensor<i32>) -> tensor<1xi32>
     %69 = stablehlo.constant dense<2> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
@@ -53,7 +53,7 @@
     %42 = stablehlo.select %38, %41, %29 : tensor<2xi1>, tensor<2xi64>
     %43 = stablehlo.convert %42 : (tensor<2xi64>) -> tensor<2xi32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %45 = "stablehlo.gather"(%35, %44) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %45 = "stablehlo.gather"(%35, %44) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %46 = stablehlo.constant dense<1> : tensor<i64>
     %47 = stablehlo.broadcast_in_dim %46, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %48 = stablehlo.constant dense<1> : tensor<i64>
@@ -68,7 +68,7 @@
     %57 = stablehlo.select %53, %56, %30 : tensor<2xi1>, tensor<2xi64>
     %58 = stablehlo.convert %57 : (tensor<2xi64>) -> tensor<2xi32>
     %59 = stablehlo.broadcast_in_dim %58, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %60 = "stablehlo.gather"(%50, %59) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %60 = "stablehlo.gather"(%50, %59) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %61 = stablehlo.subtract %45, %60 : tensor<2xi64>
     %62 = stablehlo.constant dense<0> : tensor<i64>
     %63 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %89 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %89 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x2xf32>, tensor<?x1x2xi64>) -> tensor<?x1xf32>
+    %81 = "stablehlo.gather"(%arg1, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>} : (tensor<?x2xf32>, tensor<?x1x2xi64>) -> tensor<?x1xf32>
     %82 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %83 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %84 = stablehlo.reshape %83 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir b/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
--- stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
+++ stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
@@ -80,7 +80,7 @@
     %72 = stablehlo.broadcast_in_dim %71, dims = [] : (tensor<f32>) -> tensor<1xf32>
     %73 = stablehlo.constant dense<0> : tensor<i32>
     %74 = stablehlo.broadcast_in_dim %73, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
+    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 2>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
     %76 = call @append(%72, %75) : (tensor<1xf32>, tensor<2xf32>) -> tensor<3xf32>
     %77 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
     %78 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 3, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<3x1xf32>
@@ -173,7 +173,7 @@
     %165 = stablehlo.add %163, %164 : tensor<i32>
     %166 = stablehlo.convert %165 : tensor<i32>
     %167 = stablehlo.broadcast_in_dim %166, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
+    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
     %169 = stablehlo.reverse %162, dims = [0] : tensor<3x1xf32>
     %170 = stablehlo.reverse %61, dims = [0] : tensor<3xf32>
     %171 = "stablehlo.slice"(%169) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir b/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
--- stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
+++ stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
@@ -80,7 +80,7 @@
     %72 = stablehlo.broadcast_in_dim %71, dims = [] : (tensor<f32>) -> tensor<1xf32>
     %73 = stablehlo.constant dense<0> : tensor<i32>
     %74 = stablehlo.broadcast_in_dim %73, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
+    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 2>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
     %76 = call @append(%72, %75) : (tensor<1xf32>, tensor<2xf32>) -> tensor<3xf32>
     %77 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
     %78 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 3, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<3x1xf32>
@@ -173,7 +173,7 @@
     %165 = stablehlo.add %163, %164 : tensor<i32>
     %166 = stablehlo.convert %165 : tensor<i32>
     %167 = stablehlo.broadcast_in_dim %166, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
+    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
     %169 = stablehlo.reverse %162, dims = [0] : tensor<3x1xf32>
     %170 = stablehlo.reverse %61, dims = [0] : tensor<3xf32>
     %171 = "stablehlo.slice"(%169) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
@@ -71,7 +71,7 @@
     %64 = stablehlo.concatenate %62, %63, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %65 = stablehlo.dynamic_iota %64, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %66 = stablehlo.concatenate %65, %60, dim = 1 : (tensor<?x1xi64>, tensor<?x2xi64>) -> tensor<?x3xi64>
-    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 0]> : tensor<3xi64>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x1x0xf32>
+    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 0>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x1x0xf32>
     return %67 : tensor<?x1x0xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
@@ -71,7 +71,7 @@
     %64 = stablehlo.concatenate %62, %63, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %65 = stablehlo.dynamic_iota %64, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %66 = stablehlo.concatenate %65, %60, dim = 1 : (tensor<?x1xi64>, tensor<?x2xi64>) -> tensor<?x3xi64>
-    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x2x1xf32>
+    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 2, 1>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x2x1xf32>
     return %67 : tensor<?x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x4xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x4xf32>
     return %36 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
@@ -101,7 +101,7 @@
     %94 = stablehlo.concatenate %92, %93, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %95 = stablehlo.dynamic_iota %94, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %96 = stablehlo.concatenate %95, %90, dim = 1 : (tensor<?x1xi64>, tensor<?x3xi64>) -> tensor<?x4xi64>
-    %97 = "stablehlo.gather"(%arg1, %96) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2, 3], collapsed_slice_dims = [0], start_index_map = [0, 1, 2, 3], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 3, 1, 2]> : tensor<4xi64>} : (tensor<?x7x5x3xf32>, tensor<?x4xi64>) -> tensor<?x3x1x2xf32>
+    %97 = "stablehlo.gather"(%arg1, %96) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2, 3], collapsed_slice_dims = [0], start_index_map = [0, 1, 2, 3], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 3, 1, 2>} : (tensor<?x7x5x3xf32>, tensor<?x4xi64>) -> tensor<?x3x1x2xf32>
     return %97 : tensor<?x3x1x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x1x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x1x1xi64>, tensor<?x1x2xi64>) -> tensor<?x1x3xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<3xi64>} : (tensor<?x1x2xf32>, tensor<?x1x3xi64>) -> tensor<?x1xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 1>} : (tensor<?x1x2xf32>, tensor<?x1x3xi64>) -> tensor<?x1xf32>
     return %7 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x2x1xi64>, tensor<?x2x3xi64>) -> tensor<?x2x4xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2, 3], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 3, 2]> : tensor<4xi64>} : (tensor<?x2x3x3xf32>, tensor<?x2x4xi64>) -> tensor<?x2x3x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2, 3], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 3, 2>} : (tensor<?x2x3x3xf32>, tensor<?x2x4xi64>) -> tensor<?x2x3x2xf32>
     return %7 : tensor<?x2x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x4x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x4x1xi64>, tensor<?x4x2xi64>) -> tensor<?x4x3xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 3]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x3xi64>) -> tensor<?x4x3xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 3>} : (tensor<?x4x6xf32>, tensor<?x4x3xi64>) -> tensor<?x4x3xf32>
     return %7 : tensor<?x4x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
@@ -47,7 +47,7 @@
     %36 = stablehlo.select %32, %35, %22 : tensor<2xi1>, tensor<2xi64>
     %37 = stablehlo.convert %36 : (tensor<2xi64>) -> tensor<2xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %39 = "stablehlo.gather"(%28, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %39 = "stablehlo.gather"(%28, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %40 = stablehlo.constant dense<1> : tensor<i64>
     %41 = stablehlo.broadcast_in_dim %40, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %42 = stablehlo.constant dense<1> : tensor<i64>
@@ -62,7 +62,7 @@
     %51 = stablehlo.select %47, %50, %23 : tensor<2xi1>, tensor<2xi64>
     %52 = stablehlo.convert %51 : (tensor<2xi64>) -> tensor<2xi32>
     %53 = stablehlo.broadcast_in_dim %52, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %54 = "stablehlo.gather"(%44, %53) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %54 = "stablehlo.gather"(%44, %53) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %55 = stablehlo.subtract %39, %54 : tensor<2xi64>
     %56 = stablehlo.constant dense<0> : tensor<i64>
     %57 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -85,7 +85,7 @@
       %79 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %79 : tensor<i1>
     }
-    %73 = "stablehlo.gather"(%arg1, %29) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x10xf32>, tensor<?x2xi64>) -> tensor<?xf32>
+    %73 = "stablehlo.gather"(%arg1, %29) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1>} : (tensor<?x10xf32>, tensor<?x2xi64>) -> tensor<?xf32>
     %74 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %75 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %76 = stablehlo.reshape %75 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -19,7 +19,7 @@
     %8 = stablehlo.concatenate %6, %7, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %9 = stablehlo.dynamic_iota %8, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi32>
     %10 = stablehlo.concatenate %9, %4, dim = 1 : (tensor<?x1xi32>, tensor<?x1xi32>) -> tensor<?x2xi32>
-    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
+    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
     return %11 : tensor<?x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
@@ -51,7 +51,7 @@
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %44 = stablehlo.constant dense<1> : tensor<i64>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %46 = stablehlo.constant dense<1> : tensor<i64>
@@ -70,7 +70,7 @@
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %63 = stablehlo.subtract %43, %62 : tensor<2xi64>
     %64 = stablehlo.constant dense<0> : tensor<i64>
     %65 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %96 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %96 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
+    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
     %82 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %83 = stablehlo.reshape %82 : (tensor<i32>) -> tensor<1xi32>
     %84 = stablehlo.constant dense<10> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -19,7 +19,7 @@
     %8 = stablehlo.concatenate %6, %7, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %9 = stablehlo.dynamic_iota %8, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi32>
     %10 = stablehlo.concatenate %9, %4, dim = 1 : (tensor<?x1xi32>, tensor<?x1xi32>) -> tensor<?x2xi32>
-    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
+    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
     return %11 : tensor<?x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
@@ -51,7 +51,7 @@
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %44 = stablehlo.constant dense<1> : tensor<i64>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %46 = stablehlo.constant dense<10> : tensor<i64>
@@ -70,7 +70,7 @@
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %63 = stablehlo.subtract %43, %62 : tensor<2xi64>
     %64 = stablehlo.constant dense<0> : tensor<i64>
     %65 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %96 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %96 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
+    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
     %82 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %83 = stablehlo.reshape %82 : (tensor<i32>) -> tensor<1xi32>
     %84 = stablehlo.constant dense<10> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x1x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x1x1xi32>, tensor<?x1x1xi32>) -> tensor<?x1x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x1x2xi32>) -> tensor<?x10x1x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x1x2xi32>) -> tensor<?x10x1x10xf32>
     return %13 : tensor<?x10x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xi32>, tensor<?x2x1xi32>) -> tensor<?x2x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x2x10x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x2x10x10xf32>
     return %13 : tensor<?x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xi32>, tensor<?x2x1xi32>) -> tensor<?x2x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x10x10x2xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x10x10x2xf32>
     return %13 : tensor<?x10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xui32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xui32>, tensor<?x2x1xui32>) -> tensor<?x2x2xui32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xui32>) -> tensor<?x10x2x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xui32>) -> tensor<?x10x2x10xf32>
     return %13 : tensor<?x10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
     return %15 : tensor<?x2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
     return %15 : tensor<?x10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x3x1x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x3x1x1xi32>, tensor<?x3x1x1xi32>) -> tensor<?x3x1x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 4], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x3x1x2xi32>) -> tensor<?x10x3x1x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 4], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x3x1x2xi32>) -> tensor<?x10x3x1x10xf32>
     return %15 : tensor<?x10x3x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
     return %15 : tensor<?x2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
     return %15 : tensor<?x10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x5x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x5x1xi32>, tensor<?x5x1xi32>) -> tensor<?x5x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x5x2xi32>) -> tensor<?x10x5x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x5x2xi32>) -> tensor<?x10x5x10xf32>
     return %13 : tensor<?x10x5x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -25,7 +25,7 @@
     %14 = stablehlo.concatenate %9, %10, %11, %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>
     %15 = stablehlo.dynamic_iota %14, dim = 0 : (tensor<5xi32>) -> tensor<?x2x2x1x1xi32>
     %16 = stablehlo.concatenate %15, %7, dim = 4 : (tensor<?x2x2x1x1xi32>, tensor<?x2x2x1x1xi32>) -> tensor<?x2x2x1x2xi32>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [4, 5], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 4>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x2x2x1x10x10xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [4, 5], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 4>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x2x2x1x10x10xf32>
     return %17 : tensor<?x2x2x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -25,7 +25,7 @@
     %14 = stablehlo.concatenate %9, %10, %11, %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>
     %15 = stablehlo.dynamic_iota %14, dim = 0 : (tensor<5xi32>) -> tensor<?x2x2x1x1xi32>
     %16 = stablehlo.concatenate %15, %7, dim = 4 : (tensor<?x2x2x1x1xi32>, tensor<?x2x2x1x1xi32>) -> tensor<?x2x2x1x2xi32>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 4>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x10x10x2x2x1xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 4>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x10x10x2x2x1xf32>
     return %17 : tensor<?x10x10x2x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x3x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x3x1xi64>, tensor<?x3x1xi64>) -> tensor<?x3x2xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<[1, 2]> : tensor<2xi64>} : (tensor<?x10xf32>, tensor<?x3x2xi64>) -> tensor<?x3x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 2>} : (tensor<?x10xf32>, tensor<?x3x2xi64>) -> tensor<?x3x2xf32>
     return %7 : tensor<?x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x2x1xi64>, tensor<?x2x1xi64>) -> tensor<?x2x2xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2x2xi64>) -> tensor<?x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2x2xi64>) -> tensor<?x2xf32>
     return %7 : tensor<?x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {base_dilations = dense<[1, 1, 2]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x10xf32>
+    }) {base_dilations = array<i64: 1, 1, 2>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x10xf32>
     return %2 : tensor<?x3x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
     return %2 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x12x12xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x12x12xf32>
     return %2 : tensor<?x12x12xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x6x6xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x6x6xf32>
     return %2 : tensor<?x6x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dilations = dense<[1, 1, 2]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x4xf32>
+    }) {window_dilations = array<i64: 1, 1, 2>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x4xf32>
     return %2 : tensor<?x3x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<1> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 1>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x4x6xf32>
     return %2 : tensor<?x4x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2, 2, 1]> : tensor<5xi64>} : (tensor<?x1x2x4x1xf32>, tensor<f32>) -> tensor<?x1x1x3x1xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2, 2, 1>} : (tensor<?x1x2x4x1xf32>, tensor<f32>) -> tensor<?x1x1x3x1xf32>
     return %2 : tensor<?x1x1x3x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2]> : tensor<3xi64>} : (tensor<?x1x2xf32>, tensor<f32>) -> tensor<?x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2>} : (tensor<?x1x2xf32>, tensor<f32>) -> tensor<?x1x1xf32>
     return %2 : tensor<?x1x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2]> : tensor<4xi64>} : (tensor<?x2x4x3xf32>, tensor<f32>) -> tensor<?x1x3x2xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2>} : (tensor<?x2x4x3xf32>, tensor<f32>) -> tensor<?x1x3x2xf32>
     return %2 : tensor<?x1x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x2x4xf32>, tensor<f32>) -> tensor<?x1x3xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x2x4xf32>, tensor<f32>) -> tensor<?x1x3xf32>
     return %2 : tensor<?x1x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %2 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %2 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
     return %1 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
     return %2 : tensor<?x56x56xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
     return %2 : tensor<?x56x56xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<bf16>, %arg3: tensor<bf16>):
       %2 = stablehlo.multiply %arg2, %arg3 : tensor<bf16>
       stablehlo.return %2 : tensor<bf16>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xbf16>, tensor<bf16>) -> tensor<?x3x5xbf16>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xbf16>, tensor<bf16>) -> tensor<?x3x5xbf16>
     return %1 : tensor<?x3x5xbf16>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[1, 2, 3]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x6x15xf32>, tensor<?x6x15xf32>)
+    }) {base_dilations = array<i64: 1, 2, 3>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x6x15xf32>, tensor<?x6x15xf32>)
     return %2#1 : tensor<?x6x15xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dilations = dense<[1, 2, 3]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x3xf32>, tensor<?x2x3xf32>)
+    }) {window_dilations = array<i64: 1, 2, 3>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x3xf32>, tensor<?x2x3xf32>)
     return %2#1 : tensor<?x2x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x3x5xf32>, tensor<?x3x5xf32>)
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x3x5xf32>, tensor<?x3x5xf32>)
     return %2#1 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x4x6xf32>, tensor<?x4x6xf32>)
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x4x6xf32>, tensor<?x4x6xf32>)
     return %2#1 : tensor<?x4x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>, window_strides = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x2xf32>, tensor<?x2x2xf32>)
+    }) {window_dimensions = array<i64: 1, 2, 2>, window_strides = array<i64: 1, 2, 3>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x2xf32>, tensor<?x2x2xf32>)
     return %2#1 : tensor<?x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x1x3x5xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2>} : (tensor<?x2x4x6xf32>, tensor<?x1x3x5xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 3, 1]> : tensor<4xi64>, window_strides = dense<[1, 1, 2, 1]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x2x1x6xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 3, 1>, window_strides = array<i64: 1, 1, 2, 1>} : (tensor<?x2x4x6xf32>, tensor<?x2x1x6xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2, 3]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x2x3x4xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2, 3>} : (tensor<?x2x4x6xf32>, tensor<?x2x3x4xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/tests/infer_stablehlo.mlir b/stablehlo/stablehlo/tests/infer_stablehlo.mlir
--- stablehlo/stablehlo/tests/infer_stablehlo.mlir
+++ stablehlo/stablehlo/tests/infer_stablehlo.mlir
@@ -193,7 +193,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 8>
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   // CHECK: types0 = tensor<1x5x8xi32>
   %1 = "hlo_test_infer.get_return_types"(%res) : (tensor<1x5x8xi32>) -> tensor<1x5x8xindex>
@@ -212,7 +212,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 8>
   } : (tensor<?x?x?xi32, #stablehlo.bounds<2, 4, 8>>, tensor<?x?x?xi32, #stablehlo.bounds<16, 32, 64>>)
   -> tensor<?x?x8xi32>
 
@@ -467,7 +467,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.constant dense<2.0> : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   // CHECK: types0 = tensor<4x5xf32>
   %2 = "hlo_test_infer.get_return_types"(%0) : (tensor<4x5xf32>) -> tensor<4x5xindex>
   func.return %2 : tensor<4x5xindex>
@@ -619,8 +619,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
         tensor<10x24x24x64xf32>
   // CHECK: types0 = tensor<10x24x24x64xf32>
@@ -647,8 +647,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<f64>
     "stablehlo.return"(%2) : (tensor<f64>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
         tensor<10x24x24x64xf64>
   %2 = "hlo_test_infer.get_return_types"(%1) : (tensor<10x24x24x64xf64>) -> tensor<10x24x24x64xindex>
@@ -879,7 +879,7 @@
   ^bb0(%arg2: tensor<5xf32>, %arg3: tensor<5xf32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
     "stablehlo.return"(%1) : (tensor<5xf32>) -> ()
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<7x5xf32>, tensor<5xf32>) -> tensor<5xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<7x5xf32>, tensor<5xf32>) -> tensor<5xf32>
   // CHECK: types0 = tensor<5xf32>
   %2 = "hlo_test_infer.get_return_types"(%0)
       : (tensor<5xf32>) -> tensor<5xindex>
@@ -899,7 +899,7 @@
   ^bb0(%arg2: tensor<4xf32>, %arg3: tensor<4xf32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
     "stablehlo.return"(%1) : (tensor<4xf32>) -> ()
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x?xf32>, tensor<4xf32>) -> tensor<?xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4x?xf32>, tensor<4xf32>) -> tensor<?xf32>
   %1 = "hlo_test_infer.reify_return_type_shapes"(%result): (tensor<?xf32>) -> tensor<1xindex>
   func.return %1: tensor<1xindex>
 }
@@ -913,7 +913,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<*xf32>, tensor<f32>) -> tensor<*xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<*xf32>, tensor<f32>) -> tensor<*xf32>
   // CHECK: types0 = tensor<*xf32>
   %2 = "hlo_test_infer.get_return_types"(%0)
       : (tensor<*xf32>) -> tensor<*xindex>
@@ -929,7 +929,7 @@
   ^bb0(%arg2: tensor<5xf32>, %arg3: tensor<5xf32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
     "stablehlo.return"(%1) : (tensor<5xf32>) -> ()
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<7x5xf32>, tensor<5xf32>) -> tensor<6xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<7x5xf32>, tensor<5xf32>) -> tensor<6xf32>
   func.return %0: tensor<6xf32>
 }
 
@@ -945,7 +945,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf32>
 
   func.return %0: tensor<4xf32>
 }
@@ -963,7 +963,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<i32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>, tensor<?xi32>)
 
   func.return %0#0: tensor<?xf32>
 }
@@ -981,7 +981,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<i32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?x?xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?x?xf32>)
 
   func.return %0#0: tensor<?xf32>
 }
@@ -998,7 +998,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xi32>
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xi32>
 
   func.return %0: tensor<?xi32>
 }
@@ -1030,8 +1030,9 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
+         }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
 
@@ -1055,8 +1056,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               tensor<2x2xf32>
@@ -1078,8 +1079,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x3xi32>)
@@ -1101,8 +1102,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xi32>, tensor<2x2xi32>)
@@ -1121,8 +1122,8 @@
               "stablehlo.return"(%1) : (tensor<f64>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<f32>) -> (tensor<2x2xf32>)
   func.return %0 : tensor<2x2xf32>
@@ -1561,7 +1562,7 @@
       collapsed_slice_dims = [0],
       start_index_map = [1, 0],
       index_vector_dim = 2>,
-      slice_sizes = dense<[1, 2, 2]> : tensor<3xi64>,
+      slice_sizes = array<i64: 1, 2, 2>,
       indices_are_sorted = false
   } : (tensor<3x4x2xi32>, tensor<?x3x2xi64>) -> tensor<?x3x2x2xi32>
   %1 = "hlo_test_infer.reify_return_type_shapes"(%result) : (tensor<?x3x2x2xi32>) -> tensor<4xindex>
@@ -1631,7 +1632,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<5xf32>, tensor<5xf32>) -> tensor<5xf32>
     "stablehlo.return"(%1) : (tensor<5xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
       : (tensor<?x?x5xf32, #stablehlo.bounds<3, 7, ?>>, tensor<5xf32>)
           -> tensor<?x5xf32, #stablehlo.bounds<7, ?>>
 
@@ -1653,7 +1654,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
       : (tensor<?xf32, #stablehlo.bounds<3>>, tensor<f32>)
           -> tensor<*xf32>
 
@@ -1820,8 +1821,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<?x24x24x64xf32, #stablehlo.bounds<10, ?, ?, ?>>,
        tensor<?x12x12x64xf32, #stablehlo.bounds<10, ?, ?, ?>>,
        tensor<f32>) -> tensor<*xf32>
@@ -1841,8 +1842,8 @@
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
   }) {
     padding = dense<[[0, 0], [0, 0], [2, 2], [0, 0]]> : tensor<4x2xi64>,
-    window_dimensions = dense<[1, 1, 5, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 1, 3, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 1, 5, 1>,
+    window_strides = array<i64: 1, 1, 3, 1>
   } : (tensor<4x?x?x?xf32, #stablehlo.bounds<?, ?, 4, 2>>,
        tensor<f32>) -> (tensor<*xf32>)
   // CHECK: types0 = tensor<4x?x?x?xf32, #stablehlo.bounds<?, ?, 2, 2>>
diff --ruN a/stablehlo/stablehlo/tests/interpret/gather.mlir b/stablehlo/stablehlo/tests/interpret/gather.mlir
--- stablehlo/stablehlo/tests/interpret/gather.mlir
+++ stablehlo/stablehlo/tests/interpret/gather.mlir
@@ -12,7 +12,7 @@
       collapsed_slice_dims = [0],
       start_index_map = [1, 0],
       index_vector_dim = 2>,
-    slice_sizes = dense<[1, 2, 2]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 2, 2>,
     indices_are_sorted = false
   } : (tensor<3x4x2xi64>, tensor<2x3x2xi64>) -> tensor<2x3x2x2xi64>
   check.expect_eq_const %result, dense<[[[[1, 2], [3, 4]],
diff --ruN a/stablehlo/stablehlo/tests/interpret/map.mlir b/stablehlo/stablehlo/tests/interpret/map.mlir
--- stablehlo/stablehlo/tests/interpret/map.mlir
+++ stablehlo/stablehlo/tests/interpret/map.mlir
@@ -8,7 +8,7 @@
       %0 = stablehlo.multiply %arg0, %arg1 : tensor<i64>
       stablehlo.return %0 : tensor<i64>
   }) {
-    dimensions = dense<[0, 1]> : tensor<2xi64>
+    dimensions = array<i64: 0, 1>
   } : (tensor<2x2xi64>, tensor<2x2xi64>) -> tensor<2x2xi64>
   check.expect_eq_const %result, dense<[[0, 5], [12, 21]]> : tensor<2x2xi64>
   func.return
diff --ruN a/stablehlo/stablehlo/tests/interpret/reduce.mlir b/stablehlo/stablehlo/tests/interpret/reduce.mlir
--- stablehlo/stablehlo/tests/interpret/reduce.mlir
+++ stablehlo/stablehlo/tests/interpret/reduce.mlir
@@ -8,7 +8,7 @@
       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
       stablehlo.return %0 : tensor<i64>
   }) {
-    dimensions = dense<1> : tensor<1xi64>
+    dimensions = array<i64: 1>
   } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64>
   check.expect_eq_const %result, dense<[15]> : tensor<1xi64>
   func.return
diff --ruN a/stablehlo/stablehlo/tests/interpret/reduce_window.mlir b/stablehlo/stablehlo/tests/interpret/reduce_window.mlir
--- stablehlo/stablehlo/tests/interpret/reduce_window.mlir
+++ stablehlo/stablehlo/tests/interpret/reduce_window.mlir
@@ -8,11 +8,11 @@
       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
       stablehlo.return %0 : tensor<i64>
   }) {
-    base_dilations = dense<[2, 1]> : tensor<2xi64>,
+    base_dilations = array<i64: 2, 1>,
     padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
-    window_dilations = dense<[3, 1]> : tensor<2xi64>,
-    window_dimensions = dense<[2, 1]> : tensor<2xi64>,
-    window_strides = dense<[4, 1]> : tensor<2xi64>
+    window_dilations = array<i64: 3, 1>,
+    window_dimensions = array<i64: 2, 1>,
+    window_strides = array<i64: 4, 1>
   } : (tensor<3x2xi64>, tensor<i64>) -> tensor<2x2xi64>
   check.expect_eq_const %result, dense<[[0, 0], [3, 4]]> : tensor<2x2xi64>
   func.return
@@ -28,11 +28,11 @@
       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
       stablehlo.return %0 : tensor<i64>
   }) {
-    base_dilations = dense<[2, 1]> : tensor<2xi64>,
+    base_dilations = array<i64: 2, 1>,
     padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
-    window_dilations = dense<[3, 1]> : tensor<2xi64>,
-    window_dimensions = dense<[3, 1]> : tensor<2xi64>,
-    window_strides = dense<[4, 1]> : tensor<2xi64>
+    window_dilations = array<i64: 3, 1>,
+    window_dimensions = array<i64: 3, 1>,
+    window_strides = array<i64: 4, 1>
   } : (tensor<3x2xi64>, tensor<i64>) -> tensor<1x2xi64>
   check.expect_eq_const %result, dense<[[5, 6]]> : tensor<1x2xi64>
   func.return
diff --ruN a/stablehlo/stablehlo/tests/interpret/select_and_scatter.mlir b/stablehlo/stablehlo/tests/interpret/select_and_scatter.mlir
--- stablehlo/stablehlo/tests/interpret/select_and_scatter.mlir
+++ stablehlo/stablehlo/tests/interpret/select_and_scatter.mlir
@@ -17,8 +17,8 @@
       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
       stablehlo.return %0 : tensor<i64>
   }) {
-    window_dimensions = dense<[3, 1]> : tensor<2xi64>,
-    window_strides = dense<[2, 1]> : tensor<2xi64>,
+    window_dimensions = array<i64: 3, 1>,
+    window_strides = array<i64: 2, 1>,
     padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64>
   } : (tensor<4x2xi64>, tensor<2x2xi64>, tensor<i64>) -> tensor<4x2xi64>
   check.expect_eq_const %result, dense<[[0, 0],
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo.mlir b/stablehlo/stablehlo/tests/ops_stablehlo.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo.mlir
@@ -1122,14 +1122,6 @@
 func.func @broadcast_in_dim_c5(%arg0: tensor<3xi32>) -> tensor<1x2x3xi32> {
   // expected-error@+1 {{size of operand dimension 0 (3) is not equal to 1 or size of result dimension 1 (2)}}
   %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = array<i64: 1>} : (tensor<3xi32>) -> tensor<1x2x3xi32>
-  func.return %0 : tensor<1x2x3xi32>
-}
-
-// -----
-
-func.func @broadcast_in_dim_i2(%arg0: tensor<1x2xi32>) -> tensor<1x2x3xi32> {
-  // expected-error@+1 {{failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr}}
-  %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = dense<[[1,1],[1,1]]> : tensor<2x2xi64>} : (tensor<1x2xi32>) -> tensor<1x2x3xi32>
   func.return %0 : tensor<1x2x3xi32>
 }
 
@@ -1851,7 +1843,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.constant dense<2.0> : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1864,7 +1856,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[1, 0]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 1, 0>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1877,7 +1869,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1, 2>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1890,7 +1882,7 @@
     ^bb0(%arg: tensor<f32>):
     %1 = stablehlo.add %arg, %arg : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
   func.return %0 : tensor<4xf32>
 }
 
@@ -1903,7 +1895,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<5xf32>):
     %1 = stablehlo.constant dense<2.0> : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1916,7 +1908,7 @@
     ^bb0(%arg2: tensor<i32>, %arg3: tensor<i32>):
     %1 = stablehlo.constant dense<2.0> : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1929,7 +1921,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.constant dense<2.0> : tensor<f32>
     "stablehlo.return"() : () -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1942,7 +1934,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.constant dense<2.0> : tensor<5xf32>
     "stablehlo.return"(%1) : (tensor<5xf32>) -> ()
-  }) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
+  }) {dimensions = array<i64: 0, 1>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
   func.return %0 : tensor<4x5xf32>
 }
 
@@ -1953,20 +1945,8 @@
   %0 = "stablehlo.map"(%arg0, %arg1) ({
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<i32>):
     "stablehlo.return"(%arg2) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<2xf32>, tensor<2xi32>) -> tensor<2xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<2xf32>, tensor<2xi32>) -> tensor<2xf32>
   func.return %0 : tensor<2xf32>
-}
-
-// -----
-
-func.func @map_i2(%arg0: tensor<4x5xf32>, %arg1: tensor<4x5xf32>) -> tensor<4x5xf32> {
-  // expected-error@+1 {{attribute 'dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0 = "stablehlo.map"(%arg0, %arg1) ({
-    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
-    %1 = stablehlo.constant dense<2.0> : tensor<f32>
-    "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[[0, 1]]> : tensor<1x2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
-  func.return %0 : tensor<4x5xf32>
 }
 
 // -----
@@ -1977,7 +1957,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<f32>) -> tensor<f32>
+  }) {dimensions = array<i64>} : (tensor<f32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
 
@@ -1989,7 +1969,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
   func.return %0 : tensor<*xf32>
 }
 
@@ -3562,7 +3542,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3579,7 +3559,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8, 1, 7, 1, 6, 1]> : tensor<8xi64>,
+    slice_sizes = array<i64: 1, 1, 8, 1, 7, 1, 6, 1>,
     indices_are_sorted = false
   } : (tensor<*xi32>, tensor<1x5x2xi32>) -> tensor<8x?x7x1x6x1x?xi32>
   func.return %res : tensor<8x?x7x1x6x1x?xi32>
@@ -3596,7 +3576,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<*xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3612,7 +3592,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<*xi32>, tensor<*xi32>) -> tensor<*xi32>
   func.return %res : tensor<*xi32>
@@ -3630,7 +3610,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3648,7 +3628,7 @@
       start_index_map = [0, 1],
       index_vector_dim = -1
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3666,7 +3646,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 4
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3684,7 +3664,7 @@
       start_index_map = [0],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3702,7 +3682,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 3
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3720,7 +3700,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 1
     >,
-    slice_sizes = dense<[8, 6]> : tensor<2xi64>,
+    slice_sizes = array<i64: 8, 6>,
     indices_are_sorted = false
   } : (tensor<16x11xi32>, tensor<5x2xi32>) -> tensor<5x8x6xi32>
   func.return %res : tensor<5x8x6xi32>
@@ -3738,7 +3718,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 1
     >,
-    slice_sizes = dense<[8, 6]> : tensor<2xi64>,
+    slice_sizes = array<i64: 8, 6>,
     indices_are_sorted = false
   } : (tensor<16x11xi32>, tensor<5x2xi32>) -> tensor<5x8x6xi32>
   func.return %res : tensor<5x8x6xi32>
@@ -3756,7 +3736,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3774,7 +3754,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3792,7 +3772,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3810,7 +3790,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3828,7 +3808,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3846,7 +3826,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3864,7 +3844,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<*xi32>, tensor<*xi32>) -> tensor<*xi32>
   func.return %res : tensor<*xi32>
@@ -3882,7 +3862,7 @@
       start_index_map = [0, 0],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3900,7 +3880,7 @@
       start_index_map = [-2, -1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3918,7 +3898,7 @@
       start_index_map = [0, 3],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3936,7 +3916,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 8]> : tensor<2xi64>,
+    slice_sizes = array<i64: 1, 8>,
     indices_are_sorted = false
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   func.return %res : tensor<1x5x8xi32>
@@ -3954,7 +3934,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8, 1, 2, 3]> : tensor<6xi64>,
+    slice_sizes = array<i64: 1, 1, 8, 1, 2, 3>,
     indices_are_sorted = false
   } : (tensor<*xi32>, tensor<*xi32>) -> tensor<*xi32>
   func.return %res : tensor<*xi32>
@@ -3972,7 +3952,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, -1]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, -1>,
     indices_are_sorted = false
   } : (tensor<?x?x2xi32>, tensor<*xi32>) -> tensor<*xi32>
   func.return %res : tensor<*xi32>
@@ -3990,7 +3970,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 8>,
     indices_are_sorted = false
   } : (tensor<?x?x2xi32>, tensor<*xi32>) -> tensor<*xi32>
   func.return %res : tensor<*xi32>
@@ -4009,7 +3989,7 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 8>
   } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<3xi32>
   func.return %res : tensor<3xi32>
 }
@@ -4027,26 +4007,9 @@
       start_index_map = [0, 1]
     >,
     indices_are_sorted = false,
-    slice_sizes = dense<[1, 1, 8, 1, 7, 1, 6, 1]> : tensor<8xi64>
+    slice_sizes = array<i64: 1, 1, 8, 1, 7, 1, 6, 1>
   } : (tensor<*xi32>, tensor<?x?x?xi32>) -> tensor<3xi32>
   func.return %res : tensor<3xi32>
-}
-
-// -----
-
-func.func @gather_i7(%operand : tensor<2x4x9xi32>, %start_indices : tensor<1x5x2xi32>) -> tensor<1x5x8xi32> {
-  // expected-error@+1 {{attribute 'slice_sizes' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %res = "stablehlo.gather"(%operand, %start_indices) {
-    dimension_numbers = #stablehlo.gather<
-      offset_dims = [2],
-      collapsed_slice_dims = [0, 1],
-      start_index_map = [0, 1],
-      index_vector_dim = 2
-    >,
-    slice_sizes = dense<[[1, 1, 8]]> : tensor<1x3xi64>,
-    indices_are_sorted = false
-  } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
-  func.return %res : tensor<1x5x8xi32>
 }
 
 // -----
@@ -5792,7 +5755,7 @@
 
 // CHECK-LABEL: func @broadcast_in_dim_elements
 func.func @broadcast_in_dim_elements(%arg0: tensor<1x2xi32>) -> tensor<1x2x2xi32> {
-  %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = dense<[1, 2]> : tensor<2xi64>} : (tensor<1x2xi32>) -> tensor<1x2x2xi32>
+  %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = array<i64: 1, 2>} : (tensor<1x2xi32>) -> tensor<1x2x2xi32>
   func.return %0 : tensor<1x2x2xi32>
 }
 
@@ -5812,7 +5775,7 @@
 // CHECK: reverse = [true, false]
 func.func @convolution_elements(%arg0: tensor<1x8x8x207xf32>, %arg1: tensor<3x3x207x16xf32>) -> tensor<1x6x6x16xf32> {
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_reversal = dense<[true, false]> : tensor<2xi1>,
+    window_reversal = array<i1: true, false>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir b/stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo_roundtrip.mlir
@@ -210,10 +210,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) -> tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
 }
@@ -234,10 +234,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xi8>, tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32>
   func.return %result : tensor<100x28x28x1xi32>
 }
@@ -258,11 +258,11 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>,
+    window_reversal = array<i1: true, true>
   } : (tensor<100x26x26x32xi8>, tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32>
   func.return %result : tensor<100x28x28x1xi32>
 }
@@ -370,7 +370,7 @@
       start_index_map = [0,1],
     >,
     indices_are_sorted = true,
-    slice_sizes = dense<[1, 1, 300]> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 300>
   } : (tensor<200x100x300xf32>, tensor<10x2xi32>) -> tensor<10x300xf32>
   func.return %0 : tensor<10x300xf32>
 }
@@ -415,7 +415,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
     %1 = stablehlo.add %arg2, %arg3 : tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
   func.return %0 : tensor<4xf32>
 }
 
@@ -423,7 +423,7 @@
   %0 = "stablehlo.map"(%arg0, %arg1) ({
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<i32>):
     "stablehlo.return"(%arg2) : (tensor<f32>) -> ()
-  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<4xf32>, tensor<4xi32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4xf32>, tensor<4xi32>) -> tensor<4xf32>
   func.return %0 : tensor<4xf32>
 }
 
@@ -493,7 +493,7 @@
       %fmax = "stablehlo.maximum"(%fa, %fb) {} : (tensor<f32>, tensor<f32>) -> tensor<f32>
       %imax = "stablehlo.maximum"(%ia, %ib) {} : (tensor<i32>, tensor<i32>) -> tensor<i32>
       "stablehlo.return"(%fmax, %imax) : (tensor<f32>, tensor<i32>) -> ()
-    }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<1x10xf32>, tensor<1x10xi32>, tensor<f32>, tensor<i32>) -> (tensor<1xf32>, tensor<1xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<1x10xf32>, tensor<1x10xi32>, tensor<f32>, tensor<i32>) -> (tensor<1xf32>, tensor<1xi32>)
   func.return %result0, %result1 : tensor<1xf32>, tensor<1xi32>
 }
 
@@ -545,11 +545,11 @@
     %2 = stablehlo.maximum %arg1, %arg2 : tensor<i32>
     "stablehlo.return"(%2) : (tensor<i32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>,
-    base_dilations = dense<[1, 1, 1, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    base_dilations = array<i64: 1, 1, 1, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xi32>, tensor<i32>) -> tensor<2x5x8x7xi32>
   func.return %1 : tensor<2x5x8x7xi32>
 }
@@ -562,8 +562,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> } : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) -> (tensor<2x2xf32>, tensor<2x2xi32>)
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> } : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) -> (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
 }
 
@@ -643,8 +643,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
     "stablehlo.return"(%2) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %1 : tensor<10x24x24x64xf32>
 }
diff --ruN a/stablehlo/stablehlo/tests/print_reduce.mlir b/stablehlo/stablehlo/tests/print_reduce.mlir
--- stablehlo/stablehlo/tests/print_reduce.mlir
+++ stablehlo/stablehlo/tests/print_reduce.mlir
@@ -20,7 +20,7 @@
   ^bb0(%arg2: tensor<f32> loc("foo"), %arg3: tensor<f32> loc("foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1) : (tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
 
   func.return %0: tensor<?xf32>
 }
@@ -36,7 +36,7 @@
   ^bb0(%arg2: tensor<f32> loc("foo"), %arg3: tensor<f32> loc("foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1) : (tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>, foo = "bar"} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("not_foo")
+  }) {dimensions = array<i64: 1>, foo = "bar"} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("not_foo")
 
   func.return %0: tensor<?xf32>
 }
@@ -52,7 +52,7 @@
   ^bb0(%arg2: tensor<f32> loc("foo"), %arg3: tensor<f32> loc("not_foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1) : (tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
 
   func.return %0: tensor<?xf32>
 }
@@ -70,7 +70,7 @@
   ^bb0(%arg4: tensor<f32> loc("foo"), %arg5: tensor<f32> loc("foo"), %arg6: tensor<f32> loc("foo"), %arg7: tensor<f32> loc("foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1, %1) : (tensor<f32>, tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)  loc("foo")
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)  loc("foo")
 
   func.return %0#0: tensor<?xf32>
 }
@@ -86,7 +86,7 @@
   ^bb0(%arg2: tensor<f32> loc("foo"), %arg3: tensor<f32> loc("foo")):
     %1 = "stablehlo.divide"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1) : (tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
+    }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
 
   func.return %0: tensor<?xf32>
 }
@@ -102,7 +102,7 @@
   ^bb0(%arg2: tensor<f32> loc("foo"), %arg3: tensor<f32> loc("foo")):
     %1 = "stablehlo.reshape"(%arg2) : (tensor<f32>) -> tensor<f32> loc("foo")
     "stablehlo.return"(%1) : (tensor<f32>) -> () loc("foo")
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
+    }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32> loc("foo")
 
   func.return %0: tensor<?xf32>
 }
@@ -120,7 +120,7 @@
     %1 = stablehlo.add %arg4, %arg6 : tensor<f32> loc("foo")
     %2 = stablehlo.add %arg5, %arg7 : tensor<i32> loc("foo")
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<i32>) -> () loc("foo")
-  }) {dimensions = dense<0> : tensor<1xi64>}
+  }) {dimensions = array<i64: 0>}
     : (tensor<1x8xf32>, tensor<1x8xi32>, tensor<f32>, tensor<i32>) -> (tensor<8xf32>, tensor<8xi32>) loc("foo")
 
   func.return %0#0, %0#1 : tensor<8xf32>, tensor<8xi32>
@@ -136,7 +136,7 @@
   ^bb0(%arg2: tensor<complex<f32>> loc("foo"), %arg3: tensor<complex<f32>> loc("foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<complex<f32>>, tensor<complex<f32>>) -> tensor<complex<f32>> loc("foo")
     "stablehlo.return"(%1) : (tensor<complex<f32>>) -> () loc("foo")
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<1x2xcomplex<f32>>, tensor<complex<f32>>) -> tensor<1xcomplex<f32>> loc("foo")
+  }) {dimensions = array<i64: 1>} : (tensor<1x2xcomplex<f32>>, tensor<complex<f32>>) -> tensor<1xcomplex<f32>> loc("foo")
 
   func.return %0: tensor<1xcomplex<f32>>
 }
@@ -158,7 +158,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
     "stablehlo.return"(%1) : (tensor<4xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<4xf32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<4xf32>) -> tensor<4xf32>
 
   func.return %0: tensor<4xf32>
 }
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
@@ -214,9 +214,9 @@
   %0 = stablehlo.constant dense<2> : tensor<2x2xi32>
   %1 = "stablehlo.dynamic_conv"(%arg0, %arg1, %0) {
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>,
-    window_strides = dense<1> : tensor<2xi64>,
-    lhs_dilation = dense<1> : tensor<2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
+    window_strides = array<i64: 1, 1>,
+    lhs_dilation = array<i64: 1, 1>,
+    rhs_dilation = array<i64: 1, 1>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>, tensor<2x2xi32>) -> tensor<100x28x28x1xf32>
@@ -242,9 +242,9 @@
   %0 = stablehlo.constant dense<2> : tensor<2x2xi32>
   %1 = "stablehlo.dynamic_conv"(%arg0, %arg1, %0) {
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>,
-    window_strides = dense<1> : tensor<2xi64>,
-    lhs_dilation = dense<1> : tensor<2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
+    window_strides = array<i64: 1, 1>,
+    lhs_dilation = array<i64: 1, 1>,
+    rhs_dilation = array<i64: 1, 1>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>, tensor<2x2xi32>) -> tensor<?x28x28x1xf32>
@@ -258,9 +258,9 @@
   // CHECK: stablehlo.dynamic_conv
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>,
-    window_strides = dense<1> : tensor<2xi64>,
-    lhs_dilation = dense<1> : tensor<2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
+    window_strides = array<i64: 1, 1>,
+    lhs_dilation = array<i64: 1, 1>,
+    rhs_dilation = array<i64: 1, 1>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>, tensor<2x2xi32>) -> tensor<100x28x28x1xf32>
@@ -279,7 +279,7 @@
   // CHECK-SAME:     start_index_map = [0, 1],
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
-  // CHECK-SAME:   slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+  // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
   // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
   %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
@@ -305,7 +305,7 @@
   // CHECK-SAME:     start_index_map = [0, 1],
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
-  // CHECK-SAME:   slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+  // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
   // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x?xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
   %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
@@ -569,9 +569,9 @@
   %0 = stablehlo.constant dense<[[2, 2], [2, 2]]> : tensor<2x2xi32>
   %1 = "stablehlo.dynamic_conv"(%arg0, %arg1, %0) {
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>,
-    window_strides = dense<[1, 1]> : tensor<2xi64>,
-    lhs_dilation = dense<[1, 1]> : tensor<2xi64>,
-    rhs_dilation = dense<[1, 1]> : tensor<2xi64>,
+    window_strides = array<i64: 1, 1>,
+    lhs_dilation = array<i64: 1, 1>,
+    rhs_dilation = array<i64: 1, 1>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>, tensor<2x2xi32>) -> tensor<*xf32>
diff --ruN a/stablehlo/stablehlo/tests/verify_conv.mlir b/stablehlo/stablehlo/tests/verify_conv.mlir
--- stablehlo/stablehlo/tests/verify_conv.mlir
+++ stablehlo/stablehlo/tests/verify_conv.mlir
@@ -19,10 +19,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) ->
     tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
@@ -49,10 +49,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xi8>, tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32>
   func.return %result : tensor<100x28x28x1xi32>
 }
@@ -209,10 +209,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) ->
     tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
@@ -237,10 +237,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) ->
     tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
@@ -265,10 +265,10 @@
       output_spatial_dimensions = [0, 3]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<2x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) ->
     tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
@@ -541,10 +541,10 @@
       output_spatial_dimensions = [1, 2]
     >,
     feature_group_count = 1 : i64,
-    lhs_dilation = dense<1> : tensor<2xi64>,
+    lhs_dilation = array<i64: 1, 1>,
     padding = dense<2> : tensor<3x2xi64>,
-    rhs_dilation = dense<1> : tensor<2xi64>,
-    window_strides = dense<1> : tensor<2xi64>
+    rhs_dilation = array<i64: 1, 1>,
+    window_strides = array<i64: 1, 1>
   } : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) ->
     tensor<100x28x28x1xf32>
   func.return %result : tensor<100x28x28x1xf32>
@@ -820,7 +820,7 @@
       output_batch_dimension = 1,
       output_feature_dimension = 4,
       output_spatial_dimensions = [2, 3]
-    >, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, padding = dense<1> : tensor<2x2xi64>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>], rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} :
+      >, feature_group_count = 1 : i64, lhs_dilation = array<i64: 1, 1>, padding = dense<1> : tensor<2x2xi64>, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>], rhs_dilation = array<i64: 1, 1>, window_strides = array<i64: 1, 1>} :
        (tensor<1x8x8x32x207xf32>, tensor<3x3x32x207x16xf32>) -> tensor<32x1x8x8x16xf32>
   func.return %0 : tensor<32x1x8x8x16xf32>
 }
diff --ruN a/stablehlo/stablehlo/tests/verify_reduce.mlir b/stablehlo/stablehlo/tests/verify_reduce.mlir
--- stablehlo/stablehlo/tests/verify_reduce.mlir
+++ stablehlo/stablehlo/tests/verify_reduce.mlir
@@ -11,7 +11,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
     "stablehlo.return"(%1) : (tensor<4xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<4xf32>) -> tensor<4xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<4xf32>) -> tensor<4xf32>
 
   func.return %0: tensor<4xf32>
 }
@@ -26,7 +26,7 @@
   ^bb0(%arg2: tensor<complex<f32>> loc("foo"), %arg3: tensor<complex<f32>> loc("foo")):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<complex<f32>>, tensor<complex<f32>>) -> tensor<complex<f32>> loc("foo")
     "stablehlo.return"(%1) : (tensor<complex<f32>>) -> () loc("foo")
-  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<1x2xcomplex<f32>>, tensor<complex<f32>>) -> tensor<1xcomplex<f32>> loc("foo")
+  }) {dimensions = array<i64: 1>} : (tensor<1x2xcomplex<f32>>, tensor<complex<f32>>) -> tensor<1xcomplex<f32>> loc("foo")
 
   func.return %0: tensor<1xcomplex<f32>>
 }
@@ -54,7 +54,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
     "stablehlo.return"(%1, %2) : (tensor<*xf32>, tensor<*xf32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<4x4xf32>, tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>, tensor<*xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<4x4xf32>, tensor<4x4xf32>, tensor<*xf32>, tensor<*xf32>) -> (tensor<*xf32>, tensor<*xf32>)
 
   func.return %0#0, %0#1 : tensor<*xf32>, tensor<*xf32>
 }
@@ -72,7 +72,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
     "stablehlo.return"(%1) : (tensor<4xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<8x?xf32>, tensor<4xf32>) -> tensor<?xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<8x?xf32>, tensor<4xf32>) -> tensor<?xf32>
 
   func.return %0: tensor<?xf32>
 }
@@ -89,7 +89,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
     "stablehlo.return"(%1, %2) : (tensor<4xf32>, tensor<*xf32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<*xf32>, tensor<4xf32>, tensor<*xf32>) -> (tensor<4xf32>, tensor<*xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<4x4xf32>, tensor<*xf32>, tensor<4xf32>, tensor<*xf32>) -> (tensor<4xf32>, tensor<*xf32>)
 
   func.return %0#0, %0#1 : tensor<4xf32>, tensor<*xf32>
 }
@@ -105,7 +105,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -136,7 +136,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<2x3xf32>, tensor<3x2xf32>, tensor<f32>, tensor<f32>) -> (tensor<2xf32>, tensor<2xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<2x3xf32>, tensor<3x2xf32>, tensor<f32>, tensor<f32>) -> (tensor<2xf32>, tensor<2xf32>)
 
   func.return %0#0, %0#1 : tensor<2xf32>, tensor<2xf32>
 }
@@ -154,7 +154,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xf32>
 }
@@ -172,7 +172,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%2, %1) : (tensor<i32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xi32>
 }
@@ -190,7 +190,7 @@
     %2 = "stablehlo.maximum"(%arg4, %arg6) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xi32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xi32>
 }
@@ -206,7 +206,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[-1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
+  }) {dimensions = array<i64: -1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
 
   func.return %0: tensor<?xf32>
 }
@@ -222,7 +222,7 @@
   ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[2]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
+  }) {dimensions = array<i64: 2>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
 
   func.return %0: tensor<?xf32>
 }
@@ -237,7 +237,7 @@
   ^bb0(%arg2: tensor<*xf32>, %arg3: tensor<*xf32> ):
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
     "stablehlo.return"(%1) : (tensor<*xf32>) -> ()
-  }) {dimensions = dense<[-1]> : tensor<1xi64>} : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
+  }) {dimensions = array<i64: -1>} : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>
   func.return %0: tensor<*xf32>
 }
 
@@ -253,7 +253,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1,1]> : tensor<2xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
+  }) {dimensions = array<i64: 1, 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
 
   func.return %0: tensor<?xf32>
 }
@@ -273,7 +273,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%arg5, %arg7) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xf32>
 }
@@ -290,7 +290,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"() : () -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<f32>
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<f32>) -> tensor<f32>
 
     func.return %0: tensor<f32>
 }
@@ -307,7 +307,7 @@
     %1 = "stablehlo.add"(%arg4, %arg6) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1) : (tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xf32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xf32>
 }
@@ -326,7 +326,7 @@
     %3 = "stablehlo.tuple"(%1, %2) : (tensor<f32>, tensor<i32>) -> tuple<tensor<f32>, tensor<i32>>
     "stablehlo.return"(%3, %1) : (tuple<tensor<f32>, tensor<i32>>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xi32>
 }
@@ -344,7 +344,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
     "stablehlo.return"(%1, %2) : (tensor<i32>, tensor<i32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xi32>
 }
@@ -362,7 +362,7 @@
     %2 = "stablehlo.add"(%arg5, %arg7) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xi32>
 }
@@ -380,7 +380,7 @@
     %2 = "stablehlo.maximum"(%arg4, %arg6) : (tensor<f32>, tensor<f32>) -> tensor<f32>
     "stablehlo.return"(%1, %2) : (tensor<f32>, tensor<f32>) -> ()
 
-  }) {dimensions = dense<[1]> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
+  }) {dimensions = array<i64: 1>} : (tensor<?x?xf32>, tensor<?x?xi32>, tensor<f32>, tensor<f32>) -> (tensor<?xf32>, tensor<?xf32>)
 
   func.return %0#0, %0#1 : tensor<?xf32>, tensor<?xf32>
 }
@@ -397,7 +397,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
     "stablehlo.return"(%1) : (tensor<?xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<?xf32>, tensor<?xf32>) -> tensor<f32>
+  }) {dimensions = array<i64: 0>} : (tensor<?xf32>, tensor<?xf32>) -> tensor<f32>
 
   func.return %0: tensor<f32>
 }
@@ -414,7 +414,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
     "stablehlo.return"(%1) : (tensor<4xf32>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<8x5xf32>, tensor<4xf32>) -> tensor<5xf32>
+  }) {dimensions = array<i64: 0>} : (tensor<8x5xf32>, tensor<4xf32>) -> tensor<5xf32>
 
   func.return %0: tensor<5xf32>
 }
@@ -432,7 +432,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<i8>, tensor<i8>) -> tensor<i8>
     "stablehlo.return"(%1) : (tensor<i8>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xi32>, tensor<i32>) -> tensor<4xi8>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xi32>, tensor<i32>) -> tensor<4xi8>
 
   func.return %0: tensor<4xi8>
 }
@@ -449,7 +449,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<i8>, tensor<i8>) -> tensor<i8>
     "stablehlo.return"(%1) : (tensor<i8>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xi32>, tensor<i8>) -> tensor<4xi8>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xi32>, tensor<i8>) -> tensor<4xi8>
 
   func.return %0: tensor<4xi8>
 }
@@ -486,19 +486,6 @@
   return %0 : tensor<4x!quant.uniform<i32:f32, 2.000000e+00:15>>
 }
 
-// -----
-
-func.func @reduce_i3(%input: tensor<1x6xi64>, %init_value: tensor<i64>) -> tensor<1xi64> {
-  // expected-error@+1 {{attribute 'dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0 = "stablehlo.reduce"(%input, %init_value) ({
-    ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):
-      stablehlo.return %arg0 : tensor<i64>
-  }) {
-    dimensions = dense<1> : tensor<1x1xi64>
-  } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64>
-  func.return %0 : tensor<1xi64>
-}
-
 // The following invalid cases arises while parsing a pretty-printed version of reduce-op will "non-eligible" inner-op.
 // -----
 
diff --ruN a/stablehlo/stablehlo/tests/verify_reduce_window.mlir b/stablehlo/stablehlo/tests/verify_reduce_window.mlir
--- stablehlo/stablehlo/tests/verify_reduce_window.mlir
+++ stablehlo/stablehlo/tests/verify_reduce_window.mlir
@@ -12,8 +12,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>}
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -31,8 +31,8 @@
             })
          {
            padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[4, 2]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 4, 2>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<4xf32>) -> (tensor<2x1xf32>)
   func.return %0 : tensor<2x1xf32>
@@ -50,8 +50,8 @@
             })
          {
            padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[4, 2]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 4, 2>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<2xf32>) -> (tensor<2x1xf32>)
   func.return %0 : tensor<2x1xf32>
@@ -71,10 +71,10 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 1]> : tensor<2xi64>,
-           window_dilations = dense<[1, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>,
+           base_dilations = array<i64: 1, 1>,
+           window_dilations = array<i64: 1, 1> }
          : (tensor<*xf32>, tensor<4x?xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<?x?xf32>, tensor<*xi32>)
   func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<*xi32>
@@ -94,8 +94,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -113,8 +113,8 @@
               "stablehlo.return"(%1) : (tensor<!quant.uniform<i32:f32, 2.000000e+00:15>>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2x!quant.uniform<i8:f32, 2.000000e+00:15>>, tensor<!quant.uniform<i8:f32, 2.000000e+00:15>>) -> (tensor<2x2x!quant.uniform<i32:f32, 2.000000e+00:15>>)
   func.return %0 : tensor<2x2x!quant.uniform<i32:f32, 2.000000e+00:15>>
@@ -131,8 +131,8 @@
       "stablehlo.return"() : () -> ()
     }) {
       padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-      window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-      window_strides = dense<[3, 1]> : tensor<2xi64>
+      window_dimensions = array<i64: 5, 1>,
+      window_strides = array<i64: 3, 1>
     } : () -> ()
   func.return
 }
@@ -151,8 +151,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x3xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -172,8 +172,8 @@
               "stablehlo.return"(%2,%2) : (tensor<f32>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf32>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xf32>
@@ -193,8 +193,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5]> : tensor<1xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -214,8 +214,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 0]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 0>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -235,8 +235,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[1]> : tensor<1xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -256,8 +256,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[0, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 0, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -277,10 +277,10 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1]> : tensor<1xi64>,
-           window_dilations = dense<[1, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>,
+           base_dilations = array<i64: 1>,
+           window_dilations = array<i64: 1, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -300,10 +300,10 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 0]> : tensor<2xi64>,
-           window_dilations = dense<[1, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>,
+           base_dilations = array<i64: 1, 0>,
+           window_dilations = array<i64: 1, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -323,10 +323,10 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 1]> : tensor<2xi64>,
-           window_dilations = dense<[1]> : tensor<1xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>,
+           base_dilations = array<i64: 1, 1>,
+           window_dilations = array<i64: 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -346,10 +346,10 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 1]> : tensor<2xi64>,
-           window_dilations = dense<[0, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>,
+           base_dilations = array<i64: 1, 1>,
+           window_dilations = array<i64: 0, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -369,8 +369,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2]]> : tensor<1x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -390,8 +390,8 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2], [2], [0], [0]]> : tensor<4x1xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -409,8 +409,8 @@
               "stablehlo.return"(%2) : (tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -430,8 +430,8 @@
               "stablehlo.return"() : () -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -452,8 +452,8 @@
                   -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -476,8 +476,8 @@
                   (tuple<tensor<f32>, tensor<i32>>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -497,8 +497,8 @@
               "stablehlo.return"(%3,%2) : (tensor<i32>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -518,8 +518,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -539,8 +539,8 @@
               "stablehlo.return"(%2,%2) : (tensor<f32>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
@@ -557,7 +557,7 @@
               "stablehlo.return"(%2) : (tensor<1xf32>) -> ()
             })
          {
-           window_dimensions = dense<> : tensor<0xi64>
+           window_dimensions = array<i64>
          }
          : (tensor<f32>, tensor<1xf32>) -> (tensor<f32>)
   func.return %0 : tensor<f32>
@@ -575,8 +575,8 @@
             })
          {
            padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xf32>, tensor<4x2xf32>) -> (tensor<2x2xf32>)
   func.return %0 : tensor<2x2xf32>
@@ -594,8 +594,8 @@
               "stablehlo.return"(%1) : (tensor<i8>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xi32>, tensor<i32>) -> (tensor<2x2xi8>)
   func.return %0 : tensor<2x2xi8>
@@ -613,8 +613,8 @@
               "stablehlo.return"(%1) : (tensor<i8>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2xi32>, tensor<i8>) -> (tensor<2x2xi8>)
   func.return %0 : tensor<2x2xi8>
@@ -632,8 +632,8 @@
               "stablehlo.return"(%1) : (tensor<!quant.uniform<i32:f64, 2.000000e+00:15>>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2x!quant.uniform<i8:f32, 2.000000e+00:15>>, tensor<!quant.uniform<i8:f32, 2.000000e+00:15>>) -> (tensor<2x2x!quant.uniform<i32:f64, 2.000000e+00:15>>)
   func.return %0 : tensor<2x2x!quant.uniform<i32:f64, 2.000000e+00:15>>
@@ -651,8 +651,8 @@
               "stablehlo.return"(%1) : (tensor<!quant.uniform<i32:f32, 2.000000e+00:15>>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1>
          }
          : (tensor<4x2x!quant.uniform<i8:f64, 2.000000e+00:15>>, tensor<!quant.uniform<i8:f32, 2.000000e+00:15>>) -> (tensor<2x2x!quant.uniform<i32:f32, 2.000000e+00:15>>)
   func.return %0 : tensor<2x2x!quant.uniform<i32:f32, 2.000000e+00:15>>
@@ -672,99 +672,11 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<1xf32>, tensor<1xi32>) ->
               (tensor<2x2xf32>, tensor<2x2xi32>)
   func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
-
-// -----
-
-func.func @reduce_window_i3(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xi32>,
-                    %init0: tensor<f32>, %init1: tensor<i32>) ->
-                      (tensor<2x2xf32>, tensor<2x2xi32>) {
-  // expected-error@+1 {{attribute 'window_dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[[5, 1]]> : tensor<1x2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
-         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<2x2xf32>, tensor<2x2xi32>)
-  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
-
-// -----
-
-func.func @reduce_window_i4(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xi32>,
-                    %init0: tensor<f32>, %init1: tensor<i32>) ->
-                      (tensor<2x2xf32>, tensor<2x2xi32>) {
-  // expected-error@+1 {{attribute 'window_strides' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[[3, 1]]> : tensor<1x2xi64> }
-         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<2x2xf32>, tensor<2x2xi32>)
-  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
-
-// -----
-
-func.func @reduce_window_i5(%arg0: tensor<*xf32>,
-    %arg1: tensor<4x?xi32>, %init0: tensor<f32>, %init1: tensor<i32>) ->
-        (tensor<?x?xf32>, tensor<*xi32>) {
-  // expected-error@+1 {{attribute 'base_dilations' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[[1, 1]]> : tensor<1x2xi64>,
-           window_dilations = dense<[1, 1]> : tensor<2xi64> }
-         : (tensor<*xf32>, tensor<4x?xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<?x?xf32>, tensor<*xi32>)
-  func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<*xi32>
-}
-
-// -----
-
-func.func @reduce_window_i6(%arg0: tensor<*xf32>,
-    %arg1: tensor<4x?xi32>, %init0: tensor<f32>, %init1: tensor<i32>) ->
-        (tensor<?x?xf32>, tensor<*xi32>) {
-  // expected-error@+1 {{attribute 'window_dilations' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 1]> : tensor<2xi64>,
-           window_dilations = dense<[[1, 1]]> : tensor<1x2xi64> }
-         : (tensor<*xf32>, tensor<4x?xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<?x?xf32>, tensor<*xi32>)
-  func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<*xi32>
 }
 
 // -----
@@ -781,9 +693,9 @@
               "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
             })
          { padding = dense<[[[2, 2], [0, 0]]]> : tensor<1x2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
-         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<2x2xf32>, tensor<2x2xi32>)
-  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
+         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
+              (tensor<2x2xf32>, tensor<2x2xi32>)
+  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
+}
diff --ruN a/stablehlo/stablehlo/tests/verify_select_and_scatter.mlir b/stablehlo/stablehlo/tests/verify_select_and_scatter.mlir
--- stablehlo/stablehlo/tests/verify_select_and_scatter.mlir
+++ stablehlo/stablehlo/tests/verify_select_and_scatter.mlir
@@ -17,8 +17,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -42,8 +42,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f64>
       "stablehlo.return"(%2) : (tensor<f64>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf64>
@@ -71,8 +71,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<!quant.uniform<i32:f32, 2.000000e+00:15>>
     "stablehlo.return"(%2) : (tensor<!quant.uniform<i32:f32, 2.000000e+00:15>>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64x!quant.uniform<i8:f32, 2.000000e+00:15>>,
       tensor<10x12x12x64x!quant.uniform<i8:f32, 2.000000e+00:15>>,
       tensor<!quant.uniform<i8:f32, 2.000000e+00:15>>) ->
@@ -104,8 +104,8 @@
     "stablehlo.return"(%4) : (tensor<*xbf16>) -> ()
   }) {
     padding = dense<0> : tensor<4x2xi64>,
-    window_dimensions = dense<[2, 3, 1, 1]> : tensor<4xi64>,
-    window_strides = dense<[2, 2, 1, 1]> : tensor<4xi64>}
+    window_dimensions = array<i64: 2, 3, 1, 1>,
+    window_strides = array<i64: 2, 2, 1, 1>}
   : (tensor<4x5x1x1xbf16>, tensor<2x2x1x1xbf16>, tensor<bf16>) ->
       tensor<?x?x?x?xbf16>
   func.return %3 : tensor<?x?x?x?xbf16>
@@ -129,8 +129,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<i32>
       "stablehlo.return"(%2) : (tensor<i32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xi32>, tensor<i32>) ->
           tensor<10x24x24x64xf32>
@@ -155,8 +155,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x32xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -181,8 +181,8 @@
       %2 = stablehlo.add %arg3, %arg3 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -207,7 +207,7 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -231,8 +231,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 0]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 0>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -257,8 +257,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2]> : tensor<3xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -283,8 +283,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[0, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 0, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -310,8 +310,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<3x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -336,8 +336,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x3xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -363,8 +363,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -388,8 +388,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -414,8 +414,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -439,8 +439,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -466,8 +466,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
 
@@ -493,8 +493,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
     func.return %1 : tensor<10x24x24x64xf32>
@@ -518,8 +518,8 @@
       %2 = stablehlo.add %arg3, %arg3 : tensor<f32>
       "stablehlo.return"(%2) : (tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -544,8 +544,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"() : () -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -570,8 +570,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f32>
       "stablehlo.return"(%2, %2) : (tensor<f32>, tensor<f32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -597,8 +597,8 @@
       %3 = "stablehlo.tuple"(%2) : (tensor<f32>) -> tuple<tensor<f32>>
       "stablehlo.return"(%3) : (tuple<tensor<f32>>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -623,8 +623,8 @@
       %2 = "llvm.add" (%arg3, %arg4) : (f32, f32) -> f32
       "stablehlo.return"(%2) : (f32) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -649,8 +649,8 @@
       %2 = stablehlo.constant dense<0> : tensor<i32>
       "stablehlo.return"(%2) : (tensor<i32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -675,8 +675,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<i32>
       "stablehlo.return"(%2) : (tensor<i32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf32>
@@ -701,8 +701,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<i32>
       "stablehlo.return"(%2) : (tensor<i32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<i32>) ->
           tensor<10x24x24x64xf32>
@@ -727,8 +727,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<1xf32>
       "stablehlo.return"(%2) : (tensor<1xf32>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<1xf32>) ->
           tensor<10x24x24x64xf32>
@@ -755,8 +755,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<i8>
     "stablehlo.return"(%2) : (tensor<i8>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xi32>, tensor<10x12x12x64xi32>, tensor<i32>) ->
         tensor<10x24x24x64xi8>
   func.return %1 : tensor<10x24x24x64xi8>
@@ -780,8 +780,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<i8>
       "stablehlo.return"(%2) : (tensor<i8>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<i8>) ->
           tensor<10x24x24x64xf32>
@@ -809,8 +809,8 @@
     %2 = stablehlo.add %arg3, %arg4 : tensor<!quant.uniform<i32:f64, 2.000000e+00:15>>
     "stablehlo.return"(%2) : (tensor<!quant.uniform<i32:f64, 2.000000e+00:15>>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64x!quant.uniform<i8:f32, 2.000000e+00:15>>,
       tensor<10x12x12x64x!quant.uniform<i8:f32, 2.000000e+00:15>>,
       tensor<!quant.uniform<i8:f32, 2.000000e+00:15>>) ->
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
@@ -585,7 +585,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -676,7 +676,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -734,7 +734,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1062,11 +1062,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1215,11 +1215,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1369,7 +1369,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1485,7 +1485,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1614,7 +1614,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1675,10 +1675,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1832,8 +1832,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
@@ -596,7 +596,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -687,7 +687,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -745,7 +745,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1073,11 +1073,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1226,11 +1226,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1380,7 +1380,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1496,7 +1496,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1625,7 +1625,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1686,10 +1686,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1843,8 +1843,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
@@ -594,7 +594,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -685,7 +685,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -743,7 +743,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1090,11 +1090,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1243,11 +1243,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1397,7 +1397,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1513,7 +1513,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1642,7 +1642,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1672,7 +1672,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -1738,10 +1738,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1763,8 +1763,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -1944,8 +1944,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
@@ -1967,8 +1967,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
       "stablehlo.return"(%1) : (tensor<f64>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
   func.return %0 : tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1606,7 +1606,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1667,10 +1667,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1824,8 +1824,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
@@ -599,7 +599,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -690,7 +690,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -748,7 +748,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1095,11 +1095,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1248,11 +1248,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1402,7 +1402,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1518,7 +1518,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1647,7 +1647,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1677,7 +1677,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -1743,10 +1743,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1768,8 +1768,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -1949,8 +1949,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
@@ -1972,8 +1972,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
       "stablehlo.return"(%1) : (tensor<f64>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
   func.return %0 : tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
@@ -10,7 +10,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -63,8 +63,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -114,8 +114,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f64>
       "stablehlo.return"(%2) : (tensor<f64>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
@@ -153,7 +153,7 @@
       return rewriter.notifyMatchFailure(op, "expected static slice_sizes");
     rewriter.replaceOpWithNewOp<GatherOp>(
         op, op.getType(), op.getOperand(), op.getStartIndices(),
-        op.getDimensionNumbersAttr(), rewriter.getI64TensorAttr(sliceSizes),
+        op.getDimensionNumbersAttr(), rewriter.getDenseI64ArrayAttr(sliceSizes),
         op.getIndicesAreSortedAttr());
     return success();
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
@@ -625,10 +625,11 @@
                 std::is_same<StablehloOpTy, stablehlo::DynamicConvOp>::value) {
     auto numSpatialDimensions = static_cast<int64_t>(
         stablehloOp.getDimensionNumbers().getInputSpatialDimensions().size());
-    if (!stablehloOp.getWindowStridesAttr())
+    if (!stablehloOp.getWindowStridesAttr()) {
       addDefaultAttr("window_strides",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
+    }
     if (!stablehloOp.getPaddingAttr())
       addDefaultAttr("padding",
                      DenseIntElementsAttr::get(
@@ -637,11 +638,11 @@
                          SmallVector<int64_t>(numSpatialDimensions * 2, 0ll)));
     if (!stablehloOp.getLhsDilationAttr())
       addDefaultAttr("lhs_dilation",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
     if (!stablehloOp.getRhsDilationAttr())
       addDefaultAttr("rhs_dilation",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
     if (!stablehloOp.getWindowReversalAttr())
       addDefaultAttr("window_reversal",
diff --ruN a/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
@@ -426,19 +426,21 @@
   return success();
 }
 
-SpecialResult convertDenseI64Array(
-    StringAttr vhloName, Attribute vhloAttr,
-    SmallVector<NamedAttribute>& stablehloAttrs) {
+template <typename T, typename Attr>
+SpecialResult convertDenseArray(StringAttr vhloName, Attribute vhloAttr,
+                                SmallVector<NamedAttribute>& stablehloAttrs) {
   auto tensorAttr = dyn_cast<vhlo::TensorV1Attr>(vhloAttr);
   if (!tensorAttr) return specialFailure();
 
-  if (tensorAttr.getData().size() % sizeof(int64_t) != 0)
-    return specialFailure();
-
-  auto data = ArrayRef<int64_t>(
-                  reinterpret_cast<const int64_t*>(tensorAttr.getData().data()),
-                  tensorAttr.getData().size() / sizeof(int64_t))
-                  .vec();
+  if (tensorAttr.getData().size() % sizeof(T) != 0) return specialFailure();
+
+  // Extracting the data in this way prevents misaligned memory issues.
+  auto vec =
+      ArrayRef<T>(reinterpret_cast<const T*>(tensorAttr.getData().data()),
+                  tensorAttr.getData().size() / sizeof(T))
+          .vec();
+  SmallVector<T> data;
+  for (T b : vec) data.push_back(b);
 
   // Handle splats
   if (data.size() == 1) {
@@ -449,9 +451,22 @@
     data.resize(size, data[0]);
   }
 
-  stablehloAttrs.emplace_back(
-      vhloName, DenseI64ArrayAttr::get(vhloAttr.getContext(), data));
+  stablehloAttrs.emplace_back(vhloName, Attr::get(vhloAttr.getContext(), data));
   return specialSuccess();
+}
+
+SpecialResult convertDenseI64Array(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<int64_t, DenseI64ArrayAttr>(vhloName, vhloAttr,
+                                                       stablehloAttrs);
+}
+
+SpecialResult convertDenseBoolArray(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<bool, DenseBoolArrayAttr>(vhloName, vhloAttr,
+                                                     stablehloAttrs);
 }
 
 template <typename VhloOpTy>
@@ -535,6 +550,32 @@
         vhloName == "known_expanding_dimensions" ||
         vhloName == "known_nonexpanding_dimensions")
       return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SelectAndScatterOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ReduceWindowOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides" ||
+        vhloName == "base_dilations" || vhloName == "window_dilations")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::MapOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::ReduceOpV1>::value) {
+    if (vhloName == "dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::GatherOpV1>::value) {
+    if (vhloName == "slice_sizes")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DynamicConvOpV1>::value) {
+    if (vhloName == "lhs_dilation" || vhloName == "rhs_dilation" ||
+        vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+    if (vhloName == "window_reversal")
+      return convertDenseBoolArray(vhloName, vhloAttr, stablehloAttrs);
   }
   return notSpecial();
 }
diff --ruN a/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp.orig b/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp.orig
--- stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp.orig
+++ stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp.orig
@@ -0,0 +1,907 @@
+/* Copyright 2022 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringSet.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/Debug.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributeInterfaces.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/dialect/VhloOps.h"
+#include "stablehlo/dialect/VhloTypes.h"
+#include "stablehlo/transforms/MapStablehloToVhlo.h"
+#include "stablehlo/transforms/Passes.h"
+
+#define DEBUG_TYPE "compat-passes"
+
+namespace mlir {
+namespace stablehlo {
+
+#define GEN_PASS_DEF_VHLOLEGALIZETOSTABLEHLOPASS
+#include "stablehlo/transforms/Passes.h.inc"
+
+namespace {
+
+//===----------------------------------------------------------------------===//
+// VHLO --> StableHLO types
+//===----------------------------------------------------------------------===//
+
+class VhloToStablehloTypeConverter : public vhlo::VhloTypeConverter {
+ public:
+  VhloToStablehloTypeConverter() : vhlo::VhloTypeConverter() {
+    addConversion([](Type type) -> Type { return type; });
+    addConversion([](vhlo::TokenV1Type token) -> Type {
+      LLVM_DEBUG(llvm::dbgs() << "Converting TokenType\n");
+      return stablehlo::TokenType::get(token.getContext());
+    });
+    addVhloToBuiltinConversions();
+  }
+
+  Attribute convertEncoding(Attribute attr) const final {
+    if (auto vhloAttr = attr.dyn_cast_or_null<vhlo::TypeExtensionsV1Attr>()) {
+      return stablehlo::TypeExtensionsAttr::get(vhloAttr.getContext(),
+                                                vhloAttr.getBounds());
+    }
+    // All encodings supported in StableHLO.
+    return attr;
+  }
+};
+
+//===----------------------------------------------------------------------===//
+// VHLO --> StableHLO attributes: 1) Generic case.
+// Applicable in areas where there is 1:1 mapping from VHLO to StableHLO.
+// This is the predominant case.
+//===----------------------------------------------------------------------===//
+
+#define RETURN_CONVERTED_ENUM_ATTR(Name, Version)                   \
+  auto vhloValue = vhlo::stringify##Name##Version(attr.getValue()); \
+  auto stablehloValue = stablehlo::symbolize##Name(vhloValue);      \
+  if (!stablehloValue.has_value()) return {};                       \
+  return stablehlo::Name##Attr::get(attr.getContext(), stablehloValue.value())
+
+Attribute convertGeneric(Attribute vhloAttr,
+                         const TypeConverter* typeConverter) {
+  LLVM_DEBUG(llvm::dbgs() << "Converting attr " << vhloAttr);
+  if (auto vhloAttrs = vhloAttr.dyn_cast<vhlo::ArrayV1Attr>()) {
+    SmallVector<Attribute> stablehloAttrs;
+    for (auto vhloAttr : vhloAttrs.getValue()) {
+      auto stablehloAttr = convertGeneric(vhloAttr, typeConverter);
+      if (!stablehloAttr) return {};
+      stablehloAttrs.push_back(stablehloAttr);
+    }
+    return ArrayAttr::get(vhloAttrs.getContext(), stablehloAttrs);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::BooleanV1Attr>()) {
+    return BoolAttr::get(attr.getContext(), attr.getValue());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::ComparisonDirectionV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(ComparisonDirection, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::ComparisonTypeV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(ComparisonType, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::CustomCallApiVersionV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(CustomCallApiVersion, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::DictionaryV1Attr>()) {
+    SmallVector<NamedAttribute> vhloAttrs;
+    for (auto namedAttr : attr.getValue()) {
+      auto builtinName = convertGeneric(namedAttr.first, typeConverter)
+                             .dyn_cast_or_null<StringAttr>();
+      auto builtinValue = convertGeneric(namedAttr.second, typeConverter);
+      if (!builtinName || !builtinValue) return {};
+      vhloAttrs.push_back({builtinName, builtinValue});
+    }
+    return DictionaryAttr::get(attr.getContext(), vhloAttrs);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::FftTypeV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(FftType, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::FloatV1Attr>()) {
+    auto builtinFloatType = typeConverter->convertType(attr.getType());
+    if (!builtinFloatType) return {};
+    // FIXME: What is the proper way to reconstruct a attr?
+    return FloatAttr::get(builtinFloatType, attr.getValue().convertToDouble());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::IntegerV1Attr>()) {
+    auto builtinIntegerType = typeConverter->convertType(attr.getType());
+    if (!builtinIntegerType) return {};
+    return IntegerAttr::get(builtinIntegerType, attr.getValue());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::OutputOperandAliasV1Attr>()) {
+    return stablehlo::OutputOperandAliasAttr::get(
+        attr.getContext(), attr.getOutputTupleIndices(), attr.getOperandIndex(),
+        attr.getOperandTupleIndices());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::PrecisionV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(Precision, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::RngAlgorithmV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(RngAlgorithm, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::RngDistributionV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(RngDistribution, V1);
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::StringV1Attr>()) {
+    return StringAttr::get(attr.getContext(), attr.getValue());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::TensorV1Attr>()) {
+    auto builtinType =
+        typeConverter->convertType(attr.getType()).cast<ShapedType>();
+    if (!builtinType) return {};
+    return DenseIntOrFPElementsAttr::getFromRawBuffer(builtinType,
+                                                      attr.getData());
+  }
+  if (auto attr = vhloAttr.dyn_cast<vhlo::TransposeV1Attr>()) {
+    RETURN_CONVERTED_ENUM_ATTR(Transpose, V1);
+  }
+  // NOTE: TypeExtensionsV1Attr is only used as a RankedTensorType's encoding,
+  // so it's handled during type conversion (see convertEncoding above).
+  if (auto attr = vhloAttr.dyn_cast<vhlo::TypeV1Attr>()) {
+    auto builtinType = typeConverter->convertType(attr.getValue());
+    if (!builtinType) return {};
+    return TypeAttr::get(builtinType);
+  }
+
+  // All VHLO Attributes must be converted by now.
+  if (vhloAttr.getDialect().getNamespace() ==
+      vhlo::VhloDialect::getDialectNamespace()) {
+    // All VHLO attributes must have counterparts in StableHLO.
+    return {};
+  }
+
+  // This should be unreachable unless program is a mix of VHLO and other
+  // dialects, e.g. due to user edits to textual assembly format.
+  return {};
+}
+
+//===----------------------------------------------------------------------===//
+// VHLO --> StableHLO attributes: 2) Special cases.
+// Applicable in areas where there is no 1:1 mapping from VHLO to StableHLO.
+// This is pretty infrequent.
+//===----------------------------------------------------------------------===//
+
+// Indicates the outcome of converting a potentially special case.
+// NOT_SPECIAL means that this wasn't actually a special case.
+// SPECIAL_FAILURE means that this was a special case and conversion failed.
+// SPECIAL_SUCCESS means that this was a special case and conversion succeeded.
+enum class SpecialResult {
+  SPECIAL_SUCCESS = 0,
+  SPECIAL_FAILURE = 1,
+  NOT_SPECIAL = 2,
+};
+
+SpecialResult specialSuccess() { return SpecialResult::SPECIAL_SUCCESS; }
+SpecialResult specialFailure() { return SpecialResult::SPECIAL_FAILURE; }
+SpecialResult notSpecial() { return SpecialResult::NOT_SPECIAL; }
+
+LogicalResult convertInt(Attribute vhloAttr, int64_t& result) {
+  auto vhloIntegerAttr = vhloAttr.dyn_cast<vhlo::IntegerV1Attr>();
+  if (!vhloIntegerAttr) return failure();
+  result = vhloIntegerAttr.getValue().getSExtValue();
+  return success();
+}
+
+LogicalResult convertInts(Attribute vhloAttr,
+                          const TypeConverter* typeConverter,
+                          SmallVector<int64_t>& result) {
+  auto vhloTensorAttr = vhloAttr.dyn_cast<vhlo::TensorV1Attr>();
+  if (!vhloTensorAttr) return failure();
+  auto stablehloAttr = convertGeneric(vhloAttr, typeConverter)
+                           .dyn_cast_or_null<DenseIntElementsAttr>();
+  if (!stablehloAttr) return failure();
+  llvm::append_range(result, stablehloAttr.getValues<int64_t>());
+  return success();
+}
+
+Attribute convertSymbol(Attribute vhloAttr,
+                        const TypeConverter* typeConverter) {
+  auto vhloStringAttr = vhloAttr.dyn_cast<vhlo::StringV1Attr>();
+  if (!vhloStringAttr) return {};
+  auto stablehloStringAttr = convertGeneric(vhloStringAttr, typeConverter)
+                                 .dyn_cast_or_null<StringAttr>();
+  if (!stablehloStringAttr) return {};
+  return FlatSymbolRefAttr::get(stablehloStringAttr);
+}
+
+template <typename OpType>
+Attribute convertChannelHandle(OpType vhloOp,
+                               const TypeConverter* typeConverter) {
+  int64_t channelId, channelType;
+  if (failed(convertInt(vhloOp.getChannelId(), channelId)) ||
+      failed(convertInt(vhloOp.getChannelType(), channelType)))
+    return {};
+  return stablehlo::ChannelHandleAttr::get(vhloOp.getContext(), channelId,
+                                           channelType);
+}
+
+Attribute convertChannelId(Attribute vhloAttr,
+                           const TypeConverter* typeConverter) {
+  int64_t channelId;
+  if (failed(convertInt(vhloAttr, channelId))) return {};
+  return stablehlo::ChannelHandleAttr::get(vhloAttr.getContext(), channelId,
+                                           /*channelType=*/0);
+}
+
+template <typename OpType>
+Attribute convertConvDimensionNumbers(OpType vhloOp,
+                                      const TypeConverter* typeConverter) {
+  int64_t stablehloInputBatchDimension, stablehloInputFeatureDimension;
+  SmallVector<int64_t> stablehloInputSpatialDimensions;
+  int64_t stablehloKernelInputFeatureDimension,
+      stablehloKernelOutputFeatureDimension;
+  SmallVector<int64_t> stablehloKernelSpatialDimensions;
+  int64_t stablehloOutputBatchDimension, stablehloOutputFeatureDimension;
+  SmallVector<int64_t> stablehloOutputSpatialDimensions;
+  if (failed(convertInt(vhloOp.getInputBatchDimension(),
+                        stablehloInputBatchDimension)) ||
+      failed(convertInt(vhloOp.getInputFeatureDimension(),
+                        stablehloInputFeatureDimension)) ||
+      failed(convertInts(vhloOp.getInputSpatialDimensions(), typeConverter,
+                         stablehloInputSpatialDimensions)) ||
+      failed(convertInt(vhloOp.getKernelInputFeatureDimension(),
+                        stablehloKernelInputFeatureDimension)) ||
+      failed(convertInt(vhloOp.getKernelOutputFeatureDimension(),
+                        stablehloKernelOutputFeatureDimension)) ||
+      failed(convertInts(vhloOp.getKernelSpatialDimensions(), typeConverter,
+                         stablehloKernelSpatialDimensions)) ||
+      failed(convertInt(vhloOp.getOutputBatchDimension(),
+                        stablehloOutputBatchDimension)) ||
+      failed(convertInt(vhloOp.getOutputFeatureDimension(),
+                        stablehloOutputFeatureDimension)) ||
+      failed(convertInts(vhloOp.getOutputSpatialDimensions(), typeConverter,
+                         stablehloOutputSpatialDimensions)))
+    return {};
+  return stablehlo::ConvDimensionNumbersAttr::get(
+      vhloOp.getContext(), stablehloInputBatchDimension,
+      stablehloInputFeatureDimension, stablehloInputSpatialDimensions,
+      stablehloKernelInputFeatureDimension,
+      stablehloKernelOutputFeatureDimension, stablehloKernelSpatialDimensions,
+      stablehloOutputBatchDimension, stablehloOutputFeatureDimension,
+      stablehloOutputSpatialDimensions);
+}
+
+Attribute convertCustomCallCalledComputations(
+    Attribute vhloAttr, const TypeConverter* typeConverter) {
+  if (auto vhloArrayAttr = vhloAttr.dyn_cast<vhlo::ArrayV1Attr>()) {
+    SmallVector<Attribute> stablehloAttrs;
+    for (auto vhloAttr : vhloArrayAttr.getValue()) {
+      auto stablehloAttr = convertSymbol(vhloAttr, typeConverter);
+      if (!stablehloAttr) return {};
+      stablehloAttrs.push_back(stablehloAttr);
+    }
+    return ArrayAttr::get(vhloAttr.getContext(), stablehloAttrs);
+  }
+  return {};
+}
+
+Attribute convertDotDimensionNumbers(vhlo::DotGeneralOpV1 vhloOp,
+                                     const TypeConverter* typeConverter) {
+  SmallVector<int64_t> stablehloLhsBatchingDimensions,
+      stablehloRhsBatchingDimensions, stablehloLhsContractingDimensions,
+      stablehloRhsContractingDimensions;
+  if (failed(convertInts(vhloOp.getLhsBatchingDimensions(), typeConverter,
+                         stablehloLhsBatchingDimensions)) ||
+      failed(convertInts(vhloOp.getRhsBatchingDimensions(), typeConverter,
+                         stablehloRhsBatchingDimensions)) ||
+      failed(convertInts(vhloOp.getLhsContractingDimensions(), typeConverter,
+                         stablehloLhsContractingDimensions)) ||
+      failed(convertInts(vhloOp.getRhsContractingDimensions(), typeConverter,
+                         stablehloRhsContractingDimensions)))
+    return {};
+  return stablehlo::DotDimensionNumbersAttr::get(
+      vhloOp.getContext(), stablehloLhsBatchingDimensions,
+      stablehloRhsBatchingDimensions, stablehloLhsContractingDimensions,
+      stablehloRhsContractingDimensions);
+}
+
+Attribute convertFuncCallee(Attribute vhloAttr,
+                            const TypeConverter* typeConverter) {
+  return convertSymbol(vhloAttr, typeConverter);
+}
+
+template <typename OpType>
+Attribute convertGatherDimensionNumbers(OpType vhloOp,
+                                        const TypeConverter* typeConverter) {
+  SmallVector<int64_t> stablehloOffsetDims, stablehloCollapsedSliceDims,
+      stablehloStartIndexMap;
+  int64_t stablehloIndexVectorDim;
+  if (failed(convertInts(vhloOp.getOffsetDims(), typeConverter,
+                         stablehloOffsetDims)) ||
+      failed(convertInts(vhloOp.getCollapsedSliceDims(), typeConverter,
+                         stablehloCollapsedSliceDims)) ||
+      failed(convertInts(vhloOp.getStartIndexMap(), typeConverter,
+                         stablehloStartIndexMap)) ||
+      failed(convertInt(vhloOp.getIndexVectorDim(), stablehloIndexVectorDim)))
+    return {};
+  return stablehlo::GatherDimensionNumbersAttr::get(
+      vhloOp.getContext(), stablehloOffsetDims, stablehloCollapsedSliceDims,
+      stablehloStartIndexMap, stablehloIndexVectorDim);
+}
+
+Attribute convertScatterDimensionNumbers(vhlo::ScatterOpV1 vhloOp,
+                                         const TypeConverter* typeConverter) {
+  SmallVector<int64_t> stablehloUpdateWindowDims, stablehloInsertedWindowDims,
+      stablehloScatterDimsToOperandDims;
+  int64_t stablehloIndexVectorDim;
+  if (failed(convertInts(vhloOp.getUpdateWindowDims(), typeConverter,
+                         stablehloUpdateWindowDims)) ||
+      failed(convertInts(vhloOp.getInsertedWindowDims(), typeConverter,
+                         stablehloInsertedWindowDims)) ||
+      failed(convertInts(vhloOp.getScatterDimsToOperandDims(), typeConverter,
+                         stablehloScatterDimsToOperandDims)) ||
+      failed(convertInt(vhloOp.getIndexVectorDim(), stablehloIndexVectorDim)))
+    return {};
+  return stablehlo::ScatterDimensionNumbersAttr::get(
+      vhloOp.getContext(), stablehloUpdateWindowDims,
+      stablehloInsertedWindowDims, stablehloScatterDimsToOperandDims,
+      stablehloIndexVectorDim);
+}
+
+#undef RETURN_CONVERTED_ENUM_ATTR
+
+template <typename... StringTy>
+void eraseAttrs(SmallVector<NamedAttribute>& attrs, StringTy... names) {
+  StringSet<llvm::MallocAllocator> nameSet({names...});
+  llvm::erase_if(attrs, [&](NamedAttribute attr) {
+    return nameSet.contains(attr.getName());
+  });
+}
+
+template <typename VhloOpTy>
+LogicalResult implodeSpecial(const OpConversionPattern<VhloOpTy>& pattern,
+                             VhloOpTy vhloOp,
+                             SmallVector<NamedAttribute>& vhloAttrs,
+                             SmallVector<NamedAttribute>& stablehloAttrs) {
+  if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DynamicConvOpV1>::value) {
+    auto stablehloAttr =
+        convertConvDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    if (!stablehloAttr) return failure();
+    stablehloAttrs.emplace_back(
+        StringAttr::get(pattern.getContext(), "dimension_numbers"),
+        stablehloAttr);
+    eraseAttrs(vhloAttrs, "input_batch_dimension", "input_feature_dimension",
+               "input_spatial_dimensions", "kernel_input_feature_dimension",
+               "kernel_output_feature_dimension", "kernel_spatial_dimensions",
+               "output_batch_dimension", "output_feature_dimension",
+               "output_spatial_dimensions");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::DotGeneralOpV1>::value) {
+    auto stablehloAttr =
+        convertDotDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    if (!stablehloAttr) return failure();
+    stablehloAttrs.emplace_back(
+        StringAttr::get(pattern.getContext(), "dot_dimension_numbers"),
+        stablehloAttr);
+    eraseAttrs(vhloAttrs, "lhs_batching_dimensions", "rhs_batching_dimensions",
+               "lhs_contracting_dimensions", "rhs_contracting_dimensions");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::DynamicGatherOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::GatherOpV1>::value) {
+    auto stablehloAttr =
+        convertGatherDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    if (!stablehloAttr) return failure();
+    stablehloAttrs.emplace_back(
+        StringAttr::get(pattern.getContext(), "dimension_numbers"),
+        stablehloAttr);
+    eraseAttrs(vhloAttrs, "offset_dims", "collapsed_slice_dims",
+               "start_index_map", "index_vector_dim");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ScatterOpV1>::value) {
+    auto stablehloAttr =
+        convertScatterDimensionNumbers(vhloOp, pattern.getTypeConverter());
+    if (!stablehloAttr) return failure();
+    stablehloAttrs.emplace_back(
+        StringAttr::get(pattern.getContext(), "scatter_dimension_numbers"),
+        stablehloAttr);
+    eraseAttrs(vhloAttrs, "update_window_dims", "inserted_window_dims",
+               "scatter_dims_to_operand_dims", "index_vector_dim");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::SendOpV1>::value) {
+    auto stablehloAttr =
+        convertChannelHandle(vhloOp, pattern.getTypeConverter());
+    if (!stablehloAttr) return failure();
+    stablehloAttrs.emplace_back(
+        StringAttr::get(pattern.getContext(), "channel_handle"), stablehloAttr);
+    eraseAttrs(vhloAttrs, "channel_id", "channel_type");
+  }
+  return success();
+}
+
+template <typename T, typename Attr>
+SpecialResult convertDenseArray(StringAttr vhloName, Attribute vhloAttr,
+                                SmallVector<NamedAttribute>& stablehloAttrs) {
+  auto tensorAttr = dyn_cast<vhlo::TensorV1Attr>(vhloAttr);
+  if (!tensorAttr) return specialFailure();
+
+  if (tensorAttr.getData().size() % sizeof(T) != 0) return specialFailure();
+
+  // Extracting the data in this way prevents misaligned memory issues.
+  auto vec =
+      ArrayRef<T>(reinterpret_cast<const T*>(tensorAttr.getData().data()),
+                  tensorAttr.getData().size() / sizeof(T))
+          .vec();
+  SmallVector<T> data;
+  for (T b : vec) data.push_back(b);
+
+  // Handle splats
+  if (data.size() == 1) {
+    auto tensorType = tensorAttr.getType().dyn_cast<vhlo::RankedTensorV1Type>();
+    if (!tensorType || (tensorType.getShape().size() != 1))
+      return specialFailure();
+    auto size = tensorType.getShape()[0];
+    data.resize(size, data[0]);
+  }
+
+  stablehloAttrs.emplace_back(vhloName, Attr::get(vhloAttr.getContext(), data));
+  return specialSuccess();
+}
+
+SpecialResult convertDenseI64Array(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<int64_t, DenseI64ArrayAttr>(vhloName, vhloAttr,
+                                                       stablehloAttrs);
+}
+
+SpecialResult convertDenseBoolArray(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<bool, DenseBoolArrayAttr>(vhloName, vhloAttr,
+                                                     stablehloAttrs);
+}
+
+template <typename VhloOpTy>
+SpecialResult convertSpecial(const OpConversionPattern<VhloOpTy>& pattern,
+                             StringAttr vhloName, Attribute vhloAttr,
+                             SmallVector<NamedAttribute>& stablehloAttrs) {
+  StringAttr stablehloName = vhloName;
+  Attribute stablehloAttr;
+  if constexpr (std::is_same<VhloOpTy, vhlo::AllGatherOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::AllReduceOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::AllToAllOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::CollectivePermuteOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::ReduceScatterOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::CollectiveBroadcastOpV1>::value) {
+    if (vhloName == "channel_id") {
+      stablehloName = StringAttr::get(pattern.getContext(), "channel_handle");
+      stablehloAttr = convertChannelId(vhloAttr, pattern.getTypeConverter());
+      if (!stablehloAttr) return specialFailure();
+    }
+    if (vhloName == "use_global_device_ids") {
+      auto vhloBooleanAttr = vhloAttr.dyn_cast<vhlo::BooleanV1Attr>();
+      if (!vhloBooleanAttr) return specialFailure();
+      if (!vhloBooleanAttr.getValue()) return specialSuccess();
+      stablehloAttr = UnitAttr::get(pattern.getContext());
+    }
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::CustomCallOpV1>::value) {
+    if (vhloName == "called_computations") {
+      stablehloAttr = convertCustomCallCalledComputations(
+          vhloAttr, pattern.getTypeConverter());
+      if (!stablehloAttr) return specialFailure();
+    }
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::CallOpV1>::value) {
+    if (vhloName == "callee") {
+      stablehloAttr = convertFuncCallee(vhloAttr, pattern.getTypeConverter());
+      if (!stablehloAttr) return specialFailure();
+    }
+  }
+  if (stablehloAttr) {
+    stablehloAttrs.emplace_back(stablehloName, stablehloAttr);
+    return specialSuccess();
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::FftOpV1>::value) {
+    if (vhloName == "fft_length")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::BroadcastOpV1>::value) {
+    if (vhloName == "broadcast_sizes")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::DynamicSliceOpV1>::value) {
+    if (vhloName == "slice_sizes")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ReverseOpV1>::value) {
+    if (vhloName == "dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::TransposeOpV1>::value) {
+    if (vhloName == "permutation")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::PadOpV1>::value) {
+    if (vhloName == "edge_padding_low" || vhloName == "edge_padding_high" ||
+        vhloName == "interior_padding")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SliceOpV1>::value) {
+    if (vhloName == "start_indices" || vhloName == "limit_indices" ||
+        vhloName == "strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::BroadcastInDimOpV1>::value) {
+    if (vhloName == "broadcast_dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy,
+                             vhlo::DynamicBroadcastInDimOpV1>::value) {
+    if (vhloName == "broadcast_dimensions" ||
+        vhloName == "known_expanding_dimensions" ||
+        vhloName == "known_nonexpanding_dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SelectAndScatterOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ReduceWindowOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides" ||
+        vhloName == "base_dilations" || vhloName == "window_dilations")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::MapOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::ReduceOpV1>::value) {
+    if (vhloName == "dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::GatherOpV1>::value) {
+    if (vhloName == "slice_sizes")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DynamicConvOpV1>::value) {
+    if (vhloName == "lhs_dilation" || vhloName == "rhs_dilation" ||
+        vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+    if (vhloName == "window_reversal")
+      return convertDenseBoolArray(vhloName, vhloAttr, stablehloAttrs);
+  }
+  return notSpecial();
+}
+
+//===----------------------------------------------------------------------===//
+// VHLO --> StableHLO attributes: 3) Default attributes.
+// Unlike StableHLO, VHLO doesn't have default attributes, so the corresponding
+// attributes are added explicitly during StableHLO --> VHLO conversion.
+// These attributes are removed here. (Strictly speaking, don't have to do this
+// but this makes eyeballing easier).
+//===----------------------------------------------------------------------===//
+
+bool isBoolean(Attribute vhloAttr, bool value) {
+  auto attr = vhloAttr.template dyn_cast_or_null<vhlo::BooleanV1Attr>();
+  return attr && attr.getValue() == value;
+}
+
+bool isEmptyArray(Attribute vhloAttr) {
+  auto attr = vhloAttr.dyn_cast_or_null<vhlo::ArrayV1Attr>();
+  return attr && attr.getValue().empty();
+}
+
+bool isEmptyString(Attribute vhloAttr) {
+  auto attr = vhloAttr.dyn_cast_or_null<vhlo::StringV1Attr>();
+  return attr && attr.getValue().empty();
+}
+
+bool isEmptyTensor(Attribute vhloAttr) {
+  auto attr = vhloAttr.dyn_cast_or_null<vhlo::TensorV1Attr>();
+  return attr && attr.getData().empty();
+}
+
+bool isEnum(Attribute vhloAttr, Attribute value) { return vhloAttr == value; }
+
+bool isInteger(Attribute vhloAttr, int64_t value) {
+  auto attr = vhloAttr.template dyn_cast_or_null<vhlo::IntegerV1Attr>();
+  return attr && attr.getValue().getSExtValue() == value;
+}
+
+bool isSplatArray(Attribute vhloAttr, Attribute splatValue) {
+  auto attr = vhloAttr.dyn_cast_or_null<vhlo::ArrayV1Attr>();
+  return attr && llvm::all_of(attr.getValue(), [&](Attribute attr) {
+           return attr == splatValue;
+         });
+}
+
+template <typename T>
+bool isSplatTensor(const ConversionPattern& pattern, Attribute vhloAttr,
+                   T splatValue) {
+  auto attr = convertGeneric(vhloAttr, pattern.getTypeConverter())
+                  .template dyn_cast_or_null<DenseElementsAttr>();
+  return attr && attr.isSplat() &&
+         attr.template getSplatValue<T>() == splatValue;
+}
+
+bool isString(Attribute vhloAttr, StringRef value) {
+  auto attr = vhloAttr.dyn_cast_or_null<vhlo::StringV1Attr>();
+  return attr && attr.getValue() == value;
+}
+
+// TODO(#1232): Also validate attributes before removing them.
+// Current logic assumes that these attributes are valid, but that might not
+// necessarily be the case because VHLO doesn't have verifiers.
+template <typename VhloOpTy>
+LogicalResult removeDefaults(const OpConversionPattern<VhloOpTy>& pattern,
+                             VhloOpTy vhloOp,
+                             SmallVector<NamedAttribute>& vhloAttrs) {
+  if constexpr (std::is_same<VhloOpTy, vhlo::FuncOpV1>::value) {
+    if (isString(vhloOp.getSymVisibilityAttr(), ""))
+      eraseAttrs(vhloAttrs, "sym_visibility");
+    if (isEmptyArray(vhloOp.getArgAttrsAttr()))
+      eraseAttrs(vhloAttrs, "arg_attrs");
+    if (isEmptyArray(vhloOp.getResAttrsAttr()))
+      eraseAttrs(vhloAttrs, "res_attrs");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::AllGatherOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::AllReduceOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::ReduceScatterOpV1>::value) {
+    if (isInteger(vhloOp.getChannelIdAttr(), 0))
+      eraseAttrs(vhloAttrs, "channel_id");
+    if (isBoolean(vhloOp.getUseGlobalDeviceIdsAttr(), false))
+      eraseAttrs(vhloAttrs, "use_global_device_ids");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::AllToAllOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::CollectivePermuteOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::CollectiveBroadcastOpV1>::value) {
+    if (isInteger(vhloOp.getChannelIdAttr(), 0))
+      eraseAttrs(vhloAttrs, "channel_id");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::CholeskyOpV1>::value) {
+    if (isBoolean(vhloOp.getLowerAttr(), false)) eraseAttrs(vhloAttrs, "lower");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::CompareOpV1>::value) {
+    if (isEnum(vhloOp.getCompareTypeAttr(),
+               vhlo::ComparisonTypeV1Attr::get(pattern.getContext(),
+                                               vhlo::ComparisonTypeV1::NOTYPE)))
+      eraseAttrs(vhloAttrs, "compare_type");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DynamicConvOpV1>::value) {
+    if (isSplatTensor(pattern, vhloOp.getWindowStridesAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "window_strides");
+    if (isSplatTensor(pattern, vhloOp.getPaddingAttr(), 0ll))
+      eraseAttrs(vhloAttrs, "padding");
+    if (isSplatTensor(pattern, vhloOp.getLhsDilationAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "lhs_dilation");
+    if (isSplatTensor(pattern, vhloOp.getRhsDilationAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "rhs_dilation");
+    if (isSplatTensor(pattern, vhloOp.getWindowReversalAttr(), false))
+      eraseAttrs(vhloAttrs, "window_reversal");
+    if (isSplatArray(vhloOp.getPrecisionConfigAttr(),
+                     vhlo::PrecisionV1Attr::get(pattern.getContext(),
+                                                vhlo::PrecisionV1::DEFAULT)))
+      eraseAttrs(vhloAttrs, "precision_config");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::CustomCallOpV1>::value) {
+    if (isBoolean(vhloOp.getHasSideEffectAttr(), false))
+      eraseAttrs(vhloAttrs, "has_side_effect");
+    if (isEmptyString(vhloOp.getBackendConfigAttr()))
+      eraseAttrs(vhloAttrs, "backend_config");
+    if (isEnum(vhloOp.getApiVersionAttr(),
+               vhlo::CustomCallApiVersionV1Attr::get(
+                   pattern.getContext(),
+                   vhlo::CustomCallApiVersionV1::API_VERSION_ORIGINAL)))
+      eraseAttrs(vhloAttrs, "api_version");
+    if (isEmptyArray(vhloOp.getCalledComputations()))
+      eraseAttrs(vhloAttrs, "called_computations");
+    if (isEmptyArray(vhloOp.getOperandLayouts()))
+      eraseAttrs(vhloAttrs, "operand_layouts");
+    if (isEmptyArray(vhloOp.getResultLayouts()))
+      eraseAttrs(vhloAttrs, "result_layouts");
+    if (isEmptyArray(vhloOp.getOutputOperandAliases()))
+      eraseAttrs(vhloAttrs, "output_operand_aliases");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::DotGeneralOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DotOpV1>::value) {
+    if (isSplatArray(vhloOp.getPrecisionConfigAttr(),
+                     vhlo::PrecisionV1Attr::get(pattern.getContext(),
+                                                vhlo::PrecisionV1::DEFAULT)))
+      eraseAttrs(vhloAttrs, "precision_config");
+  }
+  if constexpr (std::is_same<VhloOpTy,
+                             vhlo::DynamicBroadcastInDimOpV1>::value) {
+    if (isEmptyTensor(vhloOp.getKnownExpandingDimensionsAttr()))
+      eraseAttrs(vhloAttrs, "known_expanding_dimensions");
+    if (isEmptyTensor(vhloOp.getKnownNonexpandingDimensionsAttr()))
+      eraseAttrs(vhloAttrs, "known_nonexpanding_dimensions");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::DynamicGatherOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::GatherOpV1>::value) {
+    if (isBoolean(vhloOp.getIndicesAreSortedAttr(), false))
+      eraseAttrs(vhloAttrs, "indices_are_sorted");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::InfeedOpV1>::value) {
+    if (isEmptyString(vhloOp.getInfeedConfig()))
+      eraseAttrs(vhloAttrs, "infeed_config");
+    if (isEmptyArray(vhloOp.getLayout())) eraseAttrs(vhloAttrs, "layout");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::OutfeedOpV1>::value) {
+    if (isEmptyString(vhloOp.getOutfeedConfig()))
+      eraseAttrs(vhloAttrs, "outfeed_config");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::RecvOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::SendOpV1>::value) {
+    if (isBoolean(vhloOp.getIsHostTransferAttr(), false))
+      eraseAttrs(vhloAttrs, "is_host_transfer");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ReduceWindowOpV1>::value) {
+    if (isSplatTensor(pattern, vhloOp.getWindowStridesAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "window_strides");
+    if (isSplatTensor(pattern, vhloOp.getBaseDilationsAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "base_dilations");
+    if (isSplatTensor(pattern, vhloOp.getWindowDilationsAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "window_dilations");
+    if (isSplatTensor(pattern, vhloOp.getPaddingAttr(), 0ll))
+      eraseAttrs(vhloAttrs, "padding");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ScatterOpV1>::value) {
+    if (isBoolean(vhloOp.getIndicesAreSortedAttr(), false))
+      eraseAttrs(vhloAttrs, "indices_are_sorted");
+    if (isBoolean(vhloOp.getUniqueIndicesAttr(), false))
+      eraseAttrs(vhloAttrs, "unique_indices");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SelectAndScatterOpV1>::value) {
+    if (isSplatTensor(pattern, vhloOp.getWindowStridesAttr(), 1ll))
+      eraseAttrs(vhloAttrs, "window_strides");
+    if (isSplatTensor(pattern, vhloOp.getPaddingAttr(), 0ll))
+      eraseAttrs(vhloAttrs, "padding");
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SortOpV1>::value) {
+    if (isInteger(vhloOp.getDimensionAttr(), -1))
+      eraseAttrs(vhloAttrs, "dimension");
+    if (isBoolean(vhloOp.getIsStableAttr(), false))
+      eraseAttrs(vhloAttrs, "is_stable");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// VHLO --> StableHLO operations
+//===----------------------------------------------------------------------===//
+
+template <typename VhloOpTy>
+class VhloToStablehloOpConverter : public OpConversionPattern<VhloOpTy> {
+ public:
+  using OpConversionPattern<VhloOpTy>::OpConversionPattern;
+  LogicalResult matchAndRewrite(
+      VhloOpTy vhloOp, typename VhloOpTy::Adaptor adaptor,
+      ConversionPatternRewriter& rewriter) const final {
+    SmallVector<Type> stablehloTypes;
+    if (failed(this->getTypeConverter()->convertTypes(vhloOp->getResultTypes(),
+                                                      stablehloTypes)))
+      return failure();
+
+    // These operands have already been converted to StableHLO by
+    // the dialect conversion infrastructure.
+    ValueRange stablehloOperands = adaptor.getOperands();
+
+    // Convert VHLO attributes to StableHLO equivalents.
+    // There are two paths:
+    //   1) Generic path where default logic applies, and there is a 1:1
+    //      mapping from VHLO to StableHLO.
+    //   2) Special cases (currently, about a dozen) where there is not 1:1
+    //      mapping from VHLO to StableHLO.
+    SmallVector<NamedAttribute> vhloAttrs = to_vector(vhloOp->getAttrs());
+    SmallVector<NamedAttribute> stablehloAttrs;
+    if (failed(removeDefaults(*this, vhloOp, vhloAttrs))) return failure();
+    if (failed(implodeSpecial(*this, vhloOp, vhloAttrs, stablehloAttrs)))
+      return failure();
+    for (NamedAttribute vhloAttr : vhloAttrs) {
+      auto result = convertSpecial(*this, vhloAttr.getName(),
+                                   vhloAttr.getValue(), stablehloAttrs);
+      switch (result) {
+        case SpecialResult::SPECIAL_SUCCESS:
+          break;
+        case SpecialResult::SPECIAL_FAILURE:
+          return failure();
+        case SpecialResult::NOT_SPECIAL:
+          auto stablehloAttr =
+              convertGeneric(vhloAttr.getValue(), this->getTypeConverter());
+          if (!stablehloAttr) return failure();
+          stablehloAttrs.push_back({vhloAttr.getName(), stablehloAttr});
+          break;
+      }
+    }
+
+    // Replace vhlo.return --> func.return if direct parent is a func op.
+    if constexpr (std::is_same<VhloOpTy, vhlo::ReturnOpV1>::value) {
+      if (llvm::isa<vhlo::FuncOpV1, func::FuncOp>(vhloOp->getParentOp())) {
+        rewriter.replaceOpWithNewOp<func::ReturnOp>(
+            vhloOp, stablehloTypes, stablehloOperands, stablehloAttrs);
+        return success();
+      }
+    }
+
+    // Convert the VHLO operation to a StableHLO equivalent.
+    // This can almost be done in a generic fashion, except for
+    // vhlo.case that uses a variadic number of regions which means an
+    // additional argument for the generic builder.
+    VhloToStablehloOp<VhloOpTy> stablehloOp;
+    if constexpr (std::is_same<VhloOpTy, vhlo::CaseOpV1>::value) {
+      stablehloOp = rewriter.replaceOpWithNewOp<stablehlo::CaseOp>(
+          vhloOp, stablehloTypes, stablehloOperands, stablehloAttrs,
+          vhloOp.getBranches().size());
+    } else {
+      stablehloOp = rewriter.replaceOpWithNewOp<VhloToStablehloOp<VhloOpTy>>(
+          vhloOp, stablehloTypes, stablehloOperands, stablehloAttrs);
+    }
+
+    for (auto [vhloRegion, stablehloRegion] :
+         llvm::zip(vhloOp->getRegions(), stablehloOp->getRegions())) {
+      rewriter.inlineRegionBefore(vhloRegion, stablehloRegion,
+                                  stablehloRegion.end());
+      if (failed(rewriter.convertRegionTypes(&stablehloRegion,
+                                             *this->getTypeConverter(),
+                                             /*entryConversion=*/nullptr)))
+        return failure();
+    }
+    return success();
+  }
+};
+
+template <typename... StablehloOpTypes>
+void populateVhloToStablehloPatterns(RewritePatternSet* patterns,
+                                     TypeConverter* converter,
+                                     MLIRContext* context) {
+  patterns
+      ->add<VhloToStablehloOpConverter<StablehloToVhloOp<StablehloOpTypes>>...>(
+          *converter, context);
+}
+
+}  // namespace
+
+struct VhloLegalizeToStablehloPass
+    : public impl::VhloLegalizeToStablehloPassBase<
+          VhloLegalizeToStablehloPass> {
+  void runOnOperation() override {
+    ConversionTarget target(getContext());
+    target.addIllegalDialect<vhlo::VhloDialect>();
+    target.addLegalDialect<stablehlo::StablehloDialect>();
+    target.addLegalDialect<func::FuncDialect>();
+
+    VhloToStablehloTypeConverter converter;
+    RewritePatternSet patterns(&getContext());
+    stablehlo::populateVhloToStablehloPatterns(&patterns, &converter,
+                                               &getContext());
+
+    // Upgraded VHLO should always be convertible to StableHLO.
+    // Arbitrary VHLO might not be convertible if it uses deprecated features
+    // which are no longer available in StableHLO.
+    if (failed(applyPartialConversion(getOperation(), target,
+                                      std::move(patterns)))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+void populateVhloToStablehloPatterns(RewritePatternSet* patterns,
+                                     TypeConverter* converter,
+                                     MLIRContext* context) {
+  populateVhloToStablehloPatterns<
+#define GET_OP_LIST
+#include "stablehlo/dialect/StablehloOps.cpp.inc"
+      , func::CallOp, func::FuncOp>(patterns, converter, context);
+  // Omit ReturnOp since it is handled during conversion of vhlo::ReturnOp
+}
+
+}  // namespace stablehlo
+}  // namespace mlir

